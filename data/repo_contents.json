{
  "repo_name": "RahulSethi070801/Hybrid-Distributed-File-System",
  "files": [
    {
      "file_path": "Cache.cpp",
      "content": "#include \"Cache.h\"\n#include <unordered_map>\n#include <list>\n#include <utility>\n#include <iostream>\n\nusing namespace std;\n\nCache::Cache(size_t sizeLimit) : cacheSizeLimit(sizeLimit) {}\n\nCache::~Cache() {\n    for (auto it = cacheMap.begin(); it != cacheMap.end(); it++) {\n        delete[] it->second.first;\n    }\n}\n\nvoid Cache::addFileToCache(string &fileName, char *fileContent, size_t contentSize) {\n    lock_guard<mutex> lock(mtx_cache);\n\n    if (cacheMap.size() >= cacheSizeLimit) {\n        string lastFile = cacheOrder.back();\n        cacheOrder.pop_back();\n        delete[] cacheMap[lastFile].first;\n        cacheMap.erase(lastFile);\n    }\n    char *fileData = new char[contentSize];\n    memcpy(fileData, fileContent, contentSize);\n    cacheMap[fileName] = make_pair(fileData, contentSize);\n    cacheOrder.push_front(fileName);\n}\n\nbool Cache::getFileFromCache(string &fileName, char *&fileContent, size_t &contentSize) {\n    lock_guard<mutex> lock(mtx_cache);\n    if (cacheMap.find(fileName) == cacheMap.end()) {\n        return false;\n    }\n    cacheOrder.remove(fileName);\n    cacheOrder.push_front(fileName);\n    fileContent = new char[cacheMap[fileName].second];\n    memcpy(fileContent, cacheMap[fileName].first, cacheMap[fileName].second);\n    contentSize = cacheMap[fileName].second;\n    return true;\n}\n\nvoid Cache::invalidateFileInCache(string &fileName) {\n    lock_guard<mutex> lock(mtx_cache);\n    if (cacheMap.find(fileName) != cacheMap.end()) {\n        delete[] cacheMap[fileName].first;\n        cacheMap.erase(fileName);\n        // cacheOrder.remove(fileName);\n        cacheOrder.remove_if([&fileName](const string &item) { return item == fileName; });\n        cout<<\"File invalidated in cache: \"<<fileName<<endl;\n        printCache();\n    }\n}\n\nvoid Cache::printCache() {\n    cout<<\"Cache order: \";\n    for (auto it = cacheOrder.begin(); it != cacheOrder.end(); it++) {\n        cout << *it << \" \";\n    }\n    cout << endl;\n    cout<<\"Cache map: \";\n    for (auto it = cacheMap.begin(); it != cacheMap.end(); it++) {\n        cout << it->first << \" \";\n    }\n    cout << endl;\n}"
    },
    {
      "file_path": "Cache.h",
      "content": "#ifndef CACHE_H\n#define CACHE_H\n\n#include <unordered_map>\n#include <list>\n#include <utility>\n#include <iostream>\n\nusing namespace std;\n\nclass Cache {\n\nprivate:\n    size_t cacheSizeLimit;\n    std::list<string> cacheOrder;\n    std::unordered_map<string, pair<char*, size_t>> cacheMap;\n    mutex mtx_cache;\n\npublic:\n    Cache(size_t sizeLimit);\n\n    ~Cache();\n\n    void addFileToCache(string &fileName, char *fileContent, size_t contentSize);\n    bool getFileFromCache(string &fileName, char *&fileData, size_t &contentSize);\n    void printCache();\n    void invalidateFileInCache(string &fileName);\n    // void updateCache(string fileName, string fileData);\n\n};\n\n#endif // CACHE_H"
    },
    {
      "file_path": "ConsistentHashRing.cpp",
      "content": "#include \"ConsistentHashRing.h\"\n#include \"utils.h\"\n\n\nConsistentHashRing::ConsistentHashRing() {}\n\nvoid ConsistentHashRing::addNode(Node node) {\n    lock_guard<mutex> lock(mtx);\n    node.setRingHash(hashFunction(node.getNodeId()));\n    if(nodeRing.size() == 0){\n        selfNode = node;\n    }\n    nodeRing[node.getRingHash()] = node;\n    nodesHashTable[node.getNodeId()] = node;\n    cout << \"Added node \" << node.getNodeId() << \" at position \" << node.getRingHash() << \"\\n\";\n}\n\nvoid ConsistentHashRing::removeNode(Node node) {\n    lock_guard<mutex> lock(mtx);\n    int ringHash = nodesHashTable[node.getNodeId()].getRingHash();\n    nodeRing.erase(ringHash);\n    nodesHashTable.erase(node.getNodeId());\n    cout << \"Removed node \" << node.getNodeId() << \" from position \" << ringHash << \"\\n\";\n}\n\n\n// overload << operator\nostream& operator<<(ostream& os, const ConsistentHashRing& ring) {\n    for (const auto& pair : ring.nodeRing) {\n        os << pair.first << \" -> \" << pair.second.getNodeId() << endl;\n    }\n    return os;\n}\n\nNode ConsistentHashRing::getPrimaryNodeForFile(int hash) {\n    lock_guard<mutex> lock(mtx);\n    auto it = nodeRing.lower_bound(hash);\n    if (it == nodeRing.end()) {\n        it = nodeRing.begin();\n    }\n    // cout << \"Primary node for file with hash \" << hash << \" is \" << it->first << \" with node id \" << it->second.getNodeId() << \"\\n\";\n    return it->second;\n}\n\nNode ConsistentHashRing::getNode(int hash) {\n    lock_guard<mutex> lock(mtx);\n    return nodeRing[hash];\n}\n\nNode ConsistentHashRing::getNode(string nodeId) {\n    lock_guard<mutex> lock(mtx);\n    return nodesHashTable[nodeId];\n}\n\nbool ConsistentHashRing::isPredecessor(string nodeId) {\n    // consider the the map of nodes as a circular ring find the predecessor of the self node\n    // if the predecessor is the node with the given nodeId, return true\n    // else return false\n    lock_guard<mutex> lock(mtx);\n    auto it = nodeRing.find(selfNode.getRingHash());\n    if (it == nodeRing.begin()) {\n        it = prev(nodeRing.end());\n    } else {\n        it--;\n    }\n    return it->second.getNodeId() == nodeId;\n}\n\n\nNode ConsistentHashRing::getSelfNode() {\n    lock_guard<mutex> lock(mtx);\n    return selfNode;\n}\n\nvector<int> ConsistentHashRing::getKPredecessors(int k) {\n    lock_guard<mutex> lock(mtx);\n    vector<int> predecessors;\n    auto it = nodeRing.find(selfNode.getRingHash());\n    for (int i = 0; i < k; i++) {\n        if (it == nodeRing.begin()) {\n            it = prev(nodeRing.end());\n        } else {\n            it--;\n        }\n        if(it->second.getNodeId() == selfNode.getNodeId()){\n            break;\n        }\n        predecessors.push_back(it->first);\n    }\n    return predecessors;\n}\n\nvector<int> ConsistentHashRing::getListOfNodes() {\n    lock_guard<mutex> lock(mtx);\n    vector<int> nodes;\n    for (const auto& pair : nodeRing) {\n        nodes.push_back(pair.first);\n    }\n    return nodes;\n}\n\nvector<int> ConsistentHashRing::getKSuccessorsOf(int k, int hash) {\n    lock_guard<mutex> lock(mtx);\n    vector<int> successors;\n    auto it = nodeRing.lower_bound(hash);\n    it++;\n    for (int i = 0; i < k; i++) {\n        if (it == nodeRing.end()) {\n            it = nodeRing.begin();\n        }\n        if(it->second.getRingHash() == hash){\n            break;\n        }\n        successors.push_back(it->first);\n        it++;\n    }\n    return successors;\n}\n\n"
    },
    {
      "file_path": "ConsistentHashRing.h",
      "content": "#ifndef RING_H\n#define RING_H\n\n\n#include <map>\n#include <unordered_map>\n#include <mutex>\n#include <vector>\n#include \"Node.h\"\nusing namespace std;\n\n\nclass ConsistentHashRing {\nprivate:\n    map<int, Node> nodeRing;\n    unordered_map<string, Node> nodesHashTable;\n    Node selfNode;\n    mutable std::mutex mtx;\npublic:\n    ConsistentHashRing();\n    void addNode(Node node);\n    void removeNode(Node node);\n    Node getPrimaryNodeForFile(int hash);\n    Node getNode(int hash);\n    Node getNode(string nodeId);\n    Node getSelfNode();\n    // overload << operator\n    friend ostream& operator<<(ostream& os, const ConsistentHashRing& ring);\n    bool isPredecessor(string nodeId);\n    vector<int> getKPredecessors(int k);\n    vector<int> getListOfNodes();\n    vector<int> getKSuccessorsOf(int k, int hash);\n\n};\n\nostream& operator<<(ostream& os, const ConsistentHashRing& ring);\n\n#endif // RING_H"
    },
    {
      "file_path": "Daemon.cpp",
      "content": "#include<iostream>\n#include <unistd.h>\n#include <regex>\n#include <string>\n#include <sys/types.h>\n#include <sys/socket.h>\n#include <netdb.h>\n#include <arpa/inet.h>\n#include <thread>\n#include <atomic>\n#include <chrono>\n#include <queue>\n#include <fstream>\n#include <condition_variable>\nusing namespace std;\nusing namespace std::chrono;\n\n#include \"MembershipList.cpp\"\n#include \"Message.cpp\"\n#include \"utils.h\"\n#include \"HyDFS.h\"\n\n\n\n#define BUFFER_SIZE 4096\n#define K 5\n#define TTL 4\n#define SUSPECT_TIMEOUT 4000\n#define PING_TIMEOUT 1500\n#define CYCLE_TIME 2000\n#define DROP_PROBABILITY 0\n#define PING_VALIDATOR_CYCLE_TIME 100\n\n\n\nclass Daemon {\nprivate:\n    char *port;\n    int sockfd;\n    MembershipList membershipList;\n    std::atomic<bool> running;\n    std::atomic<bool> isSuspicionMechanismEnabled;\n    std::atomic<int> bytesReceived, bytesSent;\n    HyDFS* dfsListener;\n    // start timestamp\n\n    std::queue<Message> infoQueue;\n    std::mutex mtx_qinfo;\n    std::condition_variable cv_qinfo;\n\n    unordered_set<string> pingedNodes, suspectedNodes;\n    std::mutex mtx_pingedNodes, mtx_suspectedNodes;\n\n    string nodeId;\n    Node selfNode;\n    bool dropNextAck;\n\n    vector<thread> threads;\n\n    void sendTo(int sockfd, string message, struct sockaddr_in addr, MessageType type, Node toNode = Node()) {\n        int numBytesSent = sendto(sockfd, message.c_str(), message.size(), 0, (struct sockaddr *)&addr, sizeof(addr));\n        if (numBytesSent == -1) {\n            perror(\"sendto failed\");\n        } else {\n            bytesSent += numBytesSent;\n            // get currennt timestamp in human readable format with milliseconds precision\n            if(toNode.getNodeId() == \"\"){\n                // cout << Node::getCurrentFullTimestamp() << \": Sent \" << messageTypeToString(type) << \" to \" << inet_ntoa(addr.sin_addr) << \":\" << ntohs(addr.sin_port) << endl;\n            } else {\n                // cout << Node::getCurrentFullTimestamp() << \": Sent \" << messageTypeToString(type) << \" to \" << toNode.getNodeName() << \":\" << toNode.getPort() << endl;\n            }\n        }\n    }\n\n\n    void disseminateMessage(int sockfd, Message msg) {\n        vector<Node> randomNodes = membershipList.getKShuffledNodes(K);\n        if(msg.getType() == MessageType::FAILED || msg.getType() == MessageType::SUSPECT){\n            randomNodes.push_back(msg.getNodesList().front());\n        }\n        for (const Node &n : randomNodes) {\n            struct sockaddr_in nodeAddr = getAddrFromNode(n);\n            sendTo(sockfd, msg.serializeMessage(), nodeAddr, msg.getType(), n);\n        }\n    }\n\n    void suspectAndDisseminate(int sockfd, Node node, int ttl) {\n        bool success = membershipList.markNodeSuspected(node);\n        if(success){\n            mtx_suspectedNodes.lock();\n            suspectedNodes.insert(node.getNodeId());\n            mtx_suspectedNodes.unlock();\n            Node suspectedNode = membershipList.getNode(node.getNodeId());\n            if(ttl <= 0) {\n                return;\n            }\n            Message finalMessage = Message::createSuspectMessage(selfNode, {node}, ttl);\n            disseminateMessage(sockfd, finalMessage);\n        }\n    }\n\n    void setAliveAndDisseminate(int sockfd, Node node, int ttl) {\n        bool success = membershipList.markNodeAlive(node);\n        if(success){\n            mtx_suspectedNodes.lock();\n            suspectedNodes.erase(node.getNodeId());\n            mtx_suspectedNodes.unlock();\n            Node aliveNode = membershipList.getNode(node.getNodeId());\n            if(ttl <= 0) {\n                return;\n            }\n            Message finalMessage = Message::createAliveMessage(selfNode, {node}, ttl);\n            disseminateMessage(sockfd, finalMessage);\n        }\n    }\n\n    void failAndDisseminate(int sockfd, Node node, int ttl) {\n        bool success = membershipList.removeNode(node);\n        if(success){\n\n            // desseminate only if you havent already marked the node as failed or left\n            Node failedNode = membershipList.getNode(node.getNodeId());\n            updateListener({failedNode});\n            if(ttl <= 0) {\n                return;\n            }\n            Message finalMessage = Message::createFailedMessage(selfNode, {node}, ttl);\n            disseminateMessage(sockfd, finalMessage);\n        }\n    }\n\n    void failedMessageProcessor(int sockfd, Message msg){\n        if (msg.getNodesList().empty()) {\n            perror(\"No Node To Fail\");\n        }\n        else{\n            Node failedNode = msg.getNodesList().front();\n            if(failedNode.getNodeId() == selfNode.getNodeId()){\n                cout<<\"I am the one who is failed, killing myself\\n\";\n                running = false;\n                std::this_thread::sleep_for(std::chrono::seconds(1));\n                exit(0);\n            }\n            else{\n                cout<<\"Node \"<<failedNode.getNodeName()<<\" failed, disseminating info\\n\";\n                failAndDisseminate(sockfd, failedNode, msg.getTTL()-1);\n            }\n        }\n    }\n\n    void aliveMessageProcessor(int sockfd, Message msg){\n        if(msg.getNodesList().empty()){\n            perror(\"No Node To Mark Alive\");\n        }\n        else {\n            Node aliveNode = msg.getNodesList().front();\n            if(aliveNode.getNodeId() == selfNode.getNodeId()){\n                return;\n            } else {\n                setAliveAndDisseminate(sockfd, aliveNode, msg.getTTL()-1);\n            }\n        }\n    }\n\n    void pingMessageProcessor(int sockfd, Message msg) {\n        Message ackMessage = Message::createAckMessageForPing(selfNode);\n        Node senderNode = msg.getSenderNode();\n        if(!membershipList.contains(senderNode)){\n            membershipList.addNode(senderNode);\n            updateListener({membershipList.getNode(senderNode.getNodeId())});\n        }\n        string serializedMessage = ackMessage.serializeMessage();\n        struct sockaddr_in senderNode_addr = getAddrFromNode(senderNode);\n        sendTo(sockfd, serializedMessage, senderNode_addr, MessageType::ACK, senderNode);\n    }\n\n    void toggleSuspicionMechanismAndDisseminate(int sockfd, bool isEnabled, int ttl) {\n        if(isSuspicionMechanismEnabled == isEnabled) {\n            return;\n        }\n        cout << getCurrentFullTimestamp() << \"Toggling suspicion mechanism to \" << isEnabled << endl;\n        isSuspicionMechanismEnabled = isEnabled;\n        if(ttl <= 0) {\n            return;\n        }\n        Message susMessage = Message::createSuspectMechanismMessage(selfNode, isEnabled, ttl);\n        disseminateMessage(sockfd, susMessage);\n    }\n\n    void leaveMessageProcessor(Message msg) {\n        Node leftNode = msg.getSenderNode();\n        if (membershipList.nodeExists(leftNode.getNodeId())) {\n            if (!(leftNode.getStatus() == Status::failed || leftNode.getStatus() == Status::left)) {\n                membershipList.markNodeLeft(leftNode);\n                updateListener({membershipList.getNode(leftNode.getNodeId())});\n            }\n        }\n    }\n\n    void joinMessageProcessor(int sockfd, Message msg){\n        vector<Node> nodes = membershipList.getAliveNodes();\n        nodes.push_back(selfNode);\n        Message ackMessage = Message::createAckMessage(selfNode, nodes);\n\n        // Add the sender node to the membership list\n        Node senderNode = msg.getSenderNode();\n        membershipList.addNode(senderNode);\n        updateListener({membershipList.getNode(senderNode.getNodeId())});\n\n        // Serialize the message\n        string serializedMessage = ackMessage.serializeMessage();\n        struct sockaddr_in senderNode_addr = getAddrFromNode(senderNode);\n        // Send the ACK message back to the sender\n        sendTo(sockfd, serializedMessage, senderNode_addr, MessageType::ACK, senderNode);\n\n        // sleep for 110ms to let everone know about the to let the node read the ack message\n        // usleep(110000);\n\n        // send current sus mechanism status to the new node\n        Message susMessage = Message::createSuspectMechanismMessage(selfNode, isSuspicionMechanismEnabled, 1);\n        string serializedSusMessage = susMessage.serializeMessage();\n        sendTo(sockfd, serializedSusMessage, senderNode_addr, susMessage.getType(), senderNode);\n    }\n\n    void suspectMessageProcessor(int sockfd, Message msg) {\n        if (msg.getNodesList().empty()) {\n            perror(\"No Node To Suspect\");\n        }\n        else{\n            Node suspectedNode = msg.getNodesList().front();\n            if(suspectedNode.getNodeId() == selfNode.getNodeId()){\n                cout<<getCurrentFullTimestamp()<<\": I am the one who is suspected\\n\";\n                if(suspectedNode.getIncarnationNumber() == selfNode.getIncarnationNumber()){\n                    selfNode.incrementIncarnationNumber();\n                    cout<<getCurrentFullTimestamp()<<\": Incremented incarnation number\\n\";\n                    selfNode.printNode();\n                    Message finalMessage = Message::createAliveMessage(selfNode, {selfNode}, TTL);\n                    disseminateMessage(sockfd, finalMessage);\n                }\n                // else{\n                //     Message finalMessage = Message::createAliveMessage(selfNode, {selfNode}, TTL);\n                //     disseminateMessage(sockfd, finalMessage);\n                // }\n            }\n            else{\n                suspectAndDisseminate(sockfd, msg.getNodesList().front(), msg.getTTL()-1);\n            }\n        }\n    }\n\n    void ackMessageProcessor(Message msg) {\n        Node senderNode = msg.getSenderNode();\n        membershipList.setLastUpdated(senderNode.getNodeId(), getCurrentTSinEpoch());\n        mtx_pingedNodes.lock();\n        pingedNodes.erase(senderNode.getNodeId());\n        mtx_pingedNodes.unlock();\n    }\n\n    // message handler\n    void infoMessageHandler(int sockfd) {\n        while (running) {\n            std::unique_lock<std::mutex> lock(mtx_qinfo);\n\n            // Wait for an informational message\n            if (cv_qinfo.wait_for(lock, std::chrono::seconds(1), [this] { return !infoQueue.empty(); })) {\n\n                Message msg = infoQueue.front();\n                infoQueue.pop();\n\n                switch(msg.getType()) {\n                    case MessageType::ACK : {\n                        ackMessageProcessor(msg);\n                        break;\n                    }\n                    case MessageType::JOIN : {\n                        joinMessageProcessor(sockfd, msg);\n                        break;\n                    }\n                    case MessageType::PING : {\n                        pingMessageProcessor(sockfd, msg);\n                        break;\n                    }\n                    case (MessageType::LEAVE) : {\n                        leaveMessageProcessor(msg);\n                        break;\n                    }\n                    case (MessageType::FAILED) : {\n                        failedMessageProcessor(sockfd, msg);\n                        break;\n                    }\n                    case (MessageType::ENABLE_SUSPECT_MECHANISM) : {\n                        toggleSuspicionMechanismAndDisseminate(sockfd, true, msg.getTTL()-1);\n                        break;\n                    }\n                    case (MessageType::DISABLE_SUSPECT_MECHANISM) : {\n                        toggleSuspicionMechanismAndDisseminate(sockfd, false, msg.getTTL()-1);\n                        break;\n                    }\n                    case (MessageType::SUSPECT) : {\n                        suspectMessageProcessor(sockfd, msg);\n                        break;\n                    }\n                    case (MessageType::ALIVE) : {\n                        aliveMessageProcessor(sockfd, msg);\n                        break;\n                    }\n                    default : {\n                        cout<<\"Default case\\n\";\n                        break;\n                    }\n                }\n            } else {\n                // Handle timeout case if needed\n            }\n        }\n    }\n\n    void listener(int sockfd) {\n        // start listening on socket sockfd\n        char recvBuffer[BUFFER_SIZE];\n        struct sockaddr_in senderAddr;\n        socklen_t senderAddrLen = sizeof(senderAddr);\n        cout<<\"Listener started\\n\";\n        struct timeval tv;\n        // TODO: Set timeout to 2 seconds\n        tv.tv_sec = 2;  // 2 seconds timeout\n        tv.tv_usec = 0; // 0 microseconds\n\n        if (setsockopt(sockfd, SOL_SOCKET, SO_RCVTIMEO, &tv, sizeof(tv)) < 0) {\n            perror(\"Error setting socket timeout\");\n        }\n\n        while(running) {\n\n            int numBytes = recvfrom(sockfd, recvBuffer, BUFFER_SIZE, 0, (struct sockaddr *)&senderAddr, &senderAddrLen);\n            if(numBytes == -1){\n                // perror(\"recvfrom listener\");\n                continue;\n            }\n            bytesReceived += numBytes;\n            recvBuffer[numBytes] = '\\0';\n\n            Message msg = Message::deserializeMessage(recvBuffer);\n            if( membershipList.nodeExists(msg.getSenderNode().getNodeId()) &&\n                membershipList.getNode(msg.getSenderNode().getNodeId()).getStatus() == Status::failed){\n                cout << getCurrentFullTimestamp() << \"Node \" << msg.getSenderNode().getNodeName() << \":\" << msg.getSenderNode().getPort() << \" is marked as failed, not processing the message\" << endl;\n                continue;\n            }\n            if (msg.getType() == MessageType::ACK && dropNextAck) {\n                dropNextAck = false;\n                cout << \"Dropped ACK message from \" << msg.getSenderNode().getNodeName() << \":\" << msg.getSenderNode().getPort() << endl;\n                continue;\n            }\n            // cout<< Node::getCurrentFullTimestamp() <<\": Received \" << messageTypeToString(msg.getType()) << \" message from \" << msg.getSenderNode().getNodeName() << \":\" << msg.getSenderNode().getPort() << endl;\n\n            std::unique_lock<std::mutex> lock(mtx_qinfo);\n            infoQueue.push(msg);\n            cv_qinfo.notify_all();\n\n        }\n    }\n\n    void pinger(int sockfd) {\n        while (running) {\n            // Get the list of nodes from the membership list\n            vector<Node> nodes = membershipList.getAliveShuffledNodes();\n\n            // Iterate over the shuffled nodes and send ping messages\n            for (const Node &node : nodes) {\n\n                if (!running) break;\n                if(membershipList.getStatus(node.getNodeId()) != Status::alive){\n                    continue;\n                }\n\n                Message pingMessage = Message::createPingMessage(selfNode);\n                string serializedMessage = pingMessage.serializeMessage();\n                sockaddr_in nodeAddr = getAddrFromNode(node);\n                sendTo(sockfd, serializedMessage, nodeAddr, MessageType::PING, node);\n\n                membershipList.setLastUpdated(node.getNodeId(), getCurrentTSinEpoch());\n                mtx_pingedNodes.lock();\n                pingedNodes.insert(node.getNodeId());\n                mtx_pingedNodes.unlock();\n            }\n            std::this_thread::sleep_for(std::chrono::milliseconds(CYCLE_TIME));\n        }\n    }\n\n    void timeoutValidator(int sockfd) {\n        while(running){\n            std::this_thread::sleep_for(std::chrono::milliseconds(PING_VALIDATOR_CYCLE_TIME));\n            mtx_pingedNodes.lock();\n            vector<string> currPingedNodes(pingedNodes.begin(), pingedNodes.end());\n            for (const string& nodeId : currPingedNodes) {\n\n                string lastUpdated = membershipList.getLastUpdated(nodeId);\n                if(lastUpdated == \"\"){\n                    continue;\n                }\n                // cout<<\"Timeout - \"<<Node::differenceWithCurrentEpoch(lastUpdated)<<endl;\n                if(differenceWithCurrentEpoch(lastUpdated) > PING_TIMEOUT) {\n                    pingedNodes.erase(nodeId);\n                    if(isSuspicionMechanismEnabled){\n                        cout<<getCurrentFullTimestamp()<<\": Suspecting node, I am deciding \"<<nodeId<<endl;\n                        suspectAndDisseminate(sockfd, membershipList.getNode(nodeId), TTL);\n                    } else {\n                        cout<<getCurrentFullTimestamp()<<\": Failing node,  I am deciding \"<<nodeId<<endl;\n                        failAndDisseminate(sockfd, membershipList.getNode(nodeId), TTL);\n                    }\n                }\n\n            }\n            mtx_pingedNodes.unlock();\n\n            if(isSuspicionMechanismEnabled){\n                mtx_suspectedNodes.lock();\n                vector<string> currSuspectedNodes(suspectedNodes.begin(), suspectedNodes.end());\n                for (const string& nodeId : currSuspectedNodes) {\n                    string lastUpdated = membershipList.getLastUpdated(nodeId);\n                    if(lastUpdated == \"\"){\n                        continue;\n                    }\n                    if(differenceWithCurrentEpoch(lastUpdated) > SUSPECT_TIMEOUT) {\n                        suspectedNodes.erase(nodeId);\n                        cout<<getCurrentFullTimestamp()<<\": Failing node after sus timeout,  I am deciding \"<<nodeId<<endl;\n                        failAndDisseminate(sockfd, membershipList.getNode(nodeId), TTL);\n                    }\n                }\n                mtx_suspectedNodes.unlock();\n            }\n\n        }\n    }\n\n    bool joinMembershipList(int sockfd, char *introducerName) {\n        // Create a JOIN message\n        Message joinMessage = Message::createJoinMessage(selfNode);\n\n        // Serialize the message\n        string serializedMessage = joinMessage.serializeMessage();\n\n        // Send the JOIN message to the introducer\n        struct addrinfo hints, *introducerAddr;\n        memset(&hints, 0, sizeof(hints));\n        hints.ai_family = AF_INET;\n        hints.ai_socktype = SOCK_DGRAM;\n\n        int status = getaddrinfo(introducerName, \"4560\", &hints, &introducerAddr);\n\n        if (status != 0) {\n            std::cerr << \"Error resolving hostname: \" << gai_strerror(status) << std::endl;\n            return false;\n        }\n\n        sendTo(sockfd, serializedMessage, *(struct sockaddr_in *)introducerAddr->ai_addr, MessageType::JOIN);\n\n\n        char recvBuffer[BUFFER_SIZE];\n        struct timeval tv;\n        tv.tv_sec = 5;  // 5 seconds timeout\n        tv.tv_usec = 0; // 0 microseconds\n\n        if (setsockopt(sockfd, SOL_SOCKET, SO_RCVTIMEO, &tv, sizeof(tv)) < 0) {\n            perror(\"Error setting socket timeout\");\n        }\n\n        int numBytesReceived = recvfrom(sockfd, recvBuffer, BUFFER_SIZE, 0, nullptr, nullptr);\n        recvBuffer[numBytesReceived] = '\\0';\n        if (numBytesReceived == -1) {\n            perror(\"recv\");\n            return false;\n        } else {\n            recvBuffer[numBytesReceived] = '\\0';\n            Message ackMessage = Message::deserializeMessage(recvBuffer);\n            vector<Node> nodes = ackMessage.getNodesList();\n            membershipList.addNodes(nodes);\n            updateListener({nodes});\n        }\n\n        freeaddrinfo(introducerAddr);\n        return true;\n    }\n\n\n\n\n\n\n    void sendLeaveMessage(int sockfd) {\n        // Create a LEAVE message\n        Message leaveMessage = Message::createLeaveMessage(selfNode);\n\n        // Serialize the message\n        string serializedMessage = leaveMessage.serializeMessage();\n\n        // Send the LEAVE message to all nodes in the membership list\n        vector<Node> nodes = membershipList.getAliveNodes();\n        for (const Node &node : nodes) {\n            struct sockaddr_in nodeAddr = getAddrFromNode(node);\n            sendTo(sockfd, serializedMessage, nodeAddr, MessageType::LEAVE, node);\n        }\n    }\n\npublic:\n\n\n    Daemon(char *_port, char* tcpPort) {\n\n\n        port = new char[strlen(_port) + 1];  // +1 for null terminator\n        strcpy(port, _port);\n\n        // convert isIntroducer to bool, False if NULL\n        // bool isIntroducerBool = isIntroducer == NULL ? false : true;\n        running = true;\n        isSuspicionMechanismEnabled = true;\n        bytesReceived = 0;\n        bytesSent = 0;\n        selfNode = Node::generateNode(port, tcpPort);\n\n        dropNextAck = false;\n        // start(isIntroducerBool, introducerName);\n    }\n\n    ~Daemon() {\n\n        cout<<\"Destructor called\\n\";\n        cout<<selfNode.getNodeId()<<endl;\n        delete[] port;\n\n    }\n\n    void addListener(HyDFS* listener){\n        dfsListener = listener;\n        dfsListener->setSelfNode(selfNode);\n    }\n\n    void updateListener(vector<Node> nodes){\n        dfsListener->membershipUpdate(nodes);\n    }\n\n    void start(bool isIntroducer, char *introducerName) {\n        cout<<\"Starting Daemon\\n\";\n        sockfd = setupSocket(port);\n        if(!isIntroducer){\n            if(!joinMembershipList(sockfd, introducerName)){\n                cout<<\"Failed to join membership list\\n\";\n                return;\n            } else {\n                cout<<\"Joined membership list\\n\";\n            }\n        }\n\n        std::thread messageHandlerThread(&Daemon::infoMessageHandler, this, sockfd);\n        threads.emplace_back(std::move(messageHandlerThread));\n        std::thread listenerThread(&Daemon::listener, this, sockfd);\n        threads.emplace_back(std::move(listenerThread));\n        std::thread pingingThread(&Daemon::pinger, this, sockfd);\n        threads.emplace_back(std::move(pingingThread));\n        std::thread timeoutValidatorThread(&Daemon::timeoutValidator, this, sockfd);\n        threads.emplace_back(std::move(timeoutValidatorThread));\n\n\n\n        std::cout << \"Daemon started on port \" << port << std::endl;\n    }\n\n    void stopThreads(){\n        running = false;\n        for (thread &t : threads) {\n            t.join();\n        }\n    }\n\n    void runCommand(string command){\n        // green cout\n        cout<<\"\\033[1;32m\";\n        if(command == \"leave\" ){\n            sendLeaveMessage(sockfd);\n            stopThreads();\n            close(sockfd);\n        } else if (command == \"show\") {\n            membershipList.printMembershipList();\n        } else if (command == \"enable_sus\") {\n            toggleSuspicionMechanismAndDisseminate(sockfd, true, TTL);\n        } else if (command == \"disable_sus\") {\n            toggleSuspicionMechanismAndDisseminate(sockfd, false, TTL);\n        } else if (command == \"status_sus\") {\n            std::cout << \"SUSPICION MECHANISM STATUS: \" << isSuspicionMechanismEnabled << std::endl;\n        } else if (command == \"list_mem\"){\n            membershipList.printMembershipList();\n        } else if (command == \"list_self\") {\n            cout<<\"Self Node ------- \\n\";\n            selfNode.printNode();\n        } else if (command == \"n\"){\n            cout<<\"Alive nodes count: \"<<membershipList.getAliveNodesCount()+1<<endl;\n        } else if (command == \"d\"){\n            dropNextAck = true;\n        } else if (command == \"b\"){\n            // color to blue\n            cout<<\"\\033[1;34m\";\n            // auto end = high_resolution_clock::now();\n            // auto duration = duration_cast<microseconds>(end - startTimestamp).count();\n            cout<<\"Bytes Received: \"<<bytesReceived<<endl;\n            cout<<\"Bytes Sent: \"<<bytesSent<<endl;\n            // cout<<\"Download bandwidth \" << bytesReceived/((std::chrono::duration_cast<std::chrono::seconds>(std::chrono::system_clock::now() - startTimestamp)).count()) << \" bytes/sec\" << endl;\n            // cout<<\"Upload bandwidth \" << bytesSent/((std::chrono::duration_cast<std::chrono::seconds>(std::chrono::system_clock::now() - startTimestamp)).count()) << \" bytes/sec\" << endl;\n            cout<<\"\\033[0m\";\n        } else if (command == \"s\"){\n            // color to green\n            cout<<\"\\033[1;32m\";\n            cout<<\"Suspected nodes count: \"<<suspectedNodes.size()<<endl;\n            mtx_suspectedNodes.lock();\n            for (const string& nodeId : suspectedNodes) {\n                cout << nodeId << endl;\n            }\n            mtx_suspectedNodes.unlock();\n            cout<<\"\\033[0m\";\n        } else {\n            dfsListener->runCommand(command);\n        }\n        // change color to default\n        cout<<\"\\033[0m\";\n\n    }\n};\n\n\n\n"
    },
    {
      "file_path": "FileMetaData.cpp",
      "content": "#include \"FileMetaData.h\"\n#include \"utils.h\"\n\n// Default constructor\nFileMetaData::FileMetaData() {}\n\n// Constructor implementation\n// FileMetaData::FileMetaData(string hydfsFileName)\n//     : hydfsFileName(hydfsFileName) {}\n\nFileMetaData::FileMetaData(string hydfsFileName, int version)\n    : hydfsFileName(hydfsFileName), version(version) {}\n\n// Copy constructor\nFileMetaData::FileMetaData(const FileMetaData& other)\n    : hydfsFileName(other.hydfsFileName), version(other.version) {}\n\n// Getter for hydfsFileName\nstring FileMetaData::getHydfsFileName() const {\n    return hydfsFileName;\n}\n\n// Getter for version\nint FileMetaData::getLatestVersion() const {\n    return version;\n}\n\n\n// Setter for hydfsFileName\nvoid FileMetaData::setHydfsFileName(string hydfsFileName) {\n    this->hydfsFileName = hydfsFileName;\n}\n\n// Setter for version\nvoid FileMetaData::setVersion(int version) {\n    this->version = version;\n}\n\n// assignment operator overloading\nFileMetaData& FileMetaData::operator=(const FileMetaData& other) {\n    if (this != &other) {\n        this->hydfsFileName = other.hydfsFileName;\n        this->version = other.version;\n    }\n    return *this;\n}\n\n// << operator overloading\nostream& operator<<(ostream& os, const FileMetaData& fileMetaData) {\n    os << \"FileMetaData: \" << fileMetaData.getHydfsFileName() << \" : \" << fileMetaData.getLatestVersion();\n    return os;\n}\n\n\n\n\n// Constructor implementation\nFileList::FileList() {\n    hashToFileName = {};\n    filenameToFileMetaData = {};\n}\n\n// Add file to the file list\n// void FileList::addFile(string fileName) {\n//     lock_guard<mutex> lock(mtx_fileList);\n//     hashToFileName[hashFunction(fileName)] = fileName;\n//     filenameToFileMetaData[fileName] = FileMetaData(fileName);\n// }\n\n// Add file to the file list\n// void FileList::addFile(string fileName, int version) {\n//     lock_guard<mutex> lock(mtx_fileList);\n//     if(filenameToFileMetaData.find(fileName) != filenameToFileMetaData.end()){\n//         filenameToFileMetaData[fileName].incrementVersion();\n//     } else {\n//         hashToFileName[hashFunction(fileName)] = fileName;\n//         filenameToFileMetaData[fileName] = FileMetaData(fileName, version);\n//     }\n// }\n\n\n\n// << operator overloading\n// ostream& operator<<(ostream& os, const FileList& fileList) {\n//     for (const auto& pair : fileList.hashToFileName) {\n//         os << pair.first << \" -> \" << pair.second << endl;\n//     }\n//     return os;\n// }\n\nostream& operator<<(ostream& os, FileList& fileList) {\n    for (auto& pair : fileList.hashToFileName) {\n        os << pair.first << \" -> \" << pair.second << \" : \"<< fileList.getFileVersion(pair.second) << endl;\n    }\n    // for (const auto& pair : fileList.filenameToFileMetaData) {\n    //     os << hashFunction(pair.first) << \" -> \" << pair.second << endl;\n    // }\n    return os;\n}\n\n// Check if file is present in the file list\nbool FileList::isFilePresent(string fileName) {\n    cout << \"Checking if file is present: \" << fileName << endl;\n    lock_guard<mutex> lock(mtx_fileList);\n    cout<<\"acquird lock\"<<endl;\n    return filenameToFileMetaData.find(fileName) != filenameToFileMetaData.end();\n}\n\n// Get files smaller than the given hash\nvector<int> FileList::getFilesInRange(int myHash, int nodeHash) {\n    cout << \"Getting files between \" << myHash << \" and \" << nodeHash << endl;\n    lock_guard<mutex> lock(mtx_fileList);\n    vector<int> files;\n    // consider the the map of file hash as a circular ring find all file hashes which come before the given hash in the ring\n    if(nodeHash < myHash){\n        for (const auto& pair : hashToFileName) {\n            if (pair.first <= nodeHash || pair.first > myHash) {\n                files.push_back(pair.first);\n            }\n        }\n    } else {\n        for (const auto& pair : hashToFileName) {\n            if (pair.first <= nodeHash && pair.first > myHash) {\n                files.push_back(pair.first);\n            }\n        }\n    }\n    return files;\n}\n\n// remove file from the file list\nvoid FileList::removeFile(int fileHash) {\n    lock_guard<mutex> lock(mtx_fileList);\n    string fileName = hashToFileName[fileHash];\n    filenameToFileMetaData.erase(fileName);\n    hashToFileName.erase(fileHash);\n}\n\n// Get file name from the hash\nstring FileList::getFileName(int hash) {\n    lock_guard<mutex> lock(mtx_fileList);\n    return hashToFileName[hash];\n}\n\n// Get file name list\nvector<pair<string, int>> FileList::getFileList() {\n    lock_guard<mutex> lock(mtx_fileList);\n    vector<pair<string, int>> fileList;\n    for (const auto& pair : filenameToFileMetaData) {\n        fileList.push_back({pair.first, pair.second.getLatestVersion()});\n    }\n    return fileList;\n}\n\n// Remove file from the file list\nvoid FileList::removeFile(string fileName) {\n    lock_guard<mutex> lock(mtx_fileList);\n    int fileHash = hashFunction(fileName);\n    filenameToFileMetaData.erase(fileName);\n    hashToFileName.erase(fileHash);\n}\n\n// Get FileMetaData given file name\nFileMetaData FileList::getFileMetaData(string fileName) {\n    lock_guard<mutex> lock(mtx_fileList);\n    return filenameToFileMetaData[fileName];\n}\n\n// get file version\nint FileList::getFileVersion(string fileName) {\n    lock_guard<mutex> lock(mtx_fileList);\n    if(filenameToFileMetaData.find(fileName) == filenameToFileMetaData.end()){\n        return 0;\n    }\n    return filenameToFileMetaData[fileName].getLatestVersion();\n}\n\nint FileMetaData::incrementVersion() {\n    return ++version;\n}\n\nint FileList::incrementFileVersion(string fileName) {\n    lock_guard<mutex> lock(mtx_fileList);\n    return filenameToFileMetaData[fileName].incrementVersion();\n}\n\nstd::mutex& FileMetaData::getLock() {\n    return mtx_fileName;\n}\n\nstd::mutex& FileList::getLockForFile(string file) {\n    lock_guard<mutex> lock(mtx_fileList);\n    return filenameToFileMetaData[file].getLock();\n}\n\n// Insert file into the file list\nvoid FileList::insertFile(string fileName) {\n    lock_guard<mutex> lock(mtx_fileList);\n    hashToFileName[hashFunction(fileName)] = fileName;\n    filenameToFileMetaData[fileName] = FileMetaData(fileName, 0);\n}\n\n// Transfer file to the file list\nvoid FileList::transferFileToList(string fileName, int version) {\n    lock_guard<mutex> lock(mtx_fileList);\n    if(filenameToFileMetaData.find(fileName) != filenameToFileMetaData.end()){\n        cout<<\"ERROR: File already exists\"<<endl;\n    } else {\n        hashToFileName[hashFunction(fileName)] = fileName;\n        filenameToFileMetaData[fileName] = FileMetaData(fileName, version);\n    }\n}"
    },
    {
      "file_path": "FileMetaData.h",
      "content": "#ifndef FILEMETADATA_H\n#define FILEMETADATA_H\n\n#include <iostream>\n#include <vector>\n#include <map>\n#include <string>\n#include <mutex>\n#include <unordered_map>\n\nusing namespace std;\n\nclass FileMetaData {\nprivate:\n    string hydfsFileName;\n    mutex mtx_fileName;\n    int version;\n\npublic:\n    // Constructor\n    FileMetaData();\n    // FileMetaData(string hydfsFileName);\n    FileMetaData(string hydfsFileName, int version);\n\n    // Getter functions\n    string getHydfsFileName() const;\n    int getLatestVersion() const;\n    int incrementVersion();\n\n    // Setter functions\n    void setHydfsFileName(string hydfsFileName);\n    void setVersion(int version);\n\n    // assignment operator overloading\n    FileMetaData& operator=(const FileMetaData& other);\n    // copy constructor\n    FileMetaData(const FileMetaData& other);\n\n    std::mutex& getLock();\n\n    // << operator overloading\n    friend ostream& operator<<(ostream& os, const FileMetaData& fileMetaData);\n\n};\n\n\nclass FileList {\nprivate:\n    unordered_map<string, FileMetaData> filenameToFileMetaData;\n    map<int, string> hashToFileName;\n    mutex mtx_fileList;\n\npublic:\n    // Constructor\n    FileList();\n\n    // Getter functions\n    // void addFile(string fileName);\n    void insertFile(string fileName);\n    void removeFile(int fileHash);\n    void removeFile(string fileName);\n    bool isFilePresent(string fileName);\n    vector<int> getFilesInRange(int myHash, int nodeHash);\n    string getFileName(int hash);\n    vector<pair<string, int>> getFileList();\n    FileMetaData getFileMetaData(string fileName);\n    int getFileVersion(string fileName);\n    int incrementFileVersion(string fileName);\n    void transferFileToList(string fileName, int version);\n\n    // << operator overloading\n    friend ostream& operator<<(ostream& os, FileList& fileList);\n    std::mutex& getLockForFile(string file);\n};\n\n#endif // FILEMETADATA_H"
    },
    {
      "file_path": "HyDFS.cpp",
      "content": "#include \"HyDFS.h\"\n#include \"HyDFSMessage.h\"\n#include \"utils.h\"\n#include <iostream>\n#include <fstream>\n#include <unistd.h>\n#include <cstring>\n#include <sys/stat.h>\n#include <dirent.h>\n#include <unordered_set>\n#include <sstream>\nusing namespace std;\n\n#define FILES_DIRECTORY \"./files/\"\n#define HYDFS_FILES_DIRECTORY \"/hydfs\"\n#define LOCAL_FILES_DIRECTORY \"/local\"\n#define REPLICATION_FACTOR 2\n#define MULTI_APPEND_LOCAL_FILE \"appendFile\"\n#define MERGE_INTERVAL 5\n\nstring getLocalFilesPath(string filesDirectory, string filename){\n    return filesDirectory + LOCAL_FILES_DIRECTORY + \"/\" + filename;\n}\n\nvoid deleteFile(string filepath){\n    if (remove(filepath.c_str()) != 0) {\n        std::cerr << \"Error deleting file: \" << strerror(errno) << std::endl;\n    } else {\n        std::cout << getCurrentFullTimestamp()<<\" : \" << \"File deleted: \" << filepath << std::endl;\n    }\n}\n\nstring getHyDFSFilesPath(string filesDirectory, string filename, int version){\n    return filesDirectory + HYDFS_FILES_DIRECTORY + \"/\" + filename + '$' + to_string(version);\n}\n\nbool fileExists(string localFilepath){\n    struct stat buffer;\n    return (stat(localFilepath.c_str(), &buffer) == 0);\n}\n\nbool copyFile(string source, string dest){\n    std::ifstream src(source, std::ios::binary);\n    std::ofstream dst(dest, std::ios::binary);\n    if (!src) {\n        std::cerr << \"Error copying file: \" << strerror(errno) << std::endl;\n        return false;\n    }\n    dst << src.rdbuf();\n    if (!src || !dst) {\n        std::cerr << \"Error moving file to primary directory: \" << strerror(errno) << std::endl;\n        return false;\n    } else {\n        std::cout << getCurrentFullTimestamp()<<\" : \" << \"File moved to primary directory: \" << dest << std::endl;\n        return true;\n    }\n}\n\nHyDFS::HyDFS(char *port) : cache(4000000) {\n    tcpPort = port;\n    std::cout << getCurrentFullTimestamp()<<\" : \" << \"HyDFS started on port: \" << tcpPort << std::endl;\n    bytesReceived = 0;\n    bytesSent = 0;\n    totalBytesReceived= 0;\n    totalBytesSent = 0;\n}\n\nHyDFS::~HyDFS() {\n    // std::string dirPath = filesDirectory;\n\n    // if (rmdir(dirPath.c_str()) == -1) {\n    //     std::cerr << \"Error deleting directory: \" << strerror(errno) << std::endl;\n    // } else {\n    //     std::cout << getCurrentFullTimestamp()<<\" : \" << \"Directory deleted: \" << dirPath << std::endl;\n    // }\n}\n\nvoid HyDFS::getFileHandler(HyDFSMessage message, int clientSockfd){\n\n    // cout << getCurrentFullTimestamp()<<\" : \"<<\"Get file handler invoked\\n\";\n    string fileName = message.getFileName();\n    int numberOfShards = primaryFilsList.getFileVersion(fileName);\n    if(numberOfShards == 0){\n        cout<<\"Getting from replica\\n\";\n        numberOfShards = secondaryFilesList.getFileVersion(fileName);\n    }\n    if(numberOfShards == 0){\n        cerr<<\"File does not exist\"<<endl;\n        return;\n    }\n    for (int shard = 1; shard <= numberOfShards; shard++) {\n        string filePath = getHyDFSFilesPath(filesDirectory, fileName, shard);\n        sendFileToSocket(filePath, clientSockfd);\n    }\n\n}\n\nvoid HyDFS::putFileHelper(string fileName, int version, int clientSockfd){\n    std::lock_guard<std::mutex> lock(primaryFilsList.getLockForFile(fileName));\n    HyDFSMessage response = HyDFSMessage::getFileNameMessage(HyDFSMessage::HyDFSMessageType::READY_TO_RECEIVE_FILE, fileName);\n    sendMessageToSocket(response, clientSockfd);\n    string hydfsFilepath = getHyDFSFilesPath(filesDirectory, fileName, version);\n\n    bool didReceive = receiveFileFromSocket(hydfsFilepath, clientSockfd, fileName, false);\n    if(didReceive){\n        // add file to primary files list\n\n        primaryFilsList.incrementFileVersion(fileName);\n        cout << getCurrentFullTimestamp()<<\" : \"<<\"New version of file \"<<fileName<<\" is \"<<primaryFilsList.getFileVersion(fileName)<<endl;\n    }\n}\n\n\nvoid HyDFS::putFileHandler(HyDFSMessage msg, int clientSockfd){\n    cout << getCurrentFullTimestamp()<<\" : \"<<\"Put file handler invoked\\n\";\n    string fileName = msg.getFileName();\n    int version = msg.getVersion();\n\n    cout << getCurrentFullTimestamp()<<\" : \"<<\"File name \"<<fileName<<\" version \"<<version<<endl;\n    if(!primaryFilsList.isFilePresent(fileName)){\n        if(version == 1){\n            cout << getCurrentFullTimestamp()<<\" : \"<<\"inserted file\"<<endl;\n            primaryFilsList.insertFile(fileName);\n            putFileHelper(fileName, version, clientSockfd);\n        } else {\n            cerr<<\"Invalid version\"<<endl;\n            HyDFSMessage response = HyDFSMessage::getFileNameMessage(HyDFSMessage::HyDFSMessageType::FILE_DOES_NOT_EXIST, fileName);\n            sendMessageToSocket(response, clientSockfd);\n            return;\n        }\n    } else {\n        int myVersion = primaryFilsList.getFileVersion(fileName);\n        if(version <= myVersion){\n            cerr<<\"Invalid version\"<<endl;\n            HyDFSMessage response = HyDFSMessage::getFileNameMessage(HyDFSMessage::HyDFSMessageType::FILE_ALREADY_EXISTS, fileName);\n            sendMessageToSocket(response, clientSockfd);\n            return;\n        } else {\n            putFileHelper(fileName, version, clientSockfd);\n        }\n    }\n}\n\nvoid HyDFS::appendFileHandler(HyDFSMessage message, int clientSockfd){\n    cout << getCurrentFullTimestamp()<<\" : \"<<\"Append file handler invoked\\n\";\n    string fileName = message.getFileName();\n    int version = primaryFilsList.getFileVersion(fileName);\n    cout << getCurrentFullTimestamp()<<\" : \"<<\"File name \"<<fileName<<\" version \"<<version<<endl;\n    if(!primaryFilsList.isFilePresent(fileName)){\n        cerr<<\"File does not exist\"<<endl;\n        cout << getCurrentFullTimestamp()<<\" : \"<<\"File does not exist \"<<primaryFilsList.getFileVersion(fileName)<<endl;\n        HyDFSMessage response = HyDFSMessage::getFileNameMessage(HyDFSMessage::HyDFSMessageType::FILE_DOES_NOT_EXIST, fileName);\n        sendMessageToSocket(response, clientSockfd);\n        return;\n    }\n\n    std::lock_guard<std::mutex> lock(primaryFilsList.getLockForFile(fileName));\n\n    string shardPath = getHyDFSFilesPath(filesDirectory, fileName, primaryFilsList.getFileVersion(fileName)+1);\n    HyDFSMessage response = HyDFSMessage::getFileNameMessage(HyDFSMessage::HyDFSMessageType::READY_TO_RECEIVE_FILE, fileName);\n    sendMessageToSocket(response, clientSockfd);\n    bool didReceive = receiveFileFromSocket(shardPath, clientSockfd, fileName, false);\n    if(didReceive){\n        cout << getCurrentFullTimestamp()<<\" : \"<<\"File received successfully\"<<endl;\n        primaryFilsList.incrementFileVersion(fileName);\n    }\n}\n\nvoid HyDFS::getPrimaryFileListHandler(HyDFSMessage message, int clientSockfd){\n    // cout << getCurrentFullTimestamp()<<\" : \"<<\"Get file list handler invoked\\n\";\n    vector<pair<string, int>> filesList = primaryFilsList.getFileList();\n    if(filesList.size() == 0){\n        // cout << getCurrentFullTimestamp()<<\" : \"<<\"No files in primary files list\\n\";\n    }\n    // cout << getCurrentFullTimestamp()<<\" : \"<<\"Size of file list: \"<<filesList.size()<<endl;\n    HyDFSMessage response = HyDFSMessage::getFileListMessage(HyDFSMessage::HyDFSMessageType::GET_FILE_LIST_RESPONSE, filesList);\n    sendMessageToSocket(response, clientSockfd);\n}\n\nvoid HyDFS::getShardHandler(HyDFSMessage message, int clientSockfd){\n    // cout << getCurrentFullTimestamp()<<\" : \"<<\"Get shard handler invoked\\n\";\n    string fileName = message.getFileName();\n    int version = message.getVersion();\n    int myVersion = primaryFilsList.getFileVersion(fileName);\n    if(version > myVersion){\n        cerr<<\"Invalid version\"<<endl;\n        HyDFSMessage response = HyDFSMessage::getFileNameMessage(HyDFSMessage::HyDFSMessageType::FILE_DOES_NOT_EXIST, fileName);\n        sendMessageToSocket(response, clientSockfd);\n        return;\n    }\n    string filePath = getHyDFSFilesPath(filesDirectory, fileName, version);\n    sendFileToSocket(filePath, clientSockfd);\n}\n\nvoid HyDFS::multiAppendHandler(HyDFSMessage message){\n    appendFile(MULTI_APPEND_LOCAL_FILE, message.getFileName());\n}\n\nvoid HyDFS::getFileAllListHandler(HyDFSMessage message, int clientSockfd){\n\n    vector<pair<string, int>> filesList = primaryFilsList.getFileList();\n    vector<pair<string, int>> secFileList = secondaryFilesList.getFileList();\n    filesList.insert(filesList.end(), secFileList.begin(), secFileList.end());\n    if(filesList.size() == 0){\n        // cout << getCurrentFullTimestamp()<<\" : \"<<\"No files in primary files list\\n\";\n    }\n    HyDFSMessage response = HyDFSMessage::getFileListMessage(HyDFSMessage::HyDFSMessageType::GET_FILE_LIST_RESPONSE, filesList);\n    sendMessageToSocket(response, clientSockfd);\n}\n\nvoid HyDFS::invalidateCacheHandler(HyDFSMessage message){\n    string fileName = message.getFileName();\n    cout<<\"Invalidating cache for file: \"<<fileName<<endl;\n    cache.invalidateFileInCache(fileName);\n}\n\nvoid HyDFS::handleClient(int clientSockfd) {\n    // std::cout << getCurrentFullTimestamp()<<\" : \" << \"Handling client\" << std::endl;\n    HyDFSMessage message = receiveMessageFromSocket(clientSockfd);\n\n    switch(message.getType()) {\n        case HyDFSMessage::HyDFSMessageType::GET_FILE:\n            std::cout << getCurrentFullTimestamp()<<\" : \" << \"GET_FILE\" << std::endl;\n            getFileHandler(message, clientSockfd);\n            break;\n        case HyDFSMessage::HyDFSMessageType::PUT_FILE:\n            std::cout << getCurrentFullTimestamp()<<\" : \" << \"PUT_FILE\" << std::endl;\n            putFileHandler(message, clientSockfd);\n            break;\n        case HyDFSMessage::HyDFSMessageType::GET_FILE_LIST:\n            // std::cout << getCurrentFullTimestamp()<<\" : \" << \"GET_FILE_LIST\" << std::endl;\n            getPrimaryFileListHandler(message, clientSockfd);\n            break;\n        case HyDFSMessage::HyDFSMessageType::APPEND_FILE:\n            std::cout << getCurrentFullTimestamp()<<\" : \" << \"APPEND_FILE\" << std::endl;\n            appendFileHandler(message, clientSockfd);\n            break;\n        case HyDFSMessage::HyDFSMessageType::GET_SHARD:\n            // std::cout << getCurrentFullTimestamp()<<\" : \" << \"GET_SHARD\" << std::endl;\n            getShardHandler(message, clientSockfd);\n            break;\n        case HyDFSMessage::HyDFSMessageType::MULTI_APPEND:\n            std::cout << getCurrentFullTimestamp()<<\" : \" << \"MULTI_APPEND\" << std::endl;\n            multiAppendHandler(message);\n            break;\n        case HyDFSMessage::HyDFSMessageType::INVALIDATE_CACHE:\n            // std::cout << \"INVALIDATE_CACHE\" << std::endl;\n            invalidateCacheHandler(message);\n            break;\n        case HyDFSMessage::HyDFSMessageType::MERGE_FILE:\n            std::cout << getCurrentFullTimestamp()<<\" : \" << \"MERGE_FILE\" << std::endl;\n            mergeFileHandler(message);\n            break;\n        case HyDFSMessage::HyDFSMessageType::GET_ALL_FILE_LIST:\n            std::cout << getCurrentFullTimestamp()<<\" : \" << \"GET_ALL_FILE_LIST\" << std::endl;\n            getFileAllListHandler(message, clientSockfd);\n            break;\n        default:\n            std::cout << getCurrentFullTimestamp()<<\" : \" << \"Invalid hydfs message type received - \" << HyDFSMessage::messageTypeToString(message.getType()) << std::endl;\n    }\n    close(clientSockfd);\n}\n\nvoid HyDFS::mergeFileHandler(HyDFSMessage message){\n\n    cout<< getCurrentFullTimestamp()<<\" : \"<<\"Merging file - \"<<message.getFileName()<<endl;\n    int fileHash = hashFunction(message.getFileName());\n    Node primaryNode = ring.getPrimaryNodeForFile(fileHash);\n    // cout<<\"Primary node: \"<<primaryNode.getNodeId()<<endl;\n    vector<int> replicaNodes = ring.getKSuccessorsOf(REPLICATION_FACTOR, primaryNode.getRingHash());\n    // cout<<\"Replica nodes: \"<<replicaNodes.size()<<endl;\n    for(int hash: replicaNodes){\n        Node replicaNode = ring.getNode(hash);\n        // cout<<\"Replica node: \"<<replicaNode.getNodeId()<<endl;\n        if(replicaNode.getNodeId() == selfNodeId){\n            // cout<<\"I am a replica for file\"<<message.getFileName()<<endl;\n            HyDFSMessage getListMessage = HyDFSMessage::getInformationalMessage(HyDFSMessage::HyDFSMessageType::GET_FILE_LIST);\n            int sockfd = sendMessageToNode(getListMessage, primaryNode);\n            if(sockfd == -1){\n                cerr<<\"Error sending message\"<<endl;\n                return;\n            }\n            HyDFSMessage response = receiveMessageFromSocket(sockfd);\n            vector<pair<string, int>> filesList = response.getFilesList();\n            // cout<<\"Files list size: \"<<filesList.size()<<endl;\n            for(auto file : filesList){\n                cout<<file.first<<\" \"<<file.second<<endl;\n                if(file.first == message.getFileName()){\n                    mergeFile(file.first, file.second, primaryNode);\n                    break;\n                }\n            }\n            // cout<<\"FIle not found\"<<endl;\n            break;\n        }\n    }\n\n\n\n}\n\nvoid HyDFS::membershipUpdate(std::vector<Node>& nodes) {\n    for(Node node : nodes) {\n        updateRing(node);\n    }\n}\n\nvoid HyDFS::updateRing(Node node) {\n    if(node.getStatus() == Status::alive) {\n        ring.addNode(node);\n    } else if (node.getStatus() == Status::failed || node.getStatus() == Status::left) {\n        ring.removeNode(node);\n    }\n\n}\n\n\nvoid HyDFS::listMyFiles() {\n    cout << getCurrentFullTimestamp()<<\" : \"<<\"File list for \"<< ring.getSelfNode().getRingHash()<<\" :\"<<endl;\n    cout << primaryFilsList;\n    cout << secondaryFilesList;\n\n}\n\nvoid HyDFS::runCommand(std::string command) {\n    if(command == \"c\"){\n        cout << getCurrentFullTimestamp()<<\" : \\n\"<<ring;\n    } else if(command == \"store\"){\n        listMyFiles();\n    }\n    // else if the command start with create\n    else if (command.find(\"create\") == 0) {\n        string hydfsFileName, localFileName;\n        stringstream ss(command);\n        string word;\n        int i = 0;\n        while(ss >> word){\n            if(i == 1){\n                localFileName = word;\n            } else if(i == 2){\n                hydfsFileName = word;\n            }\n            i++;\n        }\n\n        cout << getCurrentFullTimestamp()<<\" : \"<<localFileName<<\" \"<<hydfsFileName<<endl;\n        createFile(localFileName, hydfsFileName);\n    } else if (command.find(\"getfromreplica\") == 0){\n        string vmId, hydfsFileName, localFileName;\n        stringstream ss(command);\n        string word;\n        int i = 0;\n        while(ss >> word){\n            if(i == 1){\n                vmId = word;\n            } else if(i == 2){\n                hydfsFileName = word;\n            } else if(i == 3){\n                localFileName = word;\n            }\n            i++;\n        }\n        getFileFromReplica(vmId, hydfsFileName, localFileName);\n    }\n    else if (command.find(\"get\") == 0) {\n        string hydfsFileName, localFileName;\n        stringstream ss(command);\n        string word;\n        int i = 0;\n        while(ss >> word){\n            if(i == 1){\n                hydfsFileName = word;\n            } else if(i == 2){\n                localFileName = word;\n            }\n            i++;\n        }\n        requestFile(hydfsFileName, localFileName);\n    }\n\n    else if (command.find(\"append\") == 0) {\n        string hydfsFileName, localFileName;\n        stringstream ss(command);\n        string word;\n        int i=0;\n        while (ss >> word) {\n            if (i == 1) {\n                localFileName = word;\n            } else if (i == 2) {\n                hydfsFileName = word;\n            }\n            i++;\n        }\n        appendFile(localFileName, hydfsFileName);\n    }\n    else if (command == \"rls\") {\n        listMyFiles();\n    }\n    else if (command == \"cache\") {\n        cache.printCache();\n    }\n    else if (command == \"ping\") {\n        std::cout << getCurrentFullTimestamp()<<\" : \" << \"Pinging\" << std::endl;\n        mergeAllReplicas();\n    } else if (command == \"push\"){\n        monitorFileLists();\n    } else if (command.find(\"multiappend\") == 0) {\n        stringstream ss(command);\n        string word, hydfsFileName;\n        int numOfAppends = 0, numOfClients = 0;\n        int i = 0;\n        while (ss >> word) {\n            if (i == 1) {\n                hydfsFileName = word;\n            } else if (i == 2) {\n                try {\n                    numOfAppends = stoi(word);\n                } catch (const std::invalid_argument& e) {\n                    std::cerr << \"Invalid argument: \" << e.what() << std::endl;\n                } catch (const std::out_of_range& e) {\n                    std::cerr << \"Out of range: \" << e.what() << std::endl;\n                }\n            } else if (i == 3) {\n                try {\n                    numOfClients = stoi(word);\n                } catch (const std::invalid_argument& e) {\n                    std::cerr << \"Invalid argument: \" << e.what() << std::endl;\n                } catch (const std::out_of_range& e) {\n                    std::cerr << \"Out of range: \" << e.what() << std::endl;\n                }\n            }\n            i++;\n        }\n        multiAppend(hydfsFileName, numOfAppends, numOfClients);\n    }\n    else if (command == \"list_mem_ids\"){\n        cout << getCurrentFullTimestamp()<<\" : \\n\"<<ring;\n    } else if (command.find(\"ls\") == 0){\n        stringstream ss(command);\n        string word = \"\", hydfFilename;\n        int i = 0;\n        while(ss >> word){\n            if(i == 1){\n                hydfFilename = word;\n            }\n            i++;\n        }\n        listNodesHavingFile(hydfFilename);\n    } else if (command.find(\"merge\") == 0){\n        stringstream ss(command);\n        string word = \"\", hydfsFileName;\n        int i = 0;\n        while(ss >> word){\n            if(i == 1){\n                hydfsFileName = word;\n            }\n            i++;\n        }\n        mergeFile(hydfsFileName);\n    } else {\n        std::cout << getCurrentFullTimestamp()<<\" : \" << \"Invalid command\" << std::endl;\n    }\n}\n\nvoid HyDFS::getFileFromReplica(string vmId, string hydfsFileName, string localFileName){\n    cout<<\"Getting file \"<< hydfsFileName <<\"from replica \"<< vmId<< \" to \"<<localFileName<<endl;\n    vector<int> nodes = ring.getListOfNodes();\n    for(int nodeHash : nodes){\n        Node node = ring.getNode(nodeHash);\n        if(node.getNodeId() == vmId){\n            cout << getCurrentFullTimestamp()<<\" : \"<<\"Getting file from replica \"<<node.getNodeId()<<endl;\n            HyDFSMessage message = HyDFSMessage::getFileNameMessage(HyDFSMessage::HyDFSMessageType::GET_FILE, hydfsFileName);\n            int sockfd = sendMessageToNode(message, node);\n            if(sockfd == -1){\n                cerr<<\"Error sending message\"<<endl;\n                return;\n            }\n            string localFilePath = getLocalFilesPath(filesDirectory, localFileName);\n            receiveFileFromSocket(localFilePath, sockfd, hydfsFileName, false);\n            return;\n\n        }\n    }\n    cout << getCurrentFullTimestamp()<<\" : \"<<\"No replica found\"<<endl;\n}\n\nvoid HyDFS::mergeFile(string fileName) {\n    vector<int> nodes = ring.getListOfNodes();\n    HyDFSMessage message = HyDFSMessage::getFileNameMessage(HyDFSMessage::HyDFSMessageType::MERGE_FILE, fileName);\n    for(int nodeId : nodes){\n        Node node = ring.getNode(nodeId);\n        if(node.getNodeId() == selfNodeId){\n            mergeFileHandler(message);\n        } else {\n            int sockfd = sendMessageToNode(message, node);\n            if(sockfd == -1){\n                cerr<<\"Error sending message\"<<endl;\n                return;\n            }\n            close(sockfd);\n        }\n\n    }\n}\n\nunordered_set<string> HyDFS::getAllFileListFromNode(Node node){\n    vector<pair<string, int>> filesList;\n    if(node.getNodeId() == selfNodeId){\n        filesList = primaryFilsList.getFileList();\n        vector<pair<string, int>> secList = secondaryFilesList.getFileList();\n        filesList.insert(filesList.end(), secList.begin(), secList.end());\n    } else {\n        HyDFSMessage message = HyDFSMessage::getInformationalMessage(HyDFSMessage::HyDFSMessageType::GET_ALL_FILE_LIST);\n        int sockfd = sendMessageToNode(message, node);\n        if(sockfd == -1){\n            cerr<<\"Error sending message\"<<endl;\n            return {};\n        }\n        HyDFSMessage response = receiveMessageFromSocket(sockfd);\n        filesList = response.getFilesList();\n\n    }\n    unordered_set<string> files;\n    for(auto file : filesList){\n        files.insert(file.first);\n    }\n    return files;\n}\n\nvoid HyDFS::listNodesHavingFile(string fileName){\n    int fileHash = hashFunction(fileName);\n    Node primaryNode = ring.getPrimaryNodeForFile(fileHash);\n    cout << getCurrentFullTimestamp()<<\" : \"<<\"Nodes for file \"<<fileName<<\", hash = \"<<fileHash<<endl;\n    unordered_set<string> files = getAllFileListFromNode(primaryNode);\n    if(files.find(fileName) == files.end()){\n        cout<<\"File not present in HyDFS\"<<endl;\n        return;\n    }\n    vector<int> nodes = ring.getKSuccessorsOf(REPLICATION_FACTOR, primaryNode.getRingHash());\n    cout << getCurrentFullTimestamp()<<\" : \"<<primaryNode.getRingHash()<<\"->\"<<primaryNode.getNodeId()<<endl;\n    for(int nodeId : nodes){\n        Node node = ring.getNode(nodeId);\n        unordered_set<string> files = getAllFileListFromNode(node);\n        if(files.find(fileName) != files.end()){\n            cout << getCurrentFullTimestamp()<<\" : \"<<node.getRingHash()<<\"->\"<<node.getNodeId()<<endl;\n        }\n    }\n}\n\nvoid HyDFS::multiAppend(string hydfsFileName, int numOfAppends, int numOfClients){\n\n    sendInvalidateCacheMessage(hydfsFileName);\n\n    vector<int> nodes = ring.getListOfNodes();\n    // shrink nodes to number of clients\n    while(nodes.size() > numOfClients){\n        nodes.pop_back();\n    }\n    for(int i = 0; i < numOfAppends; i++){\n        int nodeIndex = i % nodes.size();\n        Node node = ring.getNode(nodes[nodeIndex]);\n\n        if(node.getNodeId() == selfNodeId){\n            appendFile(MULTI_APPEND_LOCAL_FILE, hydfsFileName);\n        } else {\n            HyDFSMessage message = HyDFSMessage::getFileNameMessage(HyDFSMessage::HyDFSMessageType::MULTI_APPEND, hydfsFileName);\n            int sockfd = sendMessageToNode(message, node);\n            if(sockfd == -1){\n                cerr<<\"Error sending message\"<<endl;\n                return;\n            }\n            close(sockfd);\n        }\n\n    }\n}\n\nvoid createDirectorty(string dirPath){\n    if (mkdir(dirPath.c_str(), 0777) == -1) {\n        std::cerr << \"Error creating directory: \" << strerror(errno) << std::endl;\n    } else {\n        std::cout << getCurrentFullTimestamp()<<\" : \" << \"Directory created: \" << dirPath << std::endl;\n    }\n}\n\nvoid HyDFS::setSelfNode(Node node) {\n    selfNodeId = node.getNodeId();\n    ring.addNode(node);\n    cout << getCurrentFullTimestamp()<<\" : \"<<\"Self node id: \"<<selfNodeId<<endl;\n}\n\n\n\n\nvoid HyDFS::tcpListener(int tcpSockfd) {\n    std::vector<std::thread> clientThreads;\n\n    if (listen(tcpSockfd, 10) < 0) {\n        std::cerr << \"Listen failed.\\n\";\n        close(tcpSockfd);\n        return;\n    }\n\n    while(threadsRunning) {\n        struct sockaddr_in clientAddr;\n        socklen_t clientAddrLen = sizeof(clientAddr);\n\n        int clientSockfd = accept(tcpSockfd, (struct sockaddr *)&clientAddr, &clientAddrLen);\n        if(clientSockfd < 0) {\n            std::cerr << \"Error accepting connection\" << std::endl;\n            continue;\n        }\n        // std::cout << getCurrentFullTimestamp()<<\" : \" << \"Connection accepted\" << std::endl;\n\n        std::thread clientThread(&HyDFS::handleClient, this, clientSockfd);\n        clientThreads.emplace_back(std::move(clientThread));\n    }\n\n    for (std::thread &t : clientThreads) {\n        t.join();\n    }\n\n\n}\n\nvoid HyDFS::requestFile(string hydfsFileName, string localFileName){\n\n    auto start = std::chrono::high_resolution_clock::now();\n\n    string localFilePath = getLocalFilesPath(filesDirectory, localFileName);\n    // string primaryFilePath = getHyDFSFilesPath(filesDirectory, hydfsFileName);\n\n    char *fileData = nullptr;\n    size_t contentSize = 0;\n    bool inCache = cache.getFileFromCache(hydfsFileName, fileData, contentSize);\n    cout<<\"File in cache: \"<<inCache<<endl;\n    if (inCache) {\n        std::ofstream localFile(localFilePath, std::ios::binary | std::ios::app);\n        if (localFile.is_open()) {\n            localFile.write(fileData, contentSize);\n            localFile.close();\n            delete[] fileData;\n            std::cout << \"File retrieved from cache: \" << localFilePath << std::endl;\n            auto end = std::chrono::high_resolution_clock::now();\n            std::chrono::duration<double> elapsed = end - start;\n            std::cout << \"Time taken to retrieve file: \" << elapsed.count() << \"s\" << std::endl;\n            return;\n        } else {\n            std::cerr << \"Error opening local file: \" << localFilePath << std::endl;\n        }\n    }\n\n    int fileHash = hashFunction(hydfsFileName);\n    cout << getCurrentFullTimestamp()<<\" : \"<<\"hash of file: \"<<hydfsFileName<<\" is \"<<fileHash<<endl;\n\n    Node primaryNode = ring.getPrimaryNodeForFile(fileHash);\n\n    if(selfNodeId == primaryNode.getNodeId()){\n        int maxVersion = primaryFilsList.getFileMetaData(hydfsFileName).getLatestVersion();\n        cout << getCurrentFullTimestamp()<<\" : \"<<\"Mapped to self node\"<<endl;\n        std::ofstream dst(localFilePath, std::ios::binary | std::ios::trunc);\n        vector<char> buffer;\n        for (int version = 1; version <= maxVersion; version++) {\n            string chunkFilePath = getHyDFSFilesPath(filesDirectory, hydfsFileName, version);\n            // cout << getCurrentFullTimestamp()<<\" : \"<<chunkFilePath<<endl;\n            std::ifstream src(chunkFilePath, std::ios::binary);\n            dst << src.rdbuf();\n            buffer.insert(buffer.end(), std::istreambuf_iterator<char>(src), std::istreambuf_iterator<char>());\n        }\n        dst.close();\n        // cache.addFileToCache(hydfsFileName, buffer.data(), buffer.size());\n        dst.close();\n        // copyFile(primaryFilePath, localFilePath);\n        auto end = std::chrono::high_resolution_clock::now();\n        std::chrono::duration<double> elapsed = end - start;\n        std::cout << \"Time taken to retrieve file: \" << elapsed.count() << \"s\" << std::endl;\n        return;\n    }\n\n    cout << getCurrentFullTimestamp()<<\" : \"<<\"Requesting file from the primary node for file: \"<<hydfsFileName<<\" is \"<<primaryNode.getNodeId()<<endl;\n    receiveFileFromNode(localFilePath, primaryNode, hydfsFileName);\n    auto end = std::chrono::high_resolution_clock::now();\n    std::chrono::duration<double> elapsed = end - start;\n    std::cout << \"Time taken to retrieve file: \" << elapsed.count() << \"s\" << std::endl;\n}\n\nvoid HyDFS::receiveFileFromNode(string filepath, Node node, string hydfsFilename){\n    HyDFSMessage message = HyDFSMessage::getFileNameMessage(HyDFSMessage::HyDFSMessageType::GET_FILE, hydfsFilename);\n    int sockfd = sendMessageToNode(message, node);\n    if(sockfd == -1){\n        cerr<<\"Error sending message\"<<endl;\n        return;\n    }\n    receiveFileFromSocket(filepath, sockfd, hydfsFilename, true);\n    close(sockfd);\n}\n\nbool HyDFS::receiveFileFromSocket(string filepath, int sockfd, string fileName, bool forCache) {\n    std::ofstream\n    file(filepath, std::ios::out | std::ios::binary);\n    if (!file.is_open()) {\n        std::cerr << \"Error creating file: \" << strerror(errno) << std::endl;\n        close(sockfd);\n        return false;\n    }\n\n    vector<char> cachebuffer;\n    char buffer[1024] = {0};\n    int bytesRead = 0;\n    while ((bytesRead = recv(sockfd, buffer, sizeof(buffer), 0)) > 0) {\n        file.write(buffer, bytesRead);\n        if (forCache) {\n            cachebuffer.insert(cachebuffer.end(), buffer, buffer + bytesRead);\n        }\n    }\n    if (forCache)\n        cache.addFileToCache(fileName, cachebuffer.data(), cachebuffer.size());\n    file.close();\n    close(sockfd);\n    std::cout << getCurrentFullTimestamp()<<\" : \" << \"File received: \" << filepath << std::endl;\n    return true;\n}\n\nvoid HyDFS::createFile(string localFilename, string hydfsFilename){\n    string localFilepath = getLocalFilesPath(filesDirectory, localFilename);\n\n    int hash = hashFunction(hydfsFilename);\n    Node targetNode = ring.getPrimaryNodeForFile(hash);\n    int version = 1;\n\n    // check if file is present in local file path\n    if(!fileExists(localFilepath)){\n        std::cerr << \"Error: local File does not exist: \" << localFilename << std::endl;\n        return;\n    }\n\n    if(selfNodeId == targetNode.getNodeId()){\n        if(primaryFilsList.isFilePresent(hydfsFilename)){\n            std::cerr << \"Error: File already exists: \" << hydfsFilename << std::endl;\n            return;\n        }\n        primaryFilsList.insertFile(hydfsFilename);\n        std::lock_guard<std::mutex> lock(primaryFilsList.getLockForFile(hydfsFilename));\n        int newVersion = version;\n        std::string hydfsFilePath = getHyDFSFilesPath(filesDirectory, hydfsFilename, newVersion);\n        bool success = copyFile(localFilepath, hydfsFilePath);\n        if(success){\n            primaryFilsList.incrementFileVersion(hydfsFilename);\n        }\n        return;\n    }\n\n    // string hydfsFileShard = hydfsFilename + \"$\" + to_string(version);\n\n    cout << getCurrentFullTimestamp()<<\" : \"<<\"Primary node for file: \"<<hydfsFilename<<\" is \"<<targetNode.getNodeId()<<endl;\n    sendFileToNode(localFilepath, targetNode, hydfsFilename, version);\n    return;\n}\n\nvoid HyDFS::sendFileToNode(string filepath, Node node, string hydfsFilename, int version) {\n\n    HyDFSMessage message = HyDFSMessage::getFileNameVersionMessage(HyDFSMessage::HyDFSMessageType::PUT_FILE, hydfsFilename, version);\n    // cout << getCurrentFullTimestamp()<<\" : \"<<\"Sending message to node: \"<<node.getNodeId()<<endl;\n    int sockfd = sendMessageToNode(message, node);\n    if(sockfd == -1){\n        cerr<<\"Error sending message\"<<endl;\n        return;\n    }\n\n    HyDFSMessage responseMessage = receiveMessageFromSocket(sockfd);\n\n    if (responseMessage.getType() == HyDFSMessage::HyDFSMessageType::READY_TO_RECEIVE_FILE) {\n        cout << getCurrentFullTimestamp()<<\" : \"<<\"sending the files\"<<endl;\n        sendFileToSocket(filepath, sockfd);\n    } else {\n        cerr<<\"Error sending file to node - \"<<responseMessage.getTypeString()<<endl;\n    }\n\n    close(sockfd);\n}\n\nHyDFSMessage HyDFS::receiveMessageFromSocket(int sockfd){\n    char buffer[1024] = {0};\n    int bytesRead = recv(sockfd, buffer, sizeof(buffer) - 1, 0);\n    if (bytesRead < 0) {\n        std::cerr << \"Error reading from socket: \" << strerror(errno) << std::endl;\n        return HyDFSMessage::getInformationalMessage(HyDFSMessage::HyDFSMessageType::ERROR_RECEIVING_MESSAGE);\n    }\n    totalBytesReceived += bytesRead;\n    bytesReceived += bytesRead;\n    buffer[bytesRead] = '\\0';\n    std::string bufferStr(buffer);\n    HyDFSMessage message = HyDFSMessage::deserializeHyDFSMessage(bufferStr);\n    // cout << getCurrentFullTimestamp()<<\" : \"<<\"Received message \"<< message.getTypeString() <<\" from sockfd: \"<<sockfd<<endl;\n    return message;\n}\n\nvoid HyDFS::sendFileToSocket(string filepath, int sockfd){\n    FILE *file = fopen(filepath.c_str(), \"rb\");\n    if (file == NULL) {\n        std::cerr << \"Error opening file: \" << strerror(errno) << std::endl;\n        return;\n    }\n\n    char buffer[1024] = {0};\n    int bytesRead = 0;\n    while ((bytesRead = fread(buffer, 1, sizeof(buffer), file)) > 0) {\n        if (send(sockfd, buffer, bytesRead, 0) < 0) {\n            std::cerr << \"Error sending file: \" << strerror(errno) << std::endl;\n            break;\n        }\n        totalBytesSent += bytesRead;\n        bytesSent += bytesRead;\n    }\n    fclose(file);\n}\n\nint HyDFS::sendMessageToNode(HyDFSMessage message, Node node){\n    // cout << getCurrentFullTimestamp()<<\" : \"<<\"Sending message \"<< message.getTypeString() <<\" to node: \"<<node.getNodeId()<<endl;\n    int sockfd = getSocketForNode(node);\n    if(sockfd == -1){\n        cerr<<\"Error getting socket for node\"<<endl;\n        return -1;\n    }\n    sendMessageToSocket(message, sockfd);\n    return sockfd;\n}\n\nint HyDFS::getSocketForNode(Node node){\n    struct sockaddr_in nodeAddr = getTCPAddrFromNode(node);\n    int sockfd = socket(AF_INET, SOCK_STREAM, 0);\n    if (sockfd < 0) {\n        std::cerr << \"Error creating socket: \" << strerror(errno) << std::endl;\n        return -1;\n    }\n\n    if (connect(sockfd, (struct sockaddr *)&nodeAddr, sizeof(nodeAddr)) < 0) {\n        std::cerr << \"Error connecting to node: \" << strerror(errno) << std::endl;\n        close(sockfd);\n        return -1;\n    }\n    return sockfd;\n}\n\nvoid HyDFS::sendMessageToSocket(HyDFSMessage message, int sockfd){\n    // cout << getCurrentFullTimestamp()<<\" : \"<<\"Sending message \"<< message.getTypeString() <<\" to sockfd: \"<<sockfd<<endl;\n    string serializedMessage = message.serializeMessage();\n    int bytes = 0;\n    if ((bytes = send(sockfd, serializedMessage.c_str(), serializedMessage.size(), 0)) < 0) {\n        std::cerr << \"Error sending message: \" << strerror(errno) << std::endl;\n    }\n    totalBytesSent += bytes;\n    bytesSent += bytes;\n}\n\nvoid HyDFS::sendInvalidateCacheMessage(string hydfsFileName){\n    cache.invalidateFileInCache(hydfsFileName);\n    vector<int> nodes = ring.getListOfNodes();\n    for (int nodeId : nodes) {\n        Node node = ring.getNode(nodeId);\n        if (node.getNodeId() != selfNodeId) {\n            cout<<\"Sending cache invalidation message to node: \"<<node.getNodeId()<<endl;\n            HyDFSMessage message = HyDFSMessage::getFileNameMessage(HyDFSMessage::HyDFSMessageType::INVALIDATE_CACHE, hydfsFileName);\n            int sockfd = sendMessageToNode(message, node);\n            if (sockfd == -1) {\n                cerr << \"Error sending cache invalidation message to node \" << node.getNodeId() << endl;\n                continue;\n            }\n            close(sockfd);\n        }\n    }\n}\n\nvoid HyDFS::appendFile(string localFileName, string hydfsFileName){\n\n    string localFilePath = getLocalFilesPath(filesDirectory, localFileName);\n    // cout << getCurrentFullTimestamp()<<\" : \" << \"checking if file exists: \" << localFilePath << endl;\n    if(!fileExists(localFilePath)){\n        std::cerr << \"Error: local File does not exist: \" << localFileName << std::endl;\n        return;\n    }\n\n    sendInvalidateCacheMessage(hydfsFileName);\n\n    int fileHash = hashFunction(hydfsFileName);\n\n    Node primaryNode = ring.getPrimaryNodeForFile(fileHash);\n\n    if(selfNodeId == primaryNode.getNodeId()){\n        cout << getCurrentFullTimestamp()<<\" : \"<<\"Mapped to self node\"<<endl;\n        if (!primaryFilsList.isFilePresent(hydfsFileName)) {\n            std::cerr <<  \"File \" << hydfsFileName << \" is not present in primary node\" << std::endl;\n            return;\n        }\n\n        // lock file\n        std::lock_guard<std::mutex> lock(primaryFilsList.getLockForFile(hydfsFileName));\n        int currentVersion = primaryFilsList.getFileVersion(hydfsFileName);\n        string versionedFilePath = getHyDFSFilesPath(filesDirectory, hydfsFileName, currentVersion+1);\n        bool success = copyFile(localFilePath, versionedFilePath);\n        if(success){\n            primaryFilsList.incrementFileVersion(hydfsFileName);\n        }\n\n        // unlock file\n\n        return;\n    } else {\n        appendFileToNode(localFilePath, primaryNode, hydfsFileName);\n    }\n\n\n}\n\nvoid HyDFS::appendFileToNode(string filepath, Node node, string hydfsFileName) {\n    HyDFSMessage message = HyDFSMessage::getFileNameMessage(HyDFSMessage::HyDFSMessageType::APPEND_FILE, hydfsFileName);\n    int sockfd = sendMessageToNode(message, node);\n    if(sockfd == -1){\n        cerr<<\"Error sending message\"<<endl;\n        return;\n    }\n\n    HyDFSMessage response = receiveMessageFromSocket(sockfd);\n    if(response.getType() == HyDFSMessage::HyDFSMessageType::READY_TO_RECEIVE_FILE){\n        sendFileToSocket(filepath, sockfd);\n    } else {\n        cerr<<\"Error sending file to node - \"<<response.getTypeString()<<endl;\n    }\n\n    close(sockfd);\n}\n\n\nvoid HyDFS::start() {\n    startEpoch = getCurrentTSinEpoch();\n    threadsRunning = true;\n    cout << getCurrentFullTimestamp()<<\" : \"<<\"Starting HyDFS tcp listener\"<<endl;\n    tcpSocket = setupTCPSocket(tcpPort);\n    std::thread listenerThread(&HyDFS::tcpListener, this, tcpSocket);\n    threads.emplace_back(std::move(listenerThread));\n    std::thread predecessorPingerThread(&HyDFS::predecessorPinger, this);\n    threads.emplace_back(std::move(predecessorPingerThread));\n    std::thread monitorFileListsThread(&HyDFS::monitorFileLists, this);\n    threads.emplace_back(std::move(monitorFileListsThread));\n}\n\nvoid HyDFS::printBandwidth(){\n    cout << getCurrentFullTimestamp()<<\" : \"<<\"Total bytes sent: \"<<totalBytesSent<<endl;\n    cout << getCurrentFullTimestamp()<<\" : \"<<\"Total bytes received: \"<<totalBytesReceived<<endl;\n    double diff = differenceWithCurrentEpoch(startEpoch);\n    cout << getCurrentFullTimestamp()<<\" : \"<<\"Upload bandwidth: \"<<(double)totalBytesSent*1000/(diff*1024*1024)<<endl;\n    cout << getCurrentFullTimestamp()<<\" : \"<<\"Download bandwidth: \"<<(double)totalBytesReceived*1000/(diff*1024*1024)<<endl;\n}\n\n\nvoid HyDFS::setLocalFilesPath(string path) {\n    if(path == \"\"){\n        std::string dirPath = FILES_DIRECTORY + selfNodeId;\n        createDirectorty(dirPath);\n        // create three subdirectories primary, replica, local\n        std::string hydfsDirPath = dirPath + HYDFS_FILES_DIRECTORY;\n        createDirectorty(hydfsDirPath);\n\n        std::string localDirPath = dirPath + LOCAL_FILES_DIRECTORY;\n        createDirectorty(localDirPath);\n\n        filesDirectory = dirPath;\n    } else {\n        filesDirectory = path;\n    }\n}\n\nvoid HyDFS::stopThreads() {\n    threadsRunning = false;\n    for (std::thread &t : threads) {\n        t.join();\n    }\n}\n\nvoid HyDFS::mergeFile(string fileName, int numberOfShards, Node fromNode){\n    string file = fileName;\n    int version = numberOfShards;\n    int myVersion = secondaryFilesList.getFileVersion(file);\n    if(version > myVersion){\n        cout << getCurrentFullTimestamp()<<\" : \"<<\"Merging file \"<<file<<\" from node \"<<fromNode.getNodeId()<<endl;\n        string start = getCurrentTSinEpoch();\n        bytesReceived = 0;\n        bytesSent = 0;\n        if(myVersion == 0){\n            secondaryFilesList.insertFile(file);\n        }\n        std::lock_guard<std::mutex> lock(secondaryFilesList.getLockForFile(file));\n        for(int i = myVersion + 1; i <= version; i++){\n            // cout << getCurrentFullTimestamp()<<\" : \"<<\"Requesting shard \"<<i<<\" of file \"<<file<<\" out of \"<<version<<endl;\n            HyDFSMessage message = HyDFSMessage::getFileNameVersionMessage(HyDFSMessage::HyDFSMessageType::GET_SHARD, file, i);\n            int sockfd = sendMessageToNode(message, fromNode);\n            if(sockfd == -1){\n                cerr<<\"Error sending message\"<<endl;\n                return;\n            }\n            bool didReceive = receiveFileFromSocket(getHyDFSFilesPath(filesDirectory, file, i), sockfd, file, false);\n            if(didReceive){\n                secondaryFilesList.incrementFileVersion(file);\n            } else {\n                cout << getCurrentFullTimestamp()<<\" : \"<<\"Error receiving file list from predecessor: \"<<fromNode.getNodeId()<<endl;\n            }\n        }\n        double diff = differenceWithCurrentEpoch(start);\n        cout << getCurrentFullTimestamp()<<\" : \"<<\"Time taken to merge file \"<<file<<\" from node \"<<fromNode.getNodeId()<<\" is \"<<diff/1000<<endl;\n        cout << getCurrentFullTimestamp()<<\" : \"<<\"Completed merge \"<<file<<\" from node \"<<fromNode.getNodeId()<<endl;\n        printCurrentBandwidth(diff);\n        // printBandwidth();\n    }\n}\n\nvoid HyDFS::printCurrentBandwidth(double diff){\n    cout << getCurrentFullTimestamp()<<\" : \"<<\"Bytes sent: \"<<bytesSent<<endl;\n    cout << getCurrentFullTimestamp()<<\" : \"<<\"Bytes received: \"<<bytesReceived<<endl;\n    cout << getCurrentFullTimestamp()<<\" : \"<<\"curr upload bandwidth: \"<<(double)bytesSent*1000/(diff*1024*1024)<<endl;\n    cout << getCurrentFullTimestamp()<<\" : \"<<\"curr download bandwidth: \"<<(double)bytesReceived*1000/(diff*1024*1024)<<endl;\n}\n\nvoid HyDFS::mergeAllReplicas(){\n    vector<int> predecessors = ring.getKPredecessors(REPLICATION_FACTOR);\n\n        for(int predecessor : predecessors){\n            Node node = ring.getNode(predecessor);\n            HyDFSMessage message = HyDFSMessage::getInformationalMessage(HyDFSMessage::HyDFSMessageType::GET_FILE_LIST);\n            int clientSockfd = sendMessageToNode(message, node);\n            if(clientSockfd == -1){\n                cerr<<\"Error sending message\"<<endl;\n                return;\n            }\n            HyDFSMessage response = receiveMessageFromSocket(clientSockfd);\n            if(response.getType() == HyDFSMessage::HyDFSMessageType::GET_FILE_LIST_RESPONSE){\n                // cout << getCurrentFullTimestamp()<<\" : \"<<\"Received file list from predecessor: \"<<node.getNodeId()<<endl;\n                vector<pair<string, int>> filesList = response.getFilesList();\n                // cout << getCurrentFullTimestamp()<<\" : \"<<\"Size of file list: \"<<filesList.size()<<endl;\n                for(auto fileVersion : filesList){\n                    mergeFile(fileVersion.first, fileVersion.second, node);\n                }\n            } else {\n                cout << getCurrentFullTimestamp()<<\" : \"<<\"Error receiving file list from predecessor: \"<<node.getNodeId()<<endl;\n            }\n        }\n}\n\nvoid HyDFS::predecessorPinger() {\n    while(threadsRunning) {\n        sleep(MERGE_INTERVAL);\n        mergeAllReplicas();\n    }\n}\n\n\n\nvoid HyDFS::monitorFileLists(){\n    while(threadsRunning){\n        sleep(MERGE_INTERVAL);\n\n        vector<pair<string, int>> primaryFiles = primaryFilsList.getFileList();\n        vector<pair<string, int>> secondaryFiles = secondaryFilesList.getFileList();\n        vector<int> predecessors = ring.getKPredecessors(REPLICATION_FACTOR);\n        unordered_set<int> predecessorSet(predecessors.begin(), predecessors.end());\n\n        for(auto& fileVersion : primaryFiles){\n            string fileName = fileVersion.first;\n            int fileHash = hashFunction(fileName);\n            Node targetNode = ring.getPrimaryNodeForFile(fileHash);\n\n            if(targetNode.getRingHash() == ring.getSelfNode().getRingHash()){\n                continue;\n            } else {\n                bool moveToReplica = predecessorSet.find(targetNode.getRingHash()) != predecessorSet.end();\n                transferFileToNode(fileName, targetNode, moveToReplica);\n            }\n        }\n\n        for(auto& fileVersion : secondaryFiles){\n            string fileName = fileVersion.first;\n            int fileHash = hashFunction(fileName);\n            Node targetNode = ring.getPrimaryNodeForFile(fileHash);\n\n            if(targetNode.getRingHash() == ring.getSelfNode().getRingHash()){\n                // TODO: transfer file\n                secondaryFilesList.removeFile(fileName);\n                primaryFilsList.transferFileToList(fileName, fileVersion.second);\n            } else if (predecessorSet.find(targetNode.getRingHash()) == predecessorSet.end()){\n                cout << getCurrentFullTimestamp()<<\" : \"<<\"DELETING FIEL FROM SECONDARY\"<<endl;\n                // TODO: delete all shards of the file\n                // lock file\n                std::lock_guard<std::mutex> lock(secondaryFilesList.getLockForFile(fileName));\n                for(int i=1;i<=secondaryFilesList.getFileVersion(fileName);i++){\n                    deleteFile(getHyDFSFilesPath(filesDirectory, fileName, i));\n                }\n                // deleteFile(getHyDFSFilesPath(filesDirectory, fileName));\n                secondaryFilesList.removeFile(fileName);\n            }\n        }\n    }\n}\n\nvoid HyDFS::transferFileToNode(string fileName, Node node, bool moveToReplica){\n\n    int myVersions = primaryFilsList.getFileVersion(fileName);\n    primaryFilsList.removeFile(fileName);\n    for(int i=1;i<=myVersions;i++){\n        string filePath = getHyDFSFilesPath(filesDirectory, fileName, i);\n        HyDFSMessage message = HyDFSMessage::getFileNameVersionMessage(HyDFSMessage::HyDFSMessageType::PUT_FILE, fileName, i);\n        int socket = sendMessageToNode(message, node);\n        if(socket == -1){\n            cerr<<\"Error getting socket for node\"<<endl;\n            return;\n        }\n        HyDFSMessage response = receiveMessageFromSocket(socket);\n        if(response.getType() == HyDFSMessage::HyDFSMessageType::READY_TO_RECEIVE_FILE){\n            sendFileToSocket(filePath, socket);\n        } else {\n            cerr<<\"Error sending file to node - \"<< response.getTypeString()<<endl;\n        }\n        close(socket);\n    }\n\n\n    if(moveToReplica){\n        secondaryFilesList.transferFileToList(fileName, myVersions);\n    } else {\n        for(int i=1;i<=myVersions;i++){\n            deleteFile(getHyDFSFilesPath(filesDirectory, fileName, i));\n        }\n    }\n\n}"
    },
    {
      "file_path": "HyDFS.h",
      "content": "#ifndef HYDFS_H\n#define HYDFS_H\n\n#include <string>\n#include <map>\n#include <vector>\n#include <thread>\n#include <atomic>\n#include \"Node.h\"\n#include \"HyDFSMessage.h\"\n#include \"ConsistentHashRing.h\"\n#include \"FileMetaData.h\"\n#include \"Cache.h\"\nusing namespace std;\nclass HyDFS {\nprivate:\n    ConsistentHashRing ring;\n    string selfNodeId;\n    char* tcpPort;\n    int tcpSocket;\n    string filesDirectory;\n    atomic<int> threadsRunning;\n    vector<thread> threads;\n    FileList primaryFilsList, secondaryFilesList;\n    Cache cache;\n    atomic<int> bytesReceived, bytesSent;\n    atomic<int> totalBytesReceived, totalBytesSent;\n    string startEpoch;\n\n\n    // void handleClient(int clientSockfd);\n    // void tcpListener(int tcpSockfd);\n\npublic:\n    HyDFS(char *port);\n    ~HyDFS();\n\n    void runCommand(std::string command);\n    void membershipUpdate(std::vector<Node>& nodes);\n    void updateRing(Node node);\n    void setSelfNode(Node node);\n    void start();\n    void stopThreads();\n\n    // command handlers\n    void listMyFiles();\n    void requestFile(string fileName, string localFileName);\n    void createFile(string localFilename, string hydfsFilename);\n    void appendFile(string fileName, string localFileName);\n    void mergeFile(string fileName);\n    void listNodesHavingFile(string fileName);\n    void multiAppend(string hydfsFileName, int numOfAppends, int numOfClients);\n    void getFileFromReplica(string vmId, string hydfsFileName, string localFileName);\n\n    // message handlers\n    void getFileHandler(HyDFSMessage message, int clientSockfd);\n    void putFileHandler(HyDFSMessage message,int clientSockfd);\n    void multiAppendHandler(HyDFSMessage message);\n    void invalidateCacheHandler(HyDFSMessage message);\n    void putFileHelper(string fileName, int version, int clientSockfd);\n    void appendFileHandler(HyDFSMessage message, int clientSockfd);\n    void getPrimaryFileListHandler(HyDFSMessage message, int clientSockfd);\n    void getFileAllListHandler(HyDFSMessage message, int clientSockfd);\n    void getShardHandler(HyDFSMessage message, int clientSockfd);\n    void mergeFileHandler(HyDFSMessage message);\n    void getAllFileListHandler(HyDFSMessage message, int clientSockfd);\n\n\n    // threads\n    void tcpListener(int tcpSockfd);\n    void handleClient(int clientSockfd);\n    void predecessorPinger();\n    void monitorFileLists();\n\n\n    // utility functions\n    void sendFileToNode(string filepath, Node node, string hydfsFilename, int version);\n    void appendFileToNode(string filepath, Node node, string hydfsFileName);\n    int sendMessageToNode(HyDFSMessage message, Node node);\n    void sendMessageToSocket(HyDFSMessage message, int sockfd);\n    void sendInvalidateCacheMessage(string hydfsFileName);\n    void sendFileToSocket(string filepath, int sockfd);\n    void setLocalFilesPath(string path);\n    void receiveFileFromNode(string filepath, Node node, string hydfsFilename);\n    bool receiveFileFromSocket(string filepath, int sockfd, string filename, bool forCache);\n    int getSocketForNode(Node node);\n    HyDFSMessage receiveMessageFromSocket(int sockfd);\n    void transferFileToNode(string fileName, Node node, bool moveToReplica);\n    void mergeFile(string fileName, int numberOfShards, Node fromNode);\n    void mergeAllReplicas();\n    unordered_set<string> getAllFileListFromNode(Node node);\n    void printBandwidth();\n    void printCurrentBandwidth(double ms);\n};\n\n#endif // HYDFS_H"
    },
    {
      "file_path": "HyDFSMessage.cpp",
      "content": "#include \"HyDFSMessage.h\"\n#include <sstream>\n#include <iostream>\n#include <unordered_set>\n#include <cassert>\n\n\nconst std::unordered_set<HyDFSMessage::HyDFSMessageType> HyDFSMessage::LIST_TYPE_MESSAGES = {\n    HyDFSMessage::HyDFSMessageType::GET_FILE_LIST_RESPONSE\n};\n\nconst std::unordered_set<HyDFSMessage::HyDFSMessageType> HyDFSMessage::FILE_NAME_VERSION_MESSAGES = {\n    HyDFSMessage::HyDFSMessageType::\n    GET_SHARD,\n    HyDFSMessage::HyDFSMessageType::PUT_FILE,\n};\n\nconst std::unordered_set<HyDFSMessage::HyDFSMessageType> HyDFSMessage::FILE_NAME_MESSAGES = {\n    HyDFSMessage::HyDFSMessageType::GET_FILE,\n    HyDFSMessage::HyDFSMessageType::APPEND_FILE,\n    HyDFSMessage::HyDFSMessageType::FILE_ALREADY_EXISTS,\n    HyDFSMessage::HyDFSMessageType::READY_TO_RECEIVE_FILE,\n    HyDFSMessage::HyDFSMessageType::FILE_DOES_NOT_EXIST,\n    HyDFSMessage::HyDFSMessageType::MULTI_APPEND,\n    HyDFSMessage::HyDFSMessageType::INVALIDATE_CACHE,\n    HyDFSMessage::HyDFSMessageType::MERGE_FILE\n};\n\nconst std::unordered_set<HyDFSMessage::HyDFSMessageType> HyDFSMessage::INFORMATIONAL_MESSAGES = {\n    HyDFSMessage::HyDFSMessageType::ERROR_RECEIVING_MESSAGE,\n    HyDFSMessage::HyDFSMessageType::GET_FILE_LIST,\n    HyDFSMessage::HyDFSMessageType::GET_ALL_FILE_LIST\n};\n\n\nHyDFSMessage::HyDFSMessage(HyDFSMessageType type, std::string fileName, int version) : type(type), fileName(fileName), version(version) {}\n\n// HyDFSMessage::HyDFSMessage(HyDFSMessageType type, std::string fileName) : type(type), fileName(fileName) {}\n\n\n// Converts HyDFSMessageType to a string\nstd::string HyDFSMessage::messageTypeToString(HyDFSMessageType type) {\n    switch (type) {\n        case HyDFSMessageType::GET_FILE:\n            return \"GET_FILE\";\n        case HyDFSMessageType::PUT_FILE:\n            return \"PUT_FILE\";\n        case HyDFSMessageType::GET_FILE_LIST:\n            return \"GET_FILE_LIST\";\n        case HyDFSMessageType::APPEND_FILE:\n            return \"APPEND_FILE\";\n        case HyDFSMessageType::FILE_ALREADY_EXISTS:\n            return \"FILE_ALREADY_EXISTS\";\n        case HyDFSMessageType::READY_TO_RECEIVE_FILE:\n            return \"READY_TO_RECEIVE_FILE\";\n        // case HyDFSMessageType::TRANSFER_FILE:\n        //     return \"TRANSFER_FILE\";\n        case HyDFSMessageType::GET_FILE_LIST_RESPONSE:\n            return \"GET_FILE_LIST_RESPONSE\";\n        case HyDFSMessageType::GET_SHARD:\n            return \"GET_SHARD\";\n        case HyDFSMessageType::FILE_DOES_NOT_EXIST:\n            return \"FILE_DOES_NOT_EXIST\";\n        case HyDFSMessageType::ERROR_RECEIVING_MESSAGE:\n            return \"ERROR_RECEIVING_MESSAGE\";\n        case HyDFSMessageType::MULTI_APPEND:\n            return \"MULTI_APPEND\";\n        case HyDFSMessageType::INVALIDATE_CACHE:\n            return \"INVALIDATE_CACHE\";\n        case HyDFSMessageType::MERGE_FILE:\n            return \"MERGE_FILE\";\n        case HyDFSMessageType::GET_ALL_FILE_LIST:\n            return \"GET_ALL_FILE_LIST\";\n        default:\n            throw std::invalid_argument(\"Invalid hydfs message type\");\n    }\n}\n\n// Converts a string to HyDFSMessageType\nHyDFSMessage::HyDFSMessageType HyDFSMessage::stringToMessageType(const std::string& typeStr) {\n    if (typeStr == \"GET_FILE\") {\n        return HyDFSMessageType::GET_FILE;\n    } else if (typeStr == \"PUT_FILE\") {\n        return HyDFSMessageType::PUT_FILE;\n    } else if (typeStr == \"GET_FILE_LIST\") {\n        return HyDFSMessageType::GET_FILE_LIST;\n    } else if (typeStr == \"GET_FILE_LIST_RESPONSE\") {\n        return HyDFSMessageType::GET_FILE_LIST_RESPONSE;\n    } else if (typeStr == \"APPEND_FILE\") {\n        return HyDFSMessageType::APPEND_FILE;\n    } else if (typeStr == \"FILE_ALREADY_EXISTS\") {\n        return HyDFSMessageType::FILE_ALREADY_EXISTS;\n    } else if (typeStr == \"READY_TO_RECEIVE_FILE\") {\n        return HyDFSMessageType::READY_TO_RECEIVE_FILE;\n    // } else if (typeStr == \"TRANSFER_FILE\") {\n    //     return HyDFSMessageType::TRANSFER_FILE;\n    } else if (typeStr == \"GET_SHARD\") {\n        return HyDFSMessageType::GET_SHARD;\n    } else if (typeStr == \"FILE_DOES_NOT_EXIST\") {\n        return HyDFSMessageType::FILE_DOES_NOT_EXIST;\n    } else if (typeStr == \"ERROR_RECEIVING_MESSAGE\") {\n        return HyDFSMessageType::ERROR_RECEIVING_MESSAGE;\n    } else if (typeStr == \"MULTI_APPEND\") {\n        return HyDFSMessageType::MULTI_APPEND;\n    } else if (typeStr == \"INVALIDATE_CACHE\") {\n        return HyDFSMessageType::INVALIDATE_CACHE;\n    } else if (typeStr == \"MERGE_FILE\") {\n        return HyDFSMessageType::MERGE_FILE;\n    } else if (typeStr == \"GET_ALL_FILE_LIST\") {\n        return HyDFSMessageType::GET_ALL_FILE_LIST;\n    }\n    else {\n        throw std::invalid_argument(\"Invalid hydfs message type string\");\n    }\n}\n\nstd::string HyDFSMessage::serializeMessage() {\n    // Serialize the message into a string\n    std::string serializedMessage = messageTypeToString(type);\n\n    // add filelist to the serialized message\n    if(LIST_TYPE_MESSAGES.find(type) != LIST_TYPE_MESSAGES.end()){\n        if(filesList.size() > 0){\n            for (const auto& filePair : filesList) {\n                serializedMessage += \"\\n\" + filePair.first + \" \" + std::to_string(filePair.second);\n            }\n        }\n    } else if(FILE_NAME_VERSION_MESSAGES.find(type) != FILE_NAME_VERSION_MESSAGES.end()){\n        serializedMessage += \"\\n\" + fileName+\" \"+std::to_string(version);\n    } else if(FILE_NAME_MESSAGES.find(type) != FILE_NAME_MESSAGES.end()){\n        serializedMessage += \"\\n\" + fileName;\n    } else if (INFORMATIONAL_MESSAGES.find(type) != INFORMATIONAL_MESSAGES.end()) {\n        // Do nothing\n    } else {\n        throw std::invalid_argument(\"Invalid message type\");\n    }\n\n    return serializedMessage;\n}\n\nHyDFSMessage::HyDFSMessageType HyDFSMessage::getType() {\n    return type;\n}\n\nstd::string HyDFSMessage::getTypeString() {\n    return messageTypeToString(type);\n}\n\nHyDFSMessage  HyDFSMessage::deserializeHyDFSMessage(const std::string& serialized){\n    // Deserialize the message from a string\n    std::istringstream iss(serialized);\n    std::string typeStr;\n    std::getline(iss, typeStr, '\\n');\n    HyDFSMessageType type = stringToMessageType(typeStr);\n\n\n    if(LIST_TYPE_MESSAGES.find(type) != LIST_TYPE_MESSAGES.end()){\n        std::vector<std::pair<std::string, int>> filesList;\n        std::string fileName;\n        std::string fileVersion;\n        while(std::getline(iss, fileName, ' ')){\n            std::getline(iss, fileVersion, '\\n');\n            std::istringstream iss2(fileVersion);\n            int version;\n            iss2 >> version;\n            filesList.push_back({fileName, version});\n        }\n        return HyDFSMessage(type, filesList);\n    } else if(FILE_NAME_VERSION_MESSAGES.find(type) != FILE_NAME_VERSION_MESSAGES.end()){\n        std::string fileName;\n        std::string fileVersion;\n        std::getline(iss, fileName, ' ');\n        std::getline(iss, fileVersion, '\\n');\n        std::istringstream iss2(fileVersion);\n        int version;\n        iss2 >> version;\n        return HyDFSMessage(type, fileName, version);\n    } else if(FILE_NAME_MESSAGES.find(type) != FILE_NAME_MESSAGES.end()){\n        std::string fileName;\n        std::getline(iss, fileName, '\\n');\n        return HyDFSMessage(type, fileName, 0);\n    } else if (INFORMATIONAL_MESSAGES.find(type) != INFORMATIONAL_MESSAGES.end()) {\n        return HyDFSMessage(type, \"\", 0);\n    } else {\n        throw std::invalid_argument(\"Invalid message type\");\n    }\n\n}\n\nstd::string HyDFSMessage::getFileName() {\n    return fileName;\n}\nint HyDFSMessage::getVersion() {\n    return version;\n}\n\nHyDFSMessage::HyDFSMessage(HyDFSMessageType type, std::vector<std::pair<std::string, int>> filesList) : type(type), filesList(filesList) {}\n\nstd::vector<std::pair<std::string, int>> HyDFSMessage::getFilesList() {\n    return filesList;\n}\n\nHyDFSMessage HyDFSMessage::getInformationalMessage(HyDFSMessageType type) {\n    assert(INFORMATIONAL_MESSAGES.find(type) != INFORMATIONAL_MESSAGES.end());\n    return HyDFSMessage(type, \"\", 0);\n}\n\nHyDFSMessage HyDFSMessage::getFileNameMessage(HyDFSMessageType type, std::string fileName) {\n    assert(FILE_NAME_MESSAGES.find(type) != FILE_NAME_MESSAGES.end());\n    return HyDFSMessage(type, fileName, 0);\n}\n\n\n\nHyDFSMessage HyDFSMessage::getFileNameVersionMessage(HyDFSMessageType type, std::string fileName, int version) {\n    assert(FILE_NAME_VERSION_MESSAGES.find(type) != FILE_NAME_VERSION_MESSAGES.end());\n    return HyDFSMessage(type, fileName, version);\n}\n\nHyDFSMessage HyDFSMessage::getFileListMessage(HyDFSMessageType type, std::vector<std::pair<std::string, int>> filesList) {\n    assert(LIST_TYPE_MESSAGES.find(type) != LIST_TYPE_MESSAGES.end());\n    return HyDFSMessage(type, filesList);\n}\n\n"
    },
    {
      "file_path": "HyDFSMessage.h",
      "content": "#ifndef HYDFS_MESSAGE_H\n#define HYDFS_MESSAGE_H\n\n#include <string>\n#include <stdexcept>\n#include <vector>\n#include <unordered_set>\n\nclass HyDFSMessage {\npublic:\n    // Enum to represent different message types\n    enum class HyDFSMessageType {\n        GET_FILE,\n        PUT_FILE,\n        GET_FILE_LIST,\n        GET_FILE_LIST_RESPONSE,\n        APPEND_FILE,\n        FILE_ALREADY_EXISTS,\n        READY_TO_RECEIVE_FILE,\n        // TRANSFER_FILE,\n        GET_SHARD,\n        FILE_DOES_NOT_EXIST,\n        ERROR_RECEIVING_MESSAGE,\n        MULTI_APPEND,\n        INVALIDATE_CACHE,\n        MERGE_FILE,\n        GET_ALL_FILE_LIST\n    };\n\n    static const std::unordered_set<HyDFSMessageType> LIST_TYPE_MESSAGES, FILE_NAME_VERSION_MESSAGES, FILE_NAME_MESSAGES, INFORMATIONAL_MESSAGES;\n\n    // Converts HyDFSMessageType to a string\n    static std::string messageTypeToString(HyDFSMessageType type);\n    // Converts a string to HyDFSMessageType\n    static HyDFSMessageType stringToMessageType(const std::string& typeStr);\n    std::string serializeMessage();\n    static HyDFSMessage deserializeHyDFSMessage(const std::string& serialized);\n    HyDFSMessageType getType();\n    std::string getFileName();\n    int getVersion();\n    std::string getTypeString();\n    // getter for filesList\n    std::vector<std::pair<std::string, int>> getFilesList();\n\n    // HyDFSMessage(HyDFSMessageType type, std::string fileName);\n    static HyDFSMessage getInformationalMessage(HyDFSMessageType type);\n    static HyDFSMessage getFileNameVersionMessage(HyDFSMessageType type, std::string fileName, int version);\n    static HyDFSMessage getFileNameMessage(HyDFSMessageType type, std::string fileName);\n    static HyDFSMessage getFileListMessage(HyDFSMessageType type, std::vector<std::pair<std::string, int>> filesList);\n\n\n\n\nprivate:\n\n    HyDFSMessage(HyDFSMessageType type, std::string fileName, int version = 0);\n    HyDFSMessage(HyDFSMessageType type, std::vector<std::pair<std::string, int>> filesList);\n\n    HyDFSMessageType type;\n    std::string fileName;\n    int version;\n    std::vector<std::pair<std::string, int>> filesList;\n};\n\n#endif // HYDFS_MESSAGE_H"
    },
    {
      "file_path": "Makefile",
      "content": "CXX = g++\nCXXFLAGS = -Wall -Wextra -std=c++17\n# Target and source files\nTARGET = Daemon\nSRCS = main.cpp utils.cpp HyDFSMessage.cpp Node.cpp FileMetaData.cpp HyDFS.cpp ConsistentHashRing.cpp Cache.cpp \nOBJS = $(SRCS:.cpp=.o)\n\n# Rules\nall: $(TARGET)\n\n$(TARGET): $(OBJS)\n\t$(CXX) $(CXXFLAGS) -o $(TARGET) $(OBJS)\n\n%.o: %.cpp\n\t$(CXX) $(CXXFLAGS) -c $< -o $@\n\nclean:\n\trm -f $(OBJS) $(TARGET) ./stats/*.txt\n\ncfiles:\n\trm -rf ./files/A* ./files/wireless* ./files/introducer/hydfs/* ./files/remote/hydfs/* ./files/fa24-cs425*"
    },
    {
      "file_path": "MembershipList.cpp",
      "content": "#include \"Node.h\"\n#include \"utils.h\"\n#include <unordered_map>\n#include <vector>\n#include <string>\n#include <mutex>\n#include <algorithm>\n#include <random>\n\nclass MembershipList {\nprivate:\n    unordered_map<string, Node> nodes; // Assuming Node has a unique identifier of type string\n    mutable std::mutex mtx; // Mutex to protect the nodes map\n\npublic:\n\n    void addNode(const Node& node) {\n        std::lock_guard<std::mutex> lock(mtx);\n        nodes.insert(std::make_pair(node.getNodeId(), node)); // Assuming Node has a method getNodeId() that returns a unique identifier of type string\n        nodes[node.getNodeId()].printNode();\n    }\n\n    void addNodes(const std::vector<Node>& nodeList) {\n        std::lock_guard<std::mutex> lock(mtx);\n        for (const Node& node : nodeList) {\n            nodes[node.getNodeId()] = node;\n            nodes[node.getNodeId()].printNode();\n        }\n    }\n\n    bool removeNode(const Node& node) {\n        std::lock_guard<std::mutex> lock(mtx);\n        if(nodes.find(node.getNodeId()) != nodes.end() && (nodes[node.getNodeId()].getStatus() == Status::alive || nodes[node.getNodeId()].getStatus() == Status::suspected)\n        && nodes[node.getNodeId()].getIncarnationNumber() == node.getIncarnationNumber()) {\n            cout<<\"DEBUG: Removing node - \"<<node.getNodeId()<<endl;\n            nodes[node.getNodeId()].setStatus(Status::failed);\n            // red color cout\n            cout<<\"\\033[1;31m\"<<endl;\n            nodes[node.getNodeId()].printNode();\n            cout<<\"\\033[0m\"<<endl;\n            return true;\n        }\n        return false;\n    }\n\n    bool markNodeSuspected(const Node& node) {\n        std::lock_guard<std::mutex> lock(mtx);\n        if(nodes.find(node.getNodeId()) != nodes.end() && nodes[node.getNodeId()].getStatus() == Status::alive\n        && nodes[node.getNodeId()].getIncarnationNumber() == node.getIncarnationNumber()) {\n            nodes[node.getNodeId()].setStatus(Status::suspected);\n\n            // orange color cout\n            cout<<\"\\033[1;33m\"<<endl;\n            nodes[node.getNodeId()].printNode();\n            cout<<\"\\033[0m\"<<endl;\n            nodes[node.getNodeId()].setLastUpdated(getCurrentTSinEpoch());\n            return true;\n        }\n        return false;\n    }\n\n    bool markNodeAlive(const Node& node) {\n        std::lock_guard<std::mutex> lock(mtx);\n        if(nodes.find(node.getNodeId()) != nodes.end() && nodes[node.getNodeId()].getIncarnationNumber() < node.getIncarnationNumber()) {\n            cout<<\"DEBUG: Marking node alive - \"<<node.getNodeId()<<endl;\n            nodes[node.getNodeId()].setStatus(Status::alive);\n            nodes[node.getNodeId()].setIncarnationNumber(node.getIncarnationNumber());\n            cout<<\"NODE - \"<<endl;\n            // green color cout\n            cout<<\"\\033[1;32m\"<<endl;\n            nodes[node.getNodeId()].printNode();\n            cout<<\"\\033[0m\"<<endl;\n            return true;\n        }\n        return false;\n    }\n\n    void markNodeLeft(const Node& node) {\n        std::lock_guard<std::mutex> lock(mtx);\n        if(nodes.find(node.getNodeId()) != nodes.end()) {\n            nodes[node.getNodeId()].setStatus(Status::left);\n            nodes[node.getNodeId()].printNode();\n        }\n    }\n\n    bool contains(const Node& node) const {\n        std::lock_guard<std::mutex> lock(mtx);\n        return nodes.find(node.getNodeId()) != nodes.end();\n    }\n\n    std::vector<Node> getAliveNodes() const {\n        std::lock_guard<std::mutex> lock(mtx);\n        std::vector<Node> nodeList;\n        for (const auto& pair : nodes) {\n            if (pair.second.getStatus() == Status::alive) {\n                nodeList.push_back(pair.second);\n            }\n        }\n        return nodeList;\n    }\n\n    // get shuffled list of nodes\n    std::vector<Node> getAliveShuffledNodes() const {\n        std::lock_guard<std::mutex> lock(mtx);\n        std::vector<Node> nodeList;\n        for (const auto& pair : nodes) {\n            if (pair.second.getStatus() == Status::alive) {\n                nodeList.push_back(pair.second);\n            }\n        }\n        std::random_device rd;\n        std::mt19937 g(rd());\n        std::shuffle(nodeList.begin(), nodeList.end(), g);\n        return nodeList;\n    }\n\n    // set last updated time of a node given nodeId\n    void setLastUpdated(const std::string& nodeId, const std::string& lastUpdated) {\n        std::lock_guard<std::mutex> lock(mtx);\n        if (nodes.find(nodeId) != nodes.end()) {\n            auto it = nodes.find(nodeId);\n            if (it != nodes.end()) {\n                it->second.setLastUpdated(lastUpdated);\n            }\n        }\n    }\n\n    string getLastUpdated(const std::string& nodeId)  {\n        std::lock_guard<std::mutex> lock(mtx);\n        if (nodes.find(nodeId) != nodes.end()) {\n            return nodes[nodeId].getLastUpdated();\n        }\n        return \"\";\n    }\n\n\n    std::vector<Node> getKShuffledNodes(int k) const {\n        std::lock_guard<std::mutex> lock(mtx);\n        std::vector<Node> nodeList;\n        for (const auto& pair : nodes) {\n            if (pair.second.getStatus() == Status::alive || pair.second.getStatus() == Status::suspected) {\n                nodeList.push_back(pair.second);\n            }\n        }\n        if ((int)nodeList.size() > k) {\n            nodeList.resize(k);\n        }\n        std::random_device rd;\n        std::mt19937 g(rd());\n        std::shuffle(nodeList.begin(), nodeList.end(), g);\n        return nodeList;\n    }\n\n\n\n\n    // set status of a node given nodeId\n    void setStatus(const std::string& nodeId, Status status) {\n        std::lock_guard<std::mutex> lock(mtx);\n        if (nodes.find(nodeId) != nodes.end()) {\n            auto it = nodes.find(nodeId);\n            if (it != nodes.end()) {\n                it->second.setStatus(status);\n            }\n        }\n    }\n\n    // get status of a node given nodeId\n    Status getStatus(const std::string& nodeId)  {\n        std::lock_guard<std::mutex> lock(mtx);\n        if (nodes.find(nodeId) != nodes.end()) {\n            return nodes[nodeId].getStatus();\n        }\n        return Status::failed;\n    }\n\n    // bool function to check if node exists in the list\n    bool nodeExists(const std::string& nodeId) const {\n        std::lock_guard<std::mutex> lock(mtx);\n        return nodes.find(nodeId) != nodes.end();\n    }\n\n    Node getNode(const std::string& nodeId) {\n        std::lock_guard<std::mutex> lock(mtx);\n        return nodes[nodeId];\n    }\n\n\n    // serialize the membership list\n    std::string serialize() const {\n        std::lock_guard<std::mutex> lock(mtx);\n        std::string serialized;\n        for (const auto& pair : nodes) {\n            serialized += pair.second.serialise() + \"\\n\";\n        }\n        return serialized;\n    }\n\n    // deserialize the membership list and return membership list\n    static MembershipList deserialize(const std::string& serialized) {\n        MembershipList ml;\n        std::istringstream ss(serialized);\n        std::string line;\n        while (std::getline(ss, line, '\\n')) {\n            Node node = Node::deserialiseNode(line);\n            ml.addNode(node);\n        }\n        return ml;\n    }\n\n    //copy constructor\n    MembershipList(const MembershipList& ml) {\n        std::lock_guard<std::mutex> lock(mtx);\n        for (const auto& pair : ml.nodes) {\n            nodes.insert(pair);\n        }\n    }\n\n    //empty constructor\n    MembershipList() {}\n\n    //print membership list\n    void printMembershipList() const {\n        std::lock_guard<std::mutex> lock(mtx);\n        for (const auto& pair : nodes) {\n            pair.second.printNode();\n        }\n    }\n\n    // assignment operator\n    MembershipList& operator=(const MembershipList& ml) {\n        std::lock_guard<std::mutex> lock(mtx);\n        if (this != &ml) {\n            nodes.clear();\n            for (const auto& pair : ml.nodes) {\n                nodes.insert(pair);\n            }\n        }\n        return *this;\n    }\n\n    // get size of the membership list\n    int getSize() const {\n        std::lock_guard<std::mutex> lock(mtx);\n        return nodes.size();\n    }\n\n    int getAliveNodesCount() const {\n        std::lock_guard<std::mutex> lock(mtx);\n        int count = 0;\n        for (const auto& pair : nodes) {\n            if (pair.second.getStatus() == Status::alive) {\n                count++;\n            }\n        }\n        return count;\n    }\n\n\n\n\n};\n"
    },
    {
      "file_path": "Message.cpp",
      "content": "#include <string>\n#include <unordered_set>\nusing namespace std;\n\nenum MessageType { JOIN, PING, ACK, FAILED, LEAVE, SUSPECT, ENABLE_SUSPECT_MECHANISM, DISABLE_SUSPECT_MECHANISM, ALIVE};\n\nMessageType stringToMessageType(std::string type) {\n    if (type == \"JOIN\") {\n        return MessageType::JOIN;\n    } else if (type == \"PING\") {\n        return MessageType::PING;\n    } else if (type == \"ACK\") {\n        return MessageType::ACK;\n    } else if (type == \"FAILED\") {\n        return MessageType::FAILED;\n    } else if (type == \"LEAVE\") {\n        return MessageType::LEAVE;\n    } else if (type == \"SUSPECT\") {\n        return MessageType::SUSPECT;\n    } else if (type == \"ENABLE_SUSPECT_MECHANISM\") {\n        return MessageType::ENABLE_SUSPECT_MECHANISM;\n    } else if (type == \"DISABLE_SUSPECT_MECHANISM\") {\n        return MessageType::DISABLE_SUSPECT_MECHANISM;\n    } else if (type == \"ALIVE\") {\n        return MessageType::ALIVE;\n    }\n    else {\n        throw std::invalid_argument(\"Invalid message type\");\n    }\n}\n\nstring messageTypeToString(MessageType type) {\n    switch (type) {\n        case MessageType::JOIN:\n            return \"JOIN\";\n        case MessageType::PING:\n            return \"PING\";\n        case MessageType::ACK:\n            return \"ACK\";\n        case MessageType::FAILED:\n            return \"FAILED\";\n        case MessageType::LEAVE:\n            return \"LEAVE\";\n        case MessageType::SUSPECT:\n            return \"SUSPECT\";\n        case MessageType::ENABLE_SUSPECT_MECHANISM:\n            return \"ENABLE_SUSPECT_MECHANISM\";\n        case MessageType::DISABLE_SUSPECT_MECHANISM:\n            return \"DISABLE_SUSPECT_MECHANISM\";\n        case MessageType::ALIVE:\n            return \"ALIVE\";\n        default:\n            throw std::invalid_argument(\"Invalid membership message type\");\n    }\n}\n\nconst unordered_set<MessageType> messageHasPayload = {MessageType::ACK, MessageType::FAILED, MessageType::SUSPECT, MessageType::ALIVE};\n\nclass Message {\n\n    MessageType type;\n    vector<Node> nodesList;\n    Node senderNode;\n    int TTL;\n\npublic:\n    MessageType getType() {\n        return type;\n    }\n\n\n    Message(MessageType type, Node senderNode, int ttl=0) {\n        this->type = type;\n        this->senderNode = senderNode;\n        this->TTL = ttl;\n    }\n\n\n    Message(MessageType type, Node senderNode, vector<Node> ml, int ttl=0) {\n        this->type = type;\n        this->nodesList = ml;\n        this->senderNode = senderNode;\n        this->TTL = ttl;\n    }\n\n    // getter and setter for sender node\n\n    Node getSenderNode() {\n        return senderNode;\n    }\n\n    void setSenderNode(Node senderNode) {\n        this->senderNode = senderNode;\n    }\n\n    // getter and setter for TTL\n\n    int getTTL() {\n        return TTL;\n    }\n\n    void setTTL(int TTL) {\n        this->TTL = TTL;\n    }\n\n\n    string serializeMessage() {\n        string serialised = messageTypeToString(type) + \"\\n\";\n        serialised += senderNode.serialise() + \"\\n\";\n        serialised += to_string(TTL) + \"\\n\";\n        if (messageHasPayload.find(type) != messageHasPayload.end()) {\n            for (Node node : nodesList) {\n                serialised += node.serialise() + \"\\n\";\n            }\n        }\n\n        return serialised;\n    }\n\n    static Message deserializeMessage(string serialised) {\n        std::istringstream ss(serialised);\n        std::string line;\n        std::getline(ss, line, '\\n');\n        MessageType type = stringToMessageType(line);\n        std::getline(ss, line, '\\n');\n        Node senderNode = Node::deserialiseNode(line);\n        std::getline(ss, line, '\\n');\n        int ttl;\n        try {\n            ttl = std::stoi(line);\n        } catch (const std::invalid_argument& e) {\n            throw std::runtime_error(\"Failed to deserialize TTL: invalid argument\");\n        } catch (const std::out_of_range& e) {\n            throw std::runtime_error(\"Failed to deserialize TTL: out of range\");\n        }\n        if (messageHasPayload.find(type) != messageHasPayload.end()) {\n            vector<Node> nodesList;\n            while (std::getline(ss, line, '\\n')) {\n                Node node = Node::deserialiseNode(line);\n                nodesList.push_back(node);\n            }\n\n            return Message(type, senderNode, nodesList, ttl);\n        }\n        return Message(type, senderNode, ttl);\n    }\n\n    static Message createPingMessage(Node senderNode) {\n        return Message(MessageType::PING, senderNode);\n    }\n\n    static Message createJoinMessage(Node senderNode) {\n        return Message(MessageType::JOIN, senderNode);\n    }\n\n    static Message createAckMessage(Node senderNode, vector<Node> ml) {\n        return Message(MessageType::ACK, senderNode, ml);\n    }\n\n    static Message createAckMessageForPing(Node senderNode) {\n        return Message(MessageType::ACK, senderNode);\n    }\n\n    static Message createLeaveMessage(Node senderNode) {\n        return Message(MessageType::LEAVE, senderNode);\n    }\n\n    static Message createFailedMessage(Node senderNode, vector<Node> ml, int ttl) {\n        return Message(MessageType::FAILED, senderNode, ml, ttl);\n    }\n\n    static Message createSuspectMessage(Node senderNode, vector<Node> ml, int ttl) {\n        return Message(MessageType::SUSPECT, senderNode, ml, ttl);\n    }\n\n    static Message createEnableSuspectMechanismMessage(Node senderNode, int ttl) {\n        return Message(MessageType::ENABLE_SUSPECT_MECHANISM, senderNode, ttl);\n    }\n\n    static Message createDisableSuspectMechanismMessage(Node senderNode, int ttl) {\n        return Message(MessageType::DISABLE_SUSPECT_MECHANISM, senderNode, ttl);\n    }\n\n    static Message createSuspectMechanismMessage(Node senderNode, bool isEnabled, int ttl) {\n        if(isEnabled){\n            return createEnableSuspectMechanismMessage(senderNode, ttl);\n        }\n        else {\n            return createDisableSuspectMechanismMessage(senderNode, ttl);\n        }\n    }\n\n    static Message createAliveMessage(Node senderNode, vector<Node> ml, int ttl) {\n        return Message(MessageType::ALIVE, senderNode, ml, ttl);\n    }\n\n\n    void printMessage() {\n        cout << \"Message --------------------------\" << endl;\n        cout << \"Message Type: \" << messageTypeToString(type) << endl;\n        cout << \"Sender Node: \" << senderNode.getNodeName() << endl;\n        cout << \"Sender Port: \" << senderNode.getPort() << endl;\n        cout << \"TTL: \" << TTL << endl;\n        if (messageHasPayload.find(type) != messageHasPayload.end()) {\n            cout << \"Nodes List: \" << endl;\n            for (Node node : nodesList) {\n                node.printNode();\n            }\n        }\n        cout << \"----------------------------\" << endl;\n    }\n\n    // get membership list\n    vector<Node> getNodesList() {\n        return nodesList;\n    }\n};\n"
    },
    {
      "file_path": "Node.cpp",
      "content": "#include \"Node.h\"\n#include \"utils.h\"\n#include <sstream>\n#include <cstring>\n#include <chrono>\n#include <ctime>\n#include <unistd.h>\n\nstd::string convertStatusToString(Status status) {\n    switch (status) {\n        case Status::alive: return \"alive\";\n        case Status::suspected: return \"suspected\";\n        case Status::failed: return \"failed\";\n        case Status::left: return \"left\";\n        default: return \"unknown\";\n    }\n}\n\nStatus convertStringToStatus(std::string status) {\n    if (status == \"alive\") return Status::alive;\n    if (status == \"suspected\") return Status::suspected;\n    if (status == \"failed\") return Status::failed;\n    if (status == \"left\") return Status::left;\n    std::cerr << \"Error: Invalid status string '\" << status << \"' provided.\" << std::endl;\n    return Status::failed;\n}\n\nNode::Node() {}\n\nNode::Node(std::string _nodeId, char *_nodeName, char* _port, char* _tcpPort, Status _status, int _incarnationNumber)\n    : nodeId(_nodeId), nodeName(_nodeName), port(_port), tcpPort(_tcpPort), status(_status), incarnationNumber(_incarnationNumber) {}\n\nstd::string Node::getNodeId() const { return nodeId; }\nchar* Node::getNodeName() const { return nodeName; }\nchar* Node::getPort() const { return port; }\nStatus Node::getStatus() const { return status; }\nint Node::getIncarnationNumber() const { return incarnationNumber; }\nvoid Node::setIncarnationNumber(int _incarnationNumber) { incarnationNumber = _incarnationNumber; }\nvoid Node::incrementIncarnationNumber() { incarnationNumber += 1; }\nvoid Node::setLastUpdated(const std::string& _lastUpdated) { lastUpdated = _lastUpdated; }\nstd::string Node::getLastUpdated() const { return lastUpdated; }\n\nstd::string Node::serialise() const {\n    std::string serialised = nodeId + \"$\" + nodeName + \"$\" + port + \"$\" + tcpPort + \"$\" + std::to_string(incarnationNumber) + \"$\";\n    serialised += convertStatusToString(status);\n    return serialised;\n}\n\nNode Node::deserialiseNode(std::string serialised) {\n    std::stringstream ss(serialised);\n    std::string nodeId, nodeName, port, statusStr, incarnationNumberStr, tcpPort;\n    getline(ss, nodeId, '$');\n    getline(ss, nodeName, '$');\n    getline(ss, port, '$');\n    getline(ss, tcpPort, '$');\n    getline(ss, incarnationNumberStr, '$');\n    int incarnationNumber;\n    try {\n        incarnationNumber = std::stoi(incarnationNumberStr);\n    } catch (const std::invalid_argument& e) {\n        std::cerr << \"Error: Invalid argument for incarnation number: \" << e.what() << std::endl;\n        incarnationNumber = 1; // Default value\n    } catch (const std::out_of_range& e) {\n        std::cerr << \"Error: Incarnation number out of range: \" << e.what() << std::endl;\n        incarnationNumber = 1; // Default value\n    }\n    getline(ss, statusStr, '$');\n    Status status = convertStringToStatus(statusStr);\n    char *port_str = new char[port.length() + 1];\n    std::strcpy(port_str, port.c_str());\n    char *tcpPort_str = new char[tcpPort.length() + 1];\n    std::strcpy(tcpPort_str, tcpPort.c_str());\n    char *nodeName_str = new char[nodeName.length() + 1];\n    std::strcpy(nodeName_str, nodeName.c_str());\n    Node node(nodeId, nodeName_str, port_str, tcpPort_str, status, incarnationNumber);\n    return node;\n}\n\nvoid Node::setStatus(Status _status) { status = _status; }\n\nvoid Node::setStatus(std::string _status) {\n    if (_status == \"alive\") status = Status::alive;\n    else if (_status == \"suspected\") status = Status::suspected;\n    else if (_status == \"failed\") status = Status::failed;\n    else if (_status == \"left\") status = Status::left;\n    else std::cerr << \"Error: Invalid status string '\" << _status << \"' provided.\" << std::endl;\n}\n\nvoid Node::printNode() const {\n    std::cout << \"Node --------\" << std::endl;\n    std::cout << \"Node ID: \" << nodeId << std::endl;\n    std::cout << \"Node Name: \" << nodeName << std::endl;\n    std::cout << \"Port: \" << port << std::endl;\n    std::cout << \"TCP Port: \" << tcpPort << std::endl;\n    std::cout << \"Status: \" << convertStatusToString(status) << std::endl;\n    std::cout << \"Incarnation Number: \" << incarnationNumber << std::endl;\n    std::cout << \"Last Updated: \" << lastUpdated << std::endl;\n    long long timeDiff = lastUpdated!=\"\"? differenceWithCurrentEpoch(lastUpdated) : 0;\n    std::cout << \"Time since last updated: \" << timeDiff << std::endl;\n}\n\nlong long getTimeDifference(std::string ts) {\n    auto now = std::chrono::system_clock::now();\n    auto now_ms = std::chrono::time_point_cast<std::chrono::milliseconds>(now);\n    auto epoch = now_ms.time_since_epoch();\n    auto value = std::chrono::duration_cast<std::chrono::milliseconds>(epoch);\n    return value.count() - std::stoll(ts);\n}\n\nchar* getCurrentTimestamp() {\n    char* timestamp = new char[9];\n    std::time_t now = std::time(nullptr);\n    std::tm* localTime = std::localtime(&now);\n    std::strftime(timestamp, 9, \"%H:%M:%S\", localTime);\n    return timestamp;\n}\n\nNode Node::generateNode(char* port, char *tcpPort) {\n    char hostbuffer[256];\n    if (gethostname(hostbuffer, sizeof(hostbuffer)) == -1) {\n        std::cerr << \"Error getting hostname\" << std::endl;\n        exit(1);\n    }\n    char * timestamp = getCurrentTimestamp();\n    char* result = new char[std::strlen(hostbuffer) + std::strlen(port) + std::strlen(timestamp) + 3];\n    std::strcpy(result, hostbuffer);\n    std::strcat(result, \"#\");\n    std::strcat(result, port);\n    std::strcat(result, \"#\");\n    std::strcat(result, timestamp);\n    std::string resultStr(result);\n    char* hostbufferCopy = new char[std::strlen(hostbuffer) + 1];\n    std::strcpy(hostbufferCopy, hostbuffer);\n    return Node(resultStr, hostbufferCopy, port, tcpPort, Status::alive);\n}\n\nint Node::getRingHash() const {\n    return ringHash;\n}\n\nvoid Node::setRingHash(int _ringHash){\n    ringHash = _ringHash;\n}\n\nchar* Node::getTcpPort() const {\n    return tcpPort;\n}"
    },
    {
      "file_path": "Node.h",
      "content": "#ifndef NODE_H\n#define NODE_H\n\n#include <string>\n#include <iostream>\n\nenum class Status { alive, suspected, failed, left };\n\nclass Node {\nprivate:\n    std::string nodeId;\n    char *nodeName;\n    char *port;\n    char *tcpPort;\n    Status status;\n    int incarnationNumber;\n    std::string lastUpdated;\n    int ringHash;\npublic:\n    Node();\n    Node(std::string _nodeId, char *_nodeName, char* _port, char* _tcpPort, Status _status, int _incarnationNumber = 1);\n\n    std::string getNodeId() const;\n    char* getNodeName() const;\n    char* getPort() const;\n    char* getTcpPort() const;\n    // get and set ring Hash\n    int getRingHash() const;\n    void setRingHash(int _ringHash);\n    Status getStatus() const;\n    int getIncarnationNumber() const;\n    void setIncarnationNumber(int _incarnationNumber);\n    void incrementIncarnationNumber();\n    void setLastUpdated(const std::string& lastUpdated);\n    std::string getLastUpdated() const;\n    std::string serialise() const;\n    static Node deserialiseNode(std::string serialised);\n    void setStatus(Status _status);\n    void setStatus(std::string _status);\n    void printNode() const;\n\n\n    static Node generateNode(char* port, char *tcpPort);\n};\n\nstd::string convertStatusToString(Status status);\nStatus convertStringToStatus(std::string status);\nlong long getTimeDifference(std::string ts);\nchar* getCurrentTimestamp();\n\n#endif // NODE_H"
    },
    {
      "file_path": "README.md",
      "content": "## CS425 MP3 - Distributed Failure Detector\n\n  \n\nFollow these instructions to run the Hybrid Distributed File System -\n\n  \n\n1. Run following commands on each of the node - \ngit clone [https://github.com/RahulSethi070801/Hybrid-Distributed-File-System.git](https://github.com/RahulSethi070801/Hybrid-Distributed-File-System.git)\nand checkout master\n2. Build the main.cpp program on all machines on your distributed system using\n\tmake\n3. Daemon executables would be build with g++ compiler\n4. Run the introducer on one of the machine using pre configured port number using\n    ./Daemon  $updPort  $tcpPort  introducerVMName  remote  introducer\n5. Run the daemon on all other machines using following command\n\t./Daemon  $updPort  $tcpPort  introducerVMName  remote\n6. To check the list of nodes in the HyDFS system enter list_mem_ids\n7. Store any local files in ./files/remote/local directory\n8. Run create localFileName hyDFSFileName to create hyDFSFileName in the HyDFS\n9. Run get hyDFSFileName localFileName from any node to fetch hyDFSFileName to the local storage\n\n\n\nAlternatively, to build daemon on all machines run bash init.sh. Make sure to follow instruction and change paths to secret tokens and passwords in the script. You can also configure port number for server in the file."
    },
    {
      "file_path": "files/files/Rahuls-MacBook-Pro-2.local#4561#22:46:20/files/Rahuls-MacBook-Pro-2.local#4561#22:46:20/hydfs/files/Rahuls-MacBook-Pro-2.local#4561#22:46:20/hydfs/aa$1",
      "content": "a1"
    },
    {
      "file_path": "files/files/Rahuls-MacBook-Pro-2.local#4561#22:46:20/files/Rahuls-MacBook-Pro-2.local#4561#22:46:20/hydfs/files/Rahuls-MacBook-Pro-2.local#4561#22:46:20/hydfs/aa$2",
      "content": "a1"
    },
    {
      "file_path": "files/files/Rahuls-MacBook-Pro-2.local#4561#22:46:20/files/Rahuls-MacBook-Pro-2.local#4561#22:46:20/local/files/Rahuls-MacBook-Pro-2.local#4561#22:46:20/local/b1.txt",
      "content": "a1a1"
    },
    {
      "file_path": "files/files/Rahuls-MacBook-Pro-2.local#4561#22:46:20/files/Rahuls-MacBook-Pro-2.local#4561#22:46:20/local/files/Rahuls-MacBook-Pro-2.local#4561#22:46:20/local/b2",
      "content": "a1a1"
    },
    {
      "file_path": "files/files/introducer/files/introducer/hydfs/files/introducer/hydfs/aa$1",
      "content": "a1"
    },
    {
      "file_path": "files/files/introducer/files/introducer/hydfs/files/introducer/hydfs/aa$2",
      "content": "a1"
    },
    {
      "file_path": "files/files/introducer/files/introducer/local/files/introducer/local/a.txt",
      "content": "a1"
    },
    {
      "file_path": "files/files/remote/files/remote/local/files/remote/local/appendFile",
      "content": "This is one append\n"
    },
    {
      "file_path": "files/files/remote/files/remote/local/files/remote/local/appendFile40K",
      "content": "This is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\n\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\nThis is one append\n\n"
    },
    {
      "file_path": "files/files/remote/files/remote/local/files/remote/local/business_1.txt",
      "content": "Lufthansa flies back to profit\n\nGerman airline Lufthansa has returned to profit in 2004 after posting huge losses in 2003.\n\nIn a preliminary report, the airline announced net profits of 400m euros ($527.61m; £274.73m), compared with a loss of 984m euros in 2003. Operating profits were at 380m euros, ten times more than in 2003. Lufthansa was hit in 2003 by tough competition and a dip in demand following the Iraq war and the killer SARS virus. It was also hit by troubles at its US catering business. Last year, Lufthansa showed signs of recovery even as some European and US airlines were teetering on the brink of bankruptcy. The board of Lufthansa has recommended paying a 2004 dividend of 0.30 euros per share. In 2003, shareholders did not get a dividend. The company said that it will give all the details of its 2004 results on 23 March.\n"
    },
    {
      "file_path": "files/files/remote/files/remote/local/files/remote/local/business_10.txt",
      "content": "Winn-Dixie files for bankruptcy\n\nUS supermarket group Winn-Dixie has filed for bankruptcy protection after succumbing to stiff competition in a market dominated by Wal-Mart.\n\nWinn-Dixie, once among the most profitable of US grocers, said Chapter 11 protection would enable it to successfully restructure. It said its 920 stores would remain open, but analysts said it would most likely off-load a number of sites. The Jacksonville, Florida-based firm has total debts of $1.87bn (£980m). In its bankruptcy petition it listed its biggest creditor as US foods giant Kraft Foods, which it owes $15.1m.\n\nAnalysts say Winn-Dixie had not kept up with consumers' demands and had also been burdened by a number of stores in need of upgrading. A 10-month restructuring plan was deemed a failure, and following a larger-than-expected quarterly loss earlier this month, Winn-Dixie's slide into bankruptcy was widely expected. The company's new chief executive Peter Lynch said Winn-Dixie would use the Chapter 11 breathing space to take the necessary action to turn itself around. \"This includes achieving significant cost reductions, improving the merchandising and customer service in all locations and generating a sense of excitement in the stores,\" he said. Yet Evan Mann, a senior bond analyst at Gimme Credit, said Mr Lynch's job would not be easy, as the bankruptcy would inevitably put off some customers. \"The real big issue is what's going to happen over the next one or two quarters now that they are in bankruptcy and all their customers see this in their local newspapers,\" he said.\n"
    },
    {
      "file_path": "files/files/remote/files/remote/local/files/remote/local/business_11.txt",
      "content": "Saab to build Cadillacs in Sweden\n\nGeneral Motors, the world's largest car maker, has confirmed that it will build a new medium-sized Cadillac BLS at its loss-making Saab factory in Sweden.\n\nThe car, unveiled at the Geneva motor show, is intended to compete in the medium-sized luxury car market. It will not be sold in the US, said GM Europe president Carl-Peter Forster. As part of its efforts to make the US marque appeal to European drivers, the car will be the first Cadillac with a diesel engine.\n\nGM's announcement should go some way to allay fears of the Saab factory's closure. The factory in Trollhaettan has been at the centre of rumours about GM's planned severe cutbacks in its troubled European operations. But the group's new commitment to the Swedish factory may not be welcomed by the group's Opel workers in Ruesselsheim, Germany. They may now have to face a larger proportion of GM's cuts.\n\nNeither will the announcement be seen as unalloyed good news in Sweden, since it reflects Saab's failure to make significant inroads into the lucrative European luxury car market. For years, Saab has consistently said it is competing head-on with BMW, Mercedes and Jaguar. The segment's leaders do not agree.\n\nGM's plans to build the American marque in Sweden is part of its efforts to push it as an alternative luxury brand for European drivers. In the US, it has long been established as an upmarket brand - even the presidential limousine carries the badge. Yet it could prove tough for Cadillac to steal market share from the majors in Europe. Other luxury car makers, most notably the Toyota subsidiary Lexus, have enjoyed tremendous success in the US without managing to make significant inroads in Europe. There, German marques Mercedes Benz and BMW have retained their stranglehold on the luxury market.\n\nBringing Cadillac production to Sweden should help introduce desperately-needed scale to the Saab factory, which currently produces fewer than 130,000 cars per year. That is about half of what major car makers consider sufficient numbers for profitable operations, and Saab is losing money fast - albeit with losses halved in 2004 to $200m (£104m; 151m euros) from $500m the previous year. Beyond the 12,000 job cuts announced last year at its European operations, GM is reducing expenditure by building Saabs, Opels - badged as Vauxhalls in the UK - and now Cadillacs on the same framework, and by allowing the different brands to share parts. Another way to further reduce Saab's losses could be to shift some of the production of Saabs to the US, a market where drivers have adopted it as an upmarket European car. Doing so would remove the exposure to the weak US dollar, which is making Saabs more expensive to US consumers. But not everyone in the industry agree that it would be the best way forward. \"We know that in five years the US dollar will be stronger than it is today,\" the chief executive of a leading European car maker told BBC News. The current trend towards US production was \"stupid\", he said.\n\nIn a separate announcement, GM unveiled a new scheme to allow European consumers the chance to test drive its Opel and Vauxhall models. It is to deploy a fleet of 35,000 test cars across 40 countries, inviting potential buyers to try out a vehicle for 24-hours. It follows a similar initiative by GM in the US. GM said it wanted to change \"customers' perceptions\" about Opel and Vauxhall cars, showing them that the quality had improved in recent years.\n"
    },
    {
      "file_path": "files/files/remote/files/remote/local/files/remote/local/business_12.txt",
      "content": "Bank voted 8-1 for no rate change\n\nThe decision to keep interest rates on hold at 4.75% earlier this month was passed 8-1 by the Bank of England's rate-setting body, minutes have shown.\n\nOne member of the Bank's Monetary Policy Committee (MPC) - Paul Tucker - voted to raise rates to 5%. The news surprised some analysts who had expected the latest minutes to show another unanimous decision. Worries over growth rates and consumer spending were behind the decision to freeze rates, the minutes showed. The Bank's latest inflation report, released last week, had noted that the main reason inflation might fall was weaker consumer spending.\n\nHowever, MPC member Paul Tucker voted for a quarter point rise in interest rates to 5%. He argued that economic growth was picking up, and that the equity, credit and housing markets had been stronger than expected.\n\nThe Bank's minutes said that risks to the inflation forecast were \"sufficiently to the downside\" to keep rates on hold at its latest meeting. However, the minutes added: \"Some members noted that an increase might be warranted in due course if the economy evolved in line with the central projection\". Ross Walker, UK economist at Royal Bank of Scotland, said he was surprised that a dissenting vote had been made so soon. He said the minutes appeared to be \"trying to get the market to focus on the possibility of a rise in rates\". \"If the economy pans out as they expect then they are probably going to have to hike rates.\" However, he added, any rate increase is not likely to happen until later this year, with MPC members likely to look for a more sustainable pick up in consumer spending before acting.\n"
    },
    {
      "file_path": "files/files/remote/files/remote/local/files/remote/local/business_13.txt",
      "content": "Industrial revival hope for Japan\n\nJapanese industry is growing faster than expected, boosting hopes that the country's retreat back into recession is over.\n\nIndustrial output rose 2.1% - adjusted for the time of year - in January from a month earlier. At the same time, retail sales picked up faster than at any time since 1997. The news sent Tokyo shares to an eight-month high, as investors hoped for a recovery from the three quarters of contraction seen from April 2004 on. The Nikkei 225 index ended the day up 0.7% at 11,740.60 points, with the yen strengthening 0.7% against the dollar to 104.53 yen. Weaker exports, normally the engine for Japan's economy in the face of weak domestic demand, had helped trigger a 0.1% contraction in the final three months of last year after two previous quarters of shrinking GDP. Only an exceptionally strong performance in the early months of 2004 kept the year as a whole from showing a decline. The output figures brought a cautiously optimistic response from economic officials. \"Overall I see a low risk of the economy falling into serious recession,\" said Bank of Japan chief Toshihiko Fukui, despite warning that other indicators - such as the growth numbers - had been worrying.\n\nWithin the overall industrial output figure, there were signs of a pullback from the export slowdown. Among the best-performing sectors were key overseas sales areas such as cars, chemicals and electronic goods. With US growth doing better than expected the picture for exports in early 2005 could also be one of sustained demand. Electronics were also one of the keys to the improved domestic market, with products such as flat-screen TVs in high demand during January.\n"
    },
    {
      "file_path": "files/files/remote/files/remote/local/files/remote/local/business_14.txt",
      "content": "Khodorkovsky ally denies charges\n\nA close associate of former Yukos boss Mikhail Khodorkovsky has told a court that fraud charges levelled against him are \"false\".\n\nPlaton Lebedev has been on trial alongside Mr Khodorkovsky since June in a case centring around the privatisation of a fertiliser firm. The pair claim they are being punished by the authorities for the political ambitions of Mr Khodorkovsky. Mr Lebedev said there were \"absurd contradictions\" in the case. Opening his defence, he said he could not see the legal basis of the charges he faced, which also include allegations of tax evasion. \"To my embarrassment, I could not understand the file of complaints against me,\" he told a Moscow court. Mr Lebedev headed the Menatep group, the parent company of Yukos.\n\nMr Lebedev and Mr Khodorkovsky, who each face a possible 10 year jail sentence if convicted, will be questioned by a judge over the next few days. Mr Khodorkovsky began his testimony last week, telling the court that he objected to the way that the \"running of a normal business has been presented as a work of criminal fiction\". The charges are seen by supporters as politically motivated and part of a drive by Russian President Vladimir Putin to rein in the country's super-rich business leaders, the so-called oligarchs. Yukos has been presented with a $27.5bn (£13bn) tax demand by the Russian authorities and its key Yugansk division was auctioned off to part settle the bill. The company's effort to gain bankruptcy protection in the US - in a bid to win damages for the sale - were dismissed by a court in Texas.\n"
    },
    {
      "file_path": "files/files/remote/files/remote/local/files/remote/local/business_15.txt",
      "content": "China keeps tight rein on credit\n\nChina's efforts to stop the economy from overheating by clamping down on credit will continue into 2005, state media report.\n\nThe curbs were introduced earlier this year to ward off the risk that rapid expansion might lead to soaring prices. There were also fears that too much stress might be placed on the fragile banking system. Growth in China remains at a breakneck 9.1%, and corporate investment is growing at more than 25% a year. The breakneck pace of economic expansion has kept growth above 9% for more than a year. Rapid tooling-up of China's manufacturing sector means a massive demand for energy - one of the factors which has kept world oil prices sky-high for most of this year. In theory, the government has a 7% growth target, but continues to insist that the overshoot does not mean a \"hard landing\" in the shape of an overbalancing economy. A low exchange rate - China's yuan is pegged to a rate of 8.28 to the dollar, which seems to be in relentless decline - means Chinese exports are cheap on world markets. China has thus far resisted international pressure to break the link or at least to shift the level of its peg. To some extent, the credit controls do seem to be taking effect. Industrial output grew 15.7% in the year to October, down from 23% in February, and inflation slowed to 4.3% - although retail sales are still booming.\n"
    },
    {
      "file_path": "files/files/remote/files/remote/local/files/remote/local/business_16.txt",
      "content": "Verizon 'seals takeover of MCI'\n\nVerizon has won a takeover battle for US phone firm MCI with a bid worth $6.8bn (£3.6bn), reports say.\n\nThe two firms are expected to seal the deal on Monday morning, according to news agency reports, despite what was thought to be a higher bid from Qwest. The US telecoms market is consolidating fast, with former long-distance giant AT&T being bought by former subsidiary SBC earlier this year for $16bn. MCI exited bankruptcy in April, having gone bust under previous name WorldCom. The bankruptcy followed its admission in 2002 that it illegally booked expenses and inflated profits.\n\nShareholders lost about $180bn when the company collapsed, while 20,000 workers lost their jobs. Former Worldcom boss Bernie Ebbers is currently on trial, accused of overseeing an $11bn fraud. Qwest has itself come under suspicion of sub-standard behaviour, paying the Securities and Exchange Commission $250m in October to settle charges that it manipulated its results to keep Wall Street happy.\n\nMCI is the US's second-biggest long distance firm after AT&T. Consolidation in the US telecommunications industry has picked up in the past few months as companies look to cut costs and boost client bases. A merger between MCI and Verizon would be the fifth billion-dollar telecoms deal since October. Last week, SBC Communications agreed to buy its former parent and phone trailblazer AT&T for about $16bn. Buying MCI would give either Qwest or Verizon access to MCI's global network and business-based subscribers. The rationale is similar to the one underpinning SBC's AT&T deal. Verizon is by far the bigger company and has its own successful mobile arm - factors which may have swung the board in its favour since both suitors are offering a mixture of cash and shares.\n"
    },
    {
      "file_path": "files/files/remote/files/remote/local/files/remote/local/business_17.txt",
      "content": "Crossrail link 'to get go-ahead'\n\nThe £10bn Crossrail transport plan, backed by business groups, is to get the go-ahead this month, according to The Mail on Sunday.\n\nIt says the UK Treasury has allocated £7.5bn ($13.99bn) for the project and that talks with business groups on raising the rest will begin shortly. The much delayed Crossrail Link Bill would provide for a fast cross-London rail link. The paper says it will go before the House of Commons on 23 February.\n\nA second reading could follow on 16 or 17 March. \"We've always said we are going to introduce a hybrid Bill for Crossrail in the Spring and this remains the case,\" the Department for Transport said on Sunday. Jeremy de Souza, a spokesman for Crossrail, said on Sunday he could not confirm whether the Treasury was planning to invest £7.5bn or when the bill would go before Parliament.\n\nHowever, he said some impetus may have been provided by the proximity of an election.\n\nThe new line would go out as far as Maidenhead, Berkshire, to the west of London, and link Heathrow to Canary Wharf via the City. Heathrow to the City would take 40 minutes, dramatically cutting journey times for business travellers, and reducing overcrowding on the tube. The line has the support of the Mayor of London, Ken Livingstone, business groups and the government, but there have been three years of arguments over how it should be funded. The Mail on Sunday's Financial Mail said the £7.5bn of Treasury money was earmarked for spending in £2.5bn instalments in 2010, 2011 and 2012.\n"
    },
    {
      "file_path": "files/files/remote/files/remote/local/files/remote/local/business_18.txt",
      "content": "Small firms 'hit by rising costs'\n\nRising fuel and materials costs are hitting confidence among the UK's small manufacturers despite a rise in output, business lobby group the CBI says.\n\nA CBI quarterly survey found output had risen by the fastest rate in seven years but many firms were seeing the benefits offset by increasing expenses. The CBI also found spending on innovation, training and retraining is forecast to go up over the next year. However, firms continue to scale back investment in buildings and machinery.\n\nThe CBI said companies are looking to the government to lessen the regulatory load and are hoping interest rates will be kept on hold. \"Smaller manufacturers are facing an uphill struggle,\" said Hugh Morgan Williams, chair of the CBI's SME Council. \"The manufacturing sector needs a period of long-term stability in the economy.\" The CBI found some firms managed to increase prices for the first time in nine years - but many said increases failed to keep up the rise in costs. Of the companies surveyed, 30% saw orders rise and 27% saw them fall. The positive balance of plus 3 compared with minus 10 in the previous survey. When firms were questioned on output volume, the survey returned a balance of plus 8 - the highest rate of increase for seven years - and rose to plus 11 when looking ahead to the next three months.\n"
    },
    {
      "file_path": "files/files/remote/files/remote/local/files/remote/local/business_19.txt",
      "content": "Deutsche Boerse boosts dividend\n\nDeutsche Boerse, the German stock exchange that is trying to buy its London rival, has said it will boost its 2004 dividend payment by 27%.\n\nAnalysts said that the move is aimed at winning over investors opposed to its bid for the London Stock Exchange. Critics of the takeover have complained that the money could be better used by returning cash to shareholders. Deutsche Boerse also said profit in the three months to 31 December was 120.7m euros ($158.8m; £83.3m). Sales climbed to 364.4m euros, lifting revenue for the year to a record 1.45bn euros.\n\nFrankfurt-based Deutsche Boerse has offered £1.3bn ($2.48bn; 1.88bn euros) for the London Stock Exchange. Rival pan-European bourse Euronext is working also on a bid. Late on Monday, Deutsche Boerse said it would lift its 2004 dividend payment to 70 euro cents (£0.48; $0.98) from 55 euro cents a year earlier. \"There is a whiff of a sweetener in there,\" Anais Faraj, an analyst at Nomura told the BBC's World Business Report. \"Most of the disgruntled shareholders of Deutsche Boerse are complaining that the money that is being used for the bid could be better placed in their hands, paid out in dividends,\" Mr Faraj continued. Deutsche Boerse is \"trying to buy them off in a sense\", he said.\n"
    },
    {
      "file_path": "files/files/remote/files/remote/local/files/remote/local/business_2.txt",
      "content": "Japanese growth grinds to a halt\n\nGrowth in Japan evaporated in the three months to September, sparking renewed concern about an economy not long out of a decade-long trough.\n\nOutput in the period grew just 0.1%, an annual rate of 0.3%. Exports - the usual engine of recovery - faltered, while domestic demand stayed subdued and corporate investment also fell short. The growth falls well short of expectations, but does mark a sixth straight quarter of expansion.\n\nThe economy had stagnated throughout the 1990s, experiencing only brief spurts of expansion amid long periods in the doldrums. One result was deflation - prices falling rather than rising - which made Japanese shoppers cautious and kept them from spending.\n\nThe effect was to leave the economy more dependent than ever on exports for its recent recovery. But high oil prices have knocked 0.2% off the growth rate, while the falling dollar means products shipped to the US are becoming relatively more expensive.\n\nThe performance for the third quarter marks a sharp downturn from earlier in the year. The first quarter showed annual growth of 6.3%, with the second showing 1.1%, and economists had been predicting as much as 2% this time around. \"Exports slowed while capital spending became weaker,\" said Hiromichi Shirakawa, chief economist at UBS Securities in Tokyo. \"Personal consumption looks good, but it was mainly due to temporary factors such as the Olympics. \"The amber light is flashing.\" The government may now find it more difficult to raise taxes, a policy it will have to implement when the economy picks up to help deal with Japan's massive public debt.\n"
    },
    {
      "file_path": "files/files/remote/files/remote/local/files/remote/local/business_20.txt",
      "content": "Brewers' profits lose their fizz\n\nHeineken and Carlsberg, two of the world's largest brewers, have reported falling profits after beer sales in western Europe fell flat.\n\nDutch firm Heineken saw its annual profits drop 33% and warned that earnings in 2005 may also slide. Danish brewer Carlsberg suffered a 3% fall in profits due to waning demand and increased marketing costs. Both are looking to Russia and China to provide future growth as western European markets are largely mature.\n\nHeineken's net income fell to 537m euros ($701m; £371m) during 2004, from 798m euro a year ago. It blamed weak demand in western Europe and currency losses. It had warned in September that the weakening US dollar, which has cut the value of foreign sales, would knock 125m euros off its operating profits. Despite the dip in profits, Heineken's sales have been improving and total revenue for the year was 10bn euros, up 8.1% from 9.26bn euros in 2003. Heineken said it now plans to invest 100m euros in \"aggressive\" and \"high-impact\" marketing in Europe and the US in 2005. Heineken, which also owns the Amstel and Murphy's stout brands, said it would also seek to cut costs. This may involve closing down breweries.\n\nHeineken increased its dividend payment by 25% to 40 euro cents, but warned that the continued impact of a weaker dollar and an increased marketing spend may lead to a drop in 2005 net profit.\n\nCarlsberg, the world's fifth-largest brewer, saw annual pre-tax profits fall to 3.4bn Danish kroner (456m euros). Its beer sales have been affected by the sluggish European economy and by the banning of smoking in pubs in several European countries. Nevertheless, total sales increased 4% to 36bn kroner, thanks to strong sales of Carlsberg lager in Russia and Poland. Carlsberg is more optimistic than Heineken about 2005, projecting a 15% rise in net profits for the year. However, it also plans to cut 200 jobs in Sweden, where sales have been hit by demand for cheap, imported brands. \"We remain cautious about the medium-to-long term outlook for revenue growth across western Europe for a host of economic, social and structural reasons,\" investment bank Merrill Lynch said of Carlsberg.\n"
    },
    {
      "file_path": "files/files/remote/files/remote/local/files/remote/local/business_3.txt",
      "content": "WorldCom director admits lying\n\nThe former chief financial officer at US telecoms firm WorldCom has admitted before a New York court that he used to lie to fellow board members.\n\nSpeaking at the trial of his former boss Bernard Ebbers, Scott Sullivan said he lied to the board to cover up the hole in WorldCom's finances. Mr Ebbers is on trial for fraud and conspiracy in relation to WorldCom's collapse in 2002. He pleads not guilty. The firm had been overstating its accounts by $11bn (£8.5bn). Mr Sullivan, 42, has already pleaded guilty to fraud and will be sentenced following Mr Ebbers' trial, where he is appearing as a prosecution witness. Mr Ebbers, 63, has always insisted that he was unaware of any hidden shortfalls in WorldCom's finances.\n\nIn the New York court on Wednesday, Mr Ebbers' lawyer Reid Weingarten asked Mr Sullivan: \"If you believe something is in your interest, you are willing and able to lie to accomplish it, isn't that right?\"\n\n\"On that date, yes. I was lying,\" replied Mr Sullivan. Mr Weingarten has suggested that Mr Sullivan is implicating Mr Ebbers only to win a lighter sentence, something Mr Sullivan denies. Mr Sullivan also rejects a suggestion that he had once told fellow WorldCom board member Bert Roberts that Mr Ebbers was unaware of the accounting fraud at WorldCom. The trial of Mr Ebbers is now into its third week.\n\nUnder 23 hours of questioning from a federal prosecutor, Mr Sullivan has previously told the court that he repeatedly warned Mr Ebbers that falsifying the books would be the only way to meet Wall Street revenue and earnings expectations. Mr Sullivan claims that Mr Ebbers refused to stop the fraud. Mr Ebbers could face a sentence of 85 years if convicted of all the charges he is facing. WorldCom's problems appear to have begun with the collapse of the dotcom boom which cut its business from internet companies. Prosecutors allege that the company's top executives responded by orchestrating massive fraud over a two-year period. WorldCom emerged from bankruptcy protection in 2004, and is now known as MCI.\n"
    },
    {
      "file_path": "files/files/remote/files/remote/local/files/remote/local/business_4.txt",
      "content": "Glaxo aims high after profit fall\n\nGlaxoSmithKline saw its profits fall 9% last year to £6.2bn ($11.5bn), but Europe's biggest drugmaker says a recovery during 2005 is on the way.\n\nCheap copies of its drugs, particularly anti-depressants Paxil and Wellbutrin, and a weak dollar had hit profits, but global sales were up 1% in 2004. The firm is confident its new drug pipeline will deliver profits despite the failure of an obesity drug. Chief executive Jean-Pierre Garnier said it had been a \"difficult year\".\n\nIn early afternoon trade in London the company share price was down 1% at 1218 pence. Mr Garnier said the company had absorbed over £1.5bn of lost sales to generics but still managing to grow the business. \"The continuing success of our key products means we can now look forward to a good performance in 2005,\" he said. \"2005 will also be an important year in terms of research and development pipeline progress.\" However, the firm discontinued development of an experimental treatment for obesity, known as '771, after disappointing clinical trial results. Glaxo is relying on new treatments for conditions such as cancer, diabetes, depression, HIV/AIDS and allergies to lift the pace of sales growth after several disappointing years.\n"
    },
    {
      "file_path": "files/files/remote/files/remote/local/files/remote/local/business_5.txt",
      "content": "Peugeot deal boosts Mitsubishi\n\nStruggling Japanese car maker Mitsubishi Motors has struck a deal to supply French car maker Peugeot with 30,000 sports utility vehicles (SUV).\n\nThe two firms signed a Memorandum of Understanding, and say they expect to seal a final agreement by Spring 2005. The alliance comes as a badly-needed boost for loss-making Mitsubishi, after several profit warnings and poor sales. The SUVs will be built in Japan using Peugeot's diesel engines and sold mainly in the European market. Falling sales have left Mitsubishi Motors with underused capacity, and the production deal with Peugeot gives it a chance to utilise some of it.\n\nIn January, Mitsubishi Motors issued its third profits warning in nine months, and cut its sales forecasts for the year to March 2005. Its sales have slid 41% in the past year, catalysed by the revelation that the company had systematically been hiding records of faults and then secretly repairing vehicles. As a result, the Japanese car maker has sought a series of financial bailouts. Last month it said it was looking for a further 540bn yen ($5.2bn; £2.77bn) in fresh financial backing, half of it from other companies in the Mitsubishi group. US-German carmaker DaimlerChrylser, a 30% shareholder in Mitsubishi Motors, decided in April 2004 not to pump in any more money. The deal with Peugeot was celebrated by Mitsubishi's newly-appointed chief executive Takashi Nishioka, who took over after three top bosses stood down last month to shoulder responsibility for the firm's troubles. Mitsubishi Motors has forecast a net loss of 472bn yen in its current financial year to March 2005. Last month, it signed a production agreement with Japanese rival Nissan Motor to supply it with 36,000 small cars for sale in Japan. It has been making cars for Nissan since 2003.\n"
    },
    {
      "file_path": "files/files/remote/files/remote/local/files/remote/local/business_6.txt",
      "content": "US adds more jobs than expected\n\nThe US economy added 337,000 jobs in October - a seven-month high and far more than Wall Street expectations.\n\nIn a welcome economic boost for newly re-elected President George W Bush, the Labor Department figures come after a slow summer of weak jobs gains. Jobs were created in every sector of the US economy except manufacturing. While the separate unemployment rate went up to 5.5% from 5.4% in September, this was because more people were now actively seeking work.\n\nThe 337,000 new jobs added to US payrolls in October was twice the 169,000 figure that Wall Street economists had forecast. In addition, the Labor Department revised up the number of jobs created in the two previous months - to 139,000 in September instead of 96,000, and to 198,000 in August instead of 128,000. The better than expected jobs data had an immediate upward effect on stocks in New York, with the main Dow Jones index gaining 45.4 points to 10,360 by late morning trading. \"It looks like the job situation is improving and that this will support consumer spending going into the holidays, and offset some of the drag caused by high oil prices this year,\" said economist Gary Thayer of AG Edwards & Sons.\n\nOther analysts said the upbeat jobs data made it more likely that the US Federal Reserve would increase interest rates by a quarter of a percentage point to 2% when it meets next week. \"It should empower the Fed to clearly do something,\" said Robert MacIntosh, chief economist with Eaton Vance Management in Boston. Kathleen Utgoff, commissioner of the Bureau of Labor, said many of the 71,000 new construction jobs added in October were involved in rebuilding and clean-up work in Florida, and neighbouring Deep South states, following four hurricanes in August and September. The dollar rose temporarily on the job creation news before falling back to a new record low against the euro, as investors returned their attention to other economic factors, such as the US's record trade deficit. There is also speculation that President Bush will deliberately try to keep the dollar low in order to assist a growth in exports.\n"
    },
    {
      "file_path": "files/files/remote/files/remote/local/files/remote/local/business_7.txt",
      "content": "German jobless rate at new record\n\nMore than 5.2 million Germans were out of work in February, new figures show.\n\nThe figure of 5.216 million people, or 12.6% of the working-age population, is the highest jobless rate in Europe's biggest economy since the 1930s. The news comes as the head of Germany's panel of government economic advisers predicted growth would again stagnate. Speaking on German TV, Bert Ruerup said the panel's earlier forecast of 1.4% was too optimistic and warned growth would be just 1% in 2005.\n\nThe German government is trying to tackle the stubbornly-high levels of joblessness with a range of labour market reforms. At their centre is the \"Hartz-IV\" programme introduced in January to shake up welfare benefits and push people back into work - even if some of the jobs are heavily subsidised. The latest unemployment figures look set to increase the pressure on the government. Widely leaked to the German newspapers a day in advance, they produced screaming headlines criticising Chancellor Gerhard Schroeder's Social Democrat-Green Party administration. Mr Schroeder had originally come into office promising to halve unemployment.\n\nStill, some measures suggest the picture is not quite so bleak. The soaring official unemployment figure follows a change in the methodology which pushed up the jobless rate by more than 500,000 in January. Adjusted for seasonal changes, the overall unemployment rate is 4.875 million people or 11.7%, up 0.3 percentage points from the previous month. Using the most internationally-accepted methodology of the International Labour Organisation (ILO), Germany had 3.97 million people out of work in January. And ILO-based figures also suggest that 14,000 new net jobs were created that month, taking the number of people employed to 38.9 million. The ILO defines an unemployed person as someone who in the previous four weeks had actively looked for work they could take up immediately.\n"
    },
    {
      "file_path": "files/files/remote/files/remote/local/files/remote/local/business_8.txt",
      "content": "Weak dollar trims Cadbury profits\n\nThe world's biggest confectionery firm, Cadbury Schweppes, has reported a modest rise in profits after the weak dollar took a bite out of its results.\n\nUnderlying pre-tax profits rose 1% to £933m ($1.78bn) in 2004, but would have been 8% higher if currency movements were stripped out. The owner of brands such as Dairy Milk, Dr Pepper and Snapple generates more than 80% of its sales outside the UK. Cadbury said it was confident it would hit its targets for 2005. \"While the external commercial environment remains competitive, we are confident that we have the strategy, brands and people to deliver within our goal ranges in 2005,\" said chief executive Todd Stitzer.\n\nThe modest profit rise had been expected by analysts after the company said in December that the poor summer weather had hit soft drink sales in Europe.\n\nCadbury said its underlying sales were up by 4% in 2004. Growth was helped by its confectionery brands - including Cadbury, Trident and Halls - which enjoyed a \"successful\" year, with like-for-like sales up 6%. Drinks sales were up 2% with strong growth in US carbonated soft drinks, led by Dr Pepper and diet drinks, offset by the weaker sales in Europe. Cadbury added that its Fuel for Growth cost-cutting programme had saved £75m in 2004, bringing total cost savings to £100m since the scheme began in mid-2003. The programme is set to close 20% of the group's factories and shed 10% of the workforce. Cadbury Schweppes employs more than 50,000 people worldwide, with about 7,000 in the UK.\n"
    },
    {
      "file_path": "files/files/remote/files/remote/local/files/remote/local/business_9.txt",
      "content": "Oil prices fall back from highs\n\nOil prices retreated from four-month highs in early trading on Tuesday after producers' cartel Opec said it was now unlikely to cut production.\n\nFollowing the comments by acting Opec secretary general Adnan Shihab-Eldin, US light crude fell 32 cents to $51.43 a barrel. He said that high oil prices meant Opec was unlikely to stick to its plan to cut output in the second quarter. In London, Brent crude fell 32 cents to $49.74 a barrel.\n\nOpec members are next meeting to discuss production levels on 16 March. On Monday, oil prices rose for a sixth straight session, reaching a four-month high as cold weather in the US threatened stocks of heating oil. US demand for heating oil was predicted to be about 14% above normal this week, while stocks were currently about 7.5% below the levels of a year ago. Cold weather across Europe has also put upward pressure on crude prices.\n"
    },
    {
      "file_path": "files/files/remote/files/remote/local/files/remote/local/foo1.txt",
      "content": "this is foo1 append\n"
    },
    {
      "file_path": "files/files/remote/files/remote/local/files/remote/local/foo2.txt",
      "content": "This is foo2 append\n"
    },
    {
      "file_path": "files/files/remote/files/remote/local/files/remote/local/local",
      "content": "\nMain menu\n\nWikipediaThe Free Encyclopedia\nSearch Wikipedia\nSearch\nDonate\nCreate account\nLog in\n\nPersonal tools\nContents hide\n(Top)\nIntroduction\nPatterns\nParallel and distributed computing\nHistory\nArchitectures\nApplications\nExamples\nReactive distributed systems\nTheoretical foundations\nToggle Theoretical foundations subsection\nModels\nAn example\nComplexity measures\nOther problems\nElection\nProperties of distributed systems\nSee also\nNotes\nReferences\nFurther reading\nExternal links\nDistributed computing\n\nArticle\nTalk\nRead\nEdit\nView history\n\nTools\nAppearance hide\nText\n\nSmall\n\nStandard\n\nLarge\nWidth\n\nStandard\n\nWide\nColor (beta)\n\nAutomatic\n\nLight\n\nDark\nFrom Wikipedia, the free encyclopedia\nNot to be confused with Decentralized computing.\nDistributed computing is a field of computer science that studies distributed systems, defined as computer systems whose inter-communicating components are located on different networked computers.[1][2]\n\nThe components of a distributed system communicate and coordinate their actions by passing messages to one another in order to achieve a common goal. Three significant challenges of distributed systems are: maintaining concurrency of components, overcoming the lack of a global clock, and managing the independent failure of components.[1] When a component of one system fails, the entire system does not fail.[3] Examples of distributed systems vary from SOA-based systems to microservices to massively multiplayer online games to peer-to-peer applications. Distributed systems cost significantly more than monolithic architectures, primarily due to increased needs for additional hardware, servers, gateways, firewalls, new subnets, proxies, and so on.[4] Also, distributed systems are prone to fallacies of distributed computing. On the other hand, a well designed distributed system is more scalable, more durable, more changeable and more fine-tuned than a monolithic application deployed on a single machine.[5] According to Marc Brooker: \"a system is scalable in the range where marginal cost of additional workload is nearly constant.\" Serverless technologies fit this definition but you need to consider total cost of ownership not just the infra cost. [6]\n\nA computer program that runs within a distributed system is called a distributed program,[7] and distributed programming is the process of writing such programs.[8] There are many different types of implementations for the message passing mechanism, including pure HTTP, RPC-like connectors and message queues.[9]\n\nDistributed computing also refers to the use of distributed systems to solve computational problems. In distributed computing, a problem is divided into many tasks, each of which is solved by one or more computers,[10] which communicate with each other via message passing.[11]\n\nIntroduction\nThe word distributed in terms such as \"distributed system\", \"distributed programming\", and \"distributed algorithm\" originally referred to computer networks where individual computers were physically distributed within some geographical area.[12] The terms are nowadays used in a much wider sense, even referring to autonomous processes that run on the same physical computer and interact with each other by message passing.[11]\n\nWhile there is no single definition of a distributed system,[13] the following defining properties are commonly used as:\n\nThere are several autonomous computational entities (computers or nodes), each of which has its own local memory.[14]\nThe entities communicate with each other by message passing.[15]\nA distributed system may have a common goal, such as solving a large computational problem;[16] the user then perceives the collection of autonomous processors as a unit. Alternatively, each computer may have its own user with individual needs, and the purpose of the distributed system is to coordinate the use of shared resources or provide communication services to the users.[17]\n\nOther typical properties of distributed systems include the following:\n\nThe system has to tolerate failures in individual computers.[18]\nThe structure of the system (network topology, network latency, number of computers) is not known in advance, the system may consist of different kinds of computers and network links, and the system may change during the execution of a distributed program.[19]\nEach computer has only a limited, incomplete view of the system. Each computer may know only one part of the input.[20]\nPatterns\nHere are common architectural patterns used for distributed computing:[21]\n\nSaga interaction pattern\nMicroservices\nEvent driven architecture\nParallel and distributed computing\n\n(a), (b): a distributed system.\n(c): a parallel system.\nDistributed systems are groups of networked computers which share a common goal for their work. The terms \"concurrent computing\", \"parallel computing\", and \"distributed computing\" have much overlap, and no clear distinction exists between them.[22] The same system may be characterized both as \"parallel\" and \"distributed\"; the processors in a typical distributed system run concurrently in parallel.[23] Parallel computing may be seen as a particularly tightly coupled form of distributed computing,[24] and distributed computing may be seen as a loosely coupled form of parallel computing.[13] Nevertheless, it is possible to roughly classify concurrent systems as \"parallel\" or \"distributed\" using the following criteria:\n\nIn parallel computing, all processors may have access to a shared memory to exchange information between processors.[25]\nIn distributed computing, each processor has its own private memory (distributed memory). Information is exchanged by passing messages between the processors.[26]\nThe figure on the right illustrates the difference between distributed and parallel systems. Figure (a) is a schematic view of a typical distributed system; the system is represented as a network topology in which each node is a computer and each line connecting the nodes is a communication link. Figure (b) shows the same distributed system in more detail: each computer has its own local memory, and information can be exchanged only by passing messages from one node to another by using the available communication links. Figure (c) shows a parallel system in which each processor has a direct access to a shared memory.\n\nThe situation is further complicated by the traditional uses of the terms parallel and distributed algorithm that do not quite match the above definitions of parallel and distributed systems (see below for more detailed discussion). Nevertheless, as a rule of thumb, high-performance parallel computation in a shared-memory multiprocessor uses parallel algorithms while the coordination of a large-scale distributed system uses distributed algorithms.[27]\n\nHistory\nThe use of concurrent processes which communicate through message-passing has its roots in operating system architectures studied in the 1960s.[28] The first widespread distributed systems were local-area networks such as Ethernet, which was invented in the 1970s.[29]\n\nARPANET, one of the predecessors of the Internet, was introduced in the late 1960s, and ARPANET e-mail was invented in the early 1970s. E-mail became the most successful application of ARPANET,[30] and it is probably the earliest example of a large-scale distributed application. In addition to ARPANET (and its successor, the global Internet), other early worldwide computer networks included Usenet and FidoNet from the 1980s, both of which were used to support distributed discussion systems.[31]\n\nThe study of distributed computing became its own branch of computer science in the late 1970s and early 1980s. The first conference in the field, Symposium on Principles of Distributed Computing (PODC), dates back to 1982, and its counterpart International Symposium on Distributed Computing (DISC) was first held in Ottawa in 1985 as the International Workshop on Distributed Algorithms on Graphs.[32]\n\nArchitectures\nVarious hardware and software architectures are used for distributed computing. At a lower level, it is necessary to interconnect multiple CPUs with some sort of network, regardless of whether that network is printed onto a circuit board or made up of loosely coupled devices and cables. At a higher level, it is necessary to interconnect processes running on those CPUs with some sort of communication system.[33]\n\nWhether these CPUs share resources or not determines a first distinction between three types of architecture:\n\nShared memory\nShared disk\nShared nothing.\nDistributed programming typically falls into one of several basic architectures: client–server, three-tier, n-tier, or peer-to-peer; or categories: loose coupling, or tight coupling.[34]\n\nClient–server: architectures where smart clients contact the server for data then format and display it to the users. Input at the client is committed back to the server when it represents a permanent change.\nThree-tier: architectures that move the client intelligence to a middle tier so that stateless clients can be used. This simplifies application deployment. Most web applications are three-tier.\nn-tier: architectures that refer typically to web applications which further forward their requests to other enterprise services. This type of application is the one most responsible for the success of application servers.\nPeer-to-peer: architectures where there are no special machines that provide a service or manage the network resources.[35]: 227  Instead all responsibilities are uniformly divided among all machines, known as peers. Peers can serve both as clients and as servers.[36] Examples of this architecture include BitTorrent and the bitcoin network.\nAnother basic aspect of distributed computing architecture is the method of communicating and coordinating work among concurrent processes. Through various message passing protocols, processes may communicate directly with one another, typically in a main/sub relationship. Alternatively, a \"database-centric\" architecture can enable distributed computing to be done without any form of direct inter-process communication, by utilizing a shared database.[37] Database-centric architecture in particular provides relational processing analytics in a schematic architecture allowing for live environment relay. This enables distributed computing functions both within and beyond the parameters of a networked database.[38]\n\nApplications\nReasons for using distributed systems and distributed computing may include:\n\nThe very nature of an application may require the use of a communication network that connects several computers: for example, data produced in one physical location and required in another location.\nThere are many cases in which the use of a single computer would be possible in principle, but the use of a distributed system is beneficial for practical reasons. For example:\nIt can allow for much larger storage and memory, faster compute, and higher bandwidth than a single machine.\nIt can provide more reliability than a non-distributed system, as there is no single point of failure. Moreover, a distributed system may be easier to expand and manage than a monolithic uniprocessor system.[39]\nIt may be more cost-efficient to obtain the desired level of performance by using a cluster of several low-end computers, in comparison with a single high-end computer.\nExamples\nExamples of distributed systems and applications of distributed computing include the following:[40]\n\ntelecommunications networks:\ntelephone networks and cellular networks,\ncomputer networks such as the Internet,\nwireless sensor networks,\nrouting algorithms;\nnetwork applications:\nWorld Wide Web and peer-to-peer networks,\nmassively multiplayer online games and virtual reality communities,\ndistributed databases and distributed database management systems,\nnetwork file systems,\ndistributed cache such as burst buffers,\ndistributed information processing systems such as banking systems and airline reservation systems;\nreal-time process control:\naircraft control systems,\nindustrial control systems;\nparallel computation:\nscientific computing, including cluster computing, grid computing, cloud computing,[41] and various volunteer computing projects,\ndistributed rendering in computer graphics.\npeer-to-peer\nReactive distributed systems\nAccording to Reactive Manifesto, reactive distributed systems are responsive, resilient, elastic and message-driven. Subsequently, Reactive systems are more flexible, loosely-coupled and scalable. To make your systems reactive, you are advised to implement Reactive Principles. Reactive Principles are a set of principles and patterns which help to make your cloud native application as well as edge native applications more reactive. [42]\n\nTheoretical foundations\nMain article: Distributed algorithm\nModels\nMany tasks that we would like to automate by using a computer are of question–answer type: we would like to ask a question and the computer should produce an answer. In theoretical computer science, such tasks are called computational problems. Formally, a computational problem consists of instances together with a solution for each instance. Instances are questions that we can ask, and solutions are desired answers to these questions.\n\nTheoretical computer science seeks to understand which computational problems can be solved by using a computer (computability theory) and how efficiently (computational complexity theory). Traditionally, it is said that a problem can be solved by using a computer if we can design an algorithm that produces a correct solution for any given instance. Such an algorithm can be implemented as a computer program that runs on a general-purpose computer: the program reads a problem instance from input, performs some computation, and produces the solution as output. Formalisms such as random-access machines or universal Turing machines can be used as abstract models of a sequential general-purpose computer executing such an algorithm.[43][44]\n\nThe field of concurrent and distributed computing studies similar questions in the case of either multiple computers, or a computer that executes a network of interacting processes: which computational problems can be solved in such a network and how efficiently? However, it is not at all obvious what is meant by \"solving a problem\" in the case of a concurrent or distributed system: for example, what is the task of the algorithm designer, and what is the concurrent or distributed equivalent of a sequential general-purpose computer?[citation needed]\n\nThe discussion below focuses on the case of multiple computers, although many of the issues are the same for concurrent processes running on a single computer.\n\nThree viewpoints are commonly used:\n\nParallel algorithms in shared-memory model\nAll processors have access to a shared memory. The algorithm designer chooses the program executed by each processor.\nOne theoretical model is the parallel random-access machines (PRAM) that are used.[45] However, the classical PRAM model assumes synchronous access to the shared memory.\nShared-memory programs can be extended to distributed systems if the underlying operating system encapsulates the communication between nodes and virtually unifies the memory across all individual systems.\nA model that is closer to the behavior of real-world multiprocessor machines and takes into account the use of machine instructions, such as Compare-and-swap (CAS), is that of asynchronous shared memory. There is a wide body of work on this model, a summary of which can be found in the literature.[46][47]\nParallel algorithms in message-passing model\nThe algorithm designer chooses the structure of the network, as well as the program executed by each computer.\nModels such as Boolean circuits and sorting networks are used.[48] A Boolean circuit can be seen as a computer network: each gate is a computer that runs an extremely simple computer program. Similarly, a sorting network can be seen as a computer network: each comparator is a computer.\nDistributed algorithms in message-passing model\nThe algorithm designer only chooses the computer program. All computers run the same program. The system must work correctly regardless of the structure of the network.\nA commonly used model is a graph with one finite-state machine per node.\nIn the case of distributed algorithms, computational problems are typically related to graphs. Often the graph that describes the structure of the computer network is the problem instance. This is illustrated in the following example.[49]\n\nAn example\nConsider the computational problem of finding a coloring of a given graph G. Different fields might take the following approaches:\n\nCentralized algorithms[49]\nThe graph G is encoded as a string, and the string is given as input to a computer. The computer program finds a coloring of the graph, encodes the coloring as a string, and outputs the result.\nParallel algorithms\nAgain, the graph G is encoded as a string. However, multiple computers can access the same string in parallel. Each computer might focus on one part of the graph and produce a coloring for that part.\nThe main focus is on high-performance computation that exploits the processing power of multiple computers in parallel.\nDistributed algorithms\nThe graph G is the structure of the computer network. There is one computer for each node of G and one communication link for each edge of G. Initially, each computer only knows about its immediate neighbors in the graph G; the computers must exchange messages with each other to discover more about the structure of G. Each computer must produce its own color as output.\nThe main focus is on coordinating the operation of an arbitrary distributed system.[49]\nWhile the field of parallel algorithms has a different focus than the field of distributed algorithms, there is much interaction between the two fields. For example, the Cole–Vishkin algorithm for graph coloring[50] was originally presented as a parallel algorithm, but the same technique can also be used directly as a distributed algorithm.\n\nMoreover, a parallel algorithm can be implemented either in a parallel system (using shared memory) or in a distributed system (using message passing).[51] The traditional boundary between parallel and distributed algorithms (choose a suitable network vs. run in any given network) does not lie in the same place as the boundary between parallel and distributed systems (shared memory vs. message passing).\n\nComplexity measures\nIn parallel algorithms, yet another resource in addition to time and space is the number of computers. Indeed, often there is a trade-off between the running time and the number of computers: the problem can be solved faster if there are more computers running in parallel (see speedup). If a decision problem can be solved in polylogarithmic time by using a polynomial number of processors, then the problem is said to be in the class NC.[52] The class NC can be defined equally well by using the PRAM formalism or Boolean circuits—PRAM machines can simulate Boolean circuits efficiently and vice versa.[53]\n\nIn the analysis of distributed algorithms, more attention is usually paid on communication operations than computational steps. Perhaps the simplest model of distributed computing is a synchronous system where all nodes operate in a lockstep fashion. This model is commonly known as the LOCAL model. During each communication round, all nodes in parallel (1) receive the latest messages from their neighbours, (2) perform arbitrary local computation, and (3) send new messages to their neighbors. In such systems, a central complexity measure is the number of synchronous communication rounds required to complete the task.[54]\n\nThis complexity measure is closely related to the diameter of the network. Let D be the diameter of the network. On the one hand, any computable problem can be solved trivially in a synchronous distributed system in approximately 2D communication rounds: simply gather all information in one location (D rounds), solve the problem, and inform each node about the solution (D rounds).\n\nOn the other hand, if the running time of the algorithm is much smaller than D communication rounds, then the nodes in the network must produce their output without having the possibility to obtain information about distant parts of the network. In other words, the nodes must make globally consistent decisions based on information that is available in their local D-neighbourhood. Many distributed algorithms are known with the running time much smaller than D rounds, and understanding which problems can be solved by such algorithms is one of the central research questions of the field.[55] Typically an algorithm which solves a problem in polylogarithmic time in the network size is considered efficient in this model.\n\nAnother commonly used measure is the total number of bits transmitted in the network (cf. communication complexity).[56] The features of this concept are typically captured with the CONGEST(B) model, which is similarly defined as the LOCAL model, but where single messages can only contain B bits.\n\nOther problems\nTraditional computational problems take the perspective that the user asks a question, a computer (or a distributed system) processes the question, then produces an answer and stops. However, there are also problems where the system is required not to stop, including the dining philosophers problem and other similar mutual exclusion problems. In these problems, the distributed system is supposed to continuously coordinate the use of shared resources so that no conflicts or deadlocks occur.\n\nThere are also fundamental challenges that are unique to distributed computing, for example those related to fault-tolerance. Examples of related problems include consensus problems,[57] Byzantine fault tolerance,[58] and self-stabilisation.[59]\n\nMuch research is also focused on understanding the asynchronous nature of distributed systems:\n\nSynchronizers can be used to run synchronous algorithms in asynchronous systems.[60]\nLogical clocks provide a causal happened-before ordering of events.[61]\nClock synchronization algorithms provide globally consistent physical time stamps.[62]\nNote that in distributed systems, latency should be measured through \"99th percentile\" because \"median\" and \"average\" can be misleading.[63]\n\nElection\nCoordinator election (or leader election) is the process of designating a single process as the organizer of some task distributed among several computers (nodes). Before the task is begun, all network nodes are either unaware which node will serve as the \"coordinator\" (or leader) of the task, or unable to communicate with the current coordinator. After a coordinator election algorithm has been run, however, each node throughout the network recognizes a particular, unique node as the task coordinator.[64]\n\nThe network nodes communicate among themselves in order to decide which of them will get into the \"coordinator\" state. For that, they need some method in order to break the symmetry among them. For example, if each node has unique and comparable identities, then the nodes can compare their identities, and decide that the node with the highest identity is the coordinator.[64]\n\nThe definition of this problem is often attributed to LeLann, who formalized it as a method to create a new token in a token ring network in which the token has been lost.[65]\n\nCoordinator election algorithms are designed to be economical in terms of total bytes transmitted, and time. The algorithm suggested by Gallager, Humblet, and Spira[66] for general undirected graphs has had a strong impact on the design of distributed algorithms in general, and won the Dijkstra Prize for an influential paper in distributed computing.\n\nMany other algorithms were suggested for different kinds of network graphs, such as undirected rings, unidirectional rings, complete graphs, grids, directed Euler graphs, and others. A general method that decouples the issue of the graph family from the design of the coordinator election algorithm was suggested by Korach, Kutten, and Moran.[67]\n\nIn order to perform coordination, distributed systems employ the concept of coordinators. The coordinator election problem is to choose a process from among a group of processes on different processors in a distributed system to act as the central coordinator. Several central coordinator election algorithms exist.[68]\n\nProperties of distributed systems\nSo far the focus has been on designing a distributed system that solves a given problem. A complementary research problem is studying the properties of a given distributed system.[69][70]\n\nThe halting problem is an analogous example from the field of centralised computation: we are given a computer program and the task is to decide whether it halts or runs forever. The halting problem is undecidable in the general case, and naturally understanding the behaviour of a computer network is at least as hard as understanding the behaviour of one computer.[71]\n\nHowever, there are many interesting special cases that are decidable. In particular, it is possible to reason about the behaviour of a network of finite-state machines. One example is telling whether a given network of interacting (asynchronous and non-deterministic) finite-state machines can reach a deadlock. This problem is PSPACE-complete,[72] i.e., it is decidable, but not likely that there is an efficient (centralised, parallel or distributed) algorithm that solves the problem in the case of large networks.\n\nSee also\nActor model\nAppScale\nBOINC\nCode mobility\nDataflow programming\nDecentralized computing\nDistributed algorithm\nDistributed algorithmic mechanism design\nDistributed cache\nDistributed GIS\nDistributed networking\nDistributed operating system\nEventual consistency\nEdsger W. Dijkstra Prize in Distributed Computing\nFederation (information technology)\nFlat neighborhood network\nFog computing\nFolding@home\nGrid computing\nInferno\nInternet GIS\nJungle computing\nLayered queueing network\nLibrary Oriented Architecture (LOA)\nList of distributed computing conferences\nList of volunteer computing projects\nModel checking\nOpenHarmony\nHarmonyOS\nParallel distributed processing\nParallel programming model\nPlan 9 from Bell Labs\nShared nothing architecture\nWeb GIS\nNotes\n Tanenbaum, Andrew S.; Steen, Maarten van (2002). Distributed systems: principles and paradigms. Upper Saddle River, NJ: Pearson Prentice Hall. ISBN 0-13-088893-1. Archived from the original on 2020-08-12. Retrieved 2020-08-28.\n \"Distributed Programs\". Texts in Computer Science. London: Springer London. 2010. pp. 373–406. doi:10.1007/978-1-84882-745-5_11. ISBN 978-1-84882-744-8. ISSN 1868-0941. Systems consist of a number of physically distributed components that work independently using their private storage, but also communicate from time to time by explicit message passing. Such systems are called distributed systems.\n Dusseau & Dusseau 2016, p. 1–2.\n Ford, Neal (March 3, 2020). Fundamentals of Software Architecture: An Engineering Approach (1st ed.). O'Reilly Media. pp. 146–147. ISBN 978-1492043454.\n Monolith to Microservices Evolutionary Patterns to Transform Your Monolith. O'Reilly Media. ISBN 9781492047810.\n Building Serverless Applications on Knative. O'Reilly Media. ISBN 9781098142049.\n \"Distributed Programs\". Texts in Computer Science. London: Springer London. 2010. pp. 373–406. doi:10.1007/978-1-84882-745-5_11. ISBN 978-1-84882-744-8. ISSN 1868-0941. Distributed programs are abstract descriptions of distributed systems. A distributed program consists of a collection of processes that work concurrently and communicate by explicit message passing. Each process can access a set of variables which are disjoint from the variables that can be changed by any other process.\n Andrews (2000). Dolev (2000). Ghosh (2007), p. 10.\n Magnoni, L. (2015). \"Modern Messaging for Distributed Sytems (sic)\". Journal of Physics: Conference Series. 608 (1): 012038. doi:10.1088/1742-6596/608/1/012038. ISSN 1742-6596.\n Godfrey (2002).\n Andrews (2000), p. 291–292. Dolev (2000), p. 5.\n Lynch (1996), p. 1.\n Ghosh (2007), p. 10.\n Andrews (2000), pp. 8–9, 291. Dolev (2000), p. 5. Ghosh (2007), p. 3. Lynch (1996), p. xix, 1. Peleg (2000), p. xv.\n Andrews (2000), p. 291. Ghosh (2007), p. 3. Peleg (2000), p. 4.\n Ghosh (2007), p. 3–4. Peleg (2000), p. 1.\n Ghosh (2007), p. 4. Peleg (2000), p. 2.\n Ghosh (2007), p. 4, 8. Lynch (1996), p. 2–3. Peleg (2000), p. 4.\n Lynch (1996), p. 2. Peleg (2000), p. 1.\n Ghosh (2007), p. 7. Lynch (1996), p. xix, 2. Peleg (2000), p. 4.\n Fundamentals of Software Architecture: An Engineering Approach. O'Reilly Media. 2020. ISBN 978-1492043454.\n Ghosh (2007), p. 10. Keidar (2008).\n Lynch (1996), p. xix, 1–2. Peleg (2000), p. 1.\n Peleg (2000), p. 1.\n Papadimitriou (1994), Chapter 15. Keidar (2008).\n See references in Introduction.\n Bentaleb, A.; Yifan, L.; Xin, J.; et al. (2016). \"Parallel and Distributed Algorithms\" (PDF). National University of Singapore. Archived (PDF) from the original on 2017-03-26. Retrieved 20 July 2018.\n Andrews (2000), p. 348.\n Andrews (2000), p. 32.\n Peter (2004), The history of email Archived 2009-04-15 at the Wayback Machine.\n Banks, M. (2012). On the Way to the Web: The Secret History of the Internet and its Founders. Apress. pp. 44–5. ISBN 9781430250746. Archived from the original on 2023-01-20. Retrieved 2018-07-20.\n Tel, G. (2000). Introduction to Distributed Algorithms. Cambridge University Press. pp. 35–36. ISBN 9780521794831. Archived from the original on 2023-01-20. Retrieved 2018-07-20.\n Ohlídal, M.; Jaroš, J.; Schwarz, J.; et al. (2006). \"Evolutionary Design of OAB and AAB Communication Schedules for Interconnection Networks\". In Rothlauf, F.; Branke, J.; Cagnoni, S. (eds.). Applications of Evolutionary Computing. Springer Science & Business Media. pp. 267–78. ISBN 9783540332374.\n \"Real Time And Distributed Computing Systems\" (PDF). ISSN 2278-0661. Archived from the original (PDF) on 2017-01-10. Retrieved 2017-01-09. {{cite journal}}: Cite journal requires |journal= (help)\n Vigna P, Casey MJ. The Age of Cryptocurrency: How Bitcoin and the Blockchain Are Challenging the Global Economic Order St. Martin's Press January 27, 2015 ISBN 9781250065636\n Quang Hieu Vu; Mihai Lupu; Beng Chin Ooi (2010). Peer-to-peer computing : principles and applications. Heidelberg: Springer. p. 16. ISBN 9783642035135. OCLC 663093862.\n Lind P, Alm M (2006), \"A database-centric virtual chemistry system\", J Chem Inf Model, 46 (3): 1034–9, doi:10.1021/ci050360b, PMID 16711722.\n Chiu, G (1990). \"A model for optimal database allocation in distributed computing systems\". Proceedings. IEEE INFOCOM'90: Ninth Annual Joint Conference of the IEEE Computer and Communications Societies.\n Elmasri & Navathe (2000), Section 24.1.2.\n Andrews (2000), p. 10–11. Ghosh (2007), p. 4–6. Lynch (1996), p. xix, 1. Peleg (2000), p. xv. Elmasri & Navathe (2000), Section 24.\n Haussmann, J. (2019). \"Cost-efficient parallel processing of irregularly structured problems in cloud computing environments\". Journal of Cluster Computing. 22 (3): 887–909. doi:10.1007/s10586-018-2879-3. S2CID 54447518.\n Reactive Application Development. Manning. 2018. ISBN 9781638355816.\n Toomarian, N.B.; Barhen, J.; Gulati, S. (1992). \"Neural Networks for Real-Time Robotic Applications\". In Fijany, A.; Bejczy, A. (eds.). Parallel Computation Systems For Robotics: Algorithms And Architectures. World Scientific. p. 214. ISBN 9789814506175. Archived from the original on 2020-08-01. Retrieved 2018-07-20.\n Savage, J.E. (1998). Models of Computation: Exploring the Power of Computing. Addison Wesley. p. 209. ISBN 9780201895391.\n Cormen, Leiserson & Rivest (1990), Section 30.\n Herlihy & Shavit (2008), Chapters 2–6.\n Lynch (1996)\n Cormen, Leiserson & Rivest (1990), Sections 28 and 29.\n TULSIRAMJI GAIKWAD-PATIL College of Engineering & Technology, Nagpur Department of Information Technology Introduction to Distributed Systems[1]\n Cole & Vishkin (1986). Cormen, Leiserson & Rivest (1990), Section 30.5.\n Andrews (2000), p. ix.\n Arora & Barak (2009), Section 6.7. Papadimitriou (1994), Section 15.3.\n Papadimitriou (1994), Section 15.2.\n Lynch (1996), p. 17–23.\n Peleg (2000), Sections 2.3 and 7. Linial (1992). Naor & Stockmeyer (1995).\n Schneider, J.; Wattenhofer, R. (2011). \"Trading Bit, Message, and Time Complexity of Distributed Algorithms\". In Peleg, D. (ed.). Distributed Computing. Springer Science & Business Media. pp. 51–65. ISBN 9783642240997. Archived from the original on 2020-08-01. Retrieved 2018-07-20.\n Lynch (1996), Sections 5–7. Ghosh (2007), Chapter 13.\n Lynch (1996), p. 99–102. Ghosh (2007), p. 192–193.\n Dolev (2000). Ghosh (2007), Chapter 17.\n Lynch (1996), Section 16. Peleg (2000), Section 6.\n Lynch (1996), Section 18. Ghosh (2007), Sections 6.2–6.3.\n Ghosh (2007), Section 6.4.\n Foundations of Data Intensive Applications Large Scale Data Analytics Under the Hood. 2021. ISBN 9781119713012.\n Haloi, S. (2015). Apache ZooKeeper Essentials. Packt Publishing Ltd. pp. 100–101. ISBN 9781784398323. Archived from the original on 2023-01-20. Retrieved 2018-07-20.\n LeLann, G. (1977). \"Distributed systems - toward a formal approach\". Information Processing. 77: 155·160 – via Elsevier.\n R. G. Gallager, P. A. Humblet, and P. M. Spira (January 1983). \"A Distributed Algorithm for Minimum-Weight Spanning Trees\" (PDF). ACM Transactions on Programming Languages and Systems. 5 (1): 66–77. doi:10.1145/357195.357200. S2CID 2758285. Archived (PDF) from the original on 2017-09-26.\n Korach, Ephraim; Kutten, Shay; Moran, Shlomo (1990). \"A Modular Technique for the Design of Efficient Distributed Leader Finding Algorithms\" (PDF). ACM Transactions on Programming Languages and Systems. 12 (1): 84–101. CiteSeerX 10.1.1.139.7342. doi:10.1145/77606.77610. S2CID 9175968. Archived (PDF) from the original on 2007-04-18.\n Hamilton, Howard. \"Distributed Algorithms\". Archived from the original on 2012-11-24. Retrieved 2013-03-03.\n \"Major unsolved problems in distributed systems?\". cstheory.stackexchange.com. Archived from the original on 20 January 2023. Retrieved 16 March 2018.\n \"How big data and distributed systems solve traditional scalability problems\". theserverside.com. Archived from the original on 17 March 2018. Retrieved 16 March 2018.\n Svozil, K. (2011). \"Indeterminism and Randomness Through Physics\". In Hector, Z. (ed.). Randomness Through Computation: Some Answers, More Questions. World Scientific. pp. 112–3. ISBN 9789814462631. Archived from the original on 2020-08-01. Retrieved 2018-07-20.\n Papadimitriou (1994), Section 19.3.\nReferences\nBooks\nAndrews, Gregory R. (2000), Foundations of Multithreaded, Parallel, and Distributed Programming, Addison–Wesley, ISBN 978-0-201-35752-3.\nArora, Sanjeev; Barak, Boaz (2009), Computational Complexity – A Modern Approach, Cambridge, ISBN 978-0-521-42426-4.\nCormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L. (1990), Introduction to Algorithms (1st ed.), MIT Press, ISBN 978-0-262-03141-7.\nDolev, Shlomi (2000), Self-Stabilization, MIT Press, ISBN 978-0-262-04178-2.\nElmasri, Ramez; Navathe, Shamkant B. (2000), Fundamentals of Database Systems (3rd ed.), Addison–Wesley, ISBN 978-0-201-54263-9.\nGhosh, Sukumar (2007), Distributed Systems – An Algorithmic Approach, Chapman & Hall/CRC, ISBN 978-1-58488-564-1.\nLynch, Nancy A. (1996), Distributed Algorithms, Morgan Kaufmann, ISBN 978-1-55860-348-6.\nHerlihy, Maurice P.; Shavit, Nir N. (2008), The Art of Multiprocessor Programming, Morgan Kaufmann, ISBN 978-0-12-370591-4.\nPapadimitriou, Christos H. (1994), Computational Complexity, Addison–Wesley, ISBN 978-0-201-53082-7.\nPeleg, David (2000), Distributed Computing: A Locality-Sensitive Approach, SIAM, ISBN 978-0-89871-464-7, archived from the original on 2009-08-06, retrieved 2009-07-16.\nArticles\nCole, Richard; Vishkin, Uzi (1986), \"Deterministic coin tossing with applications to optimal parallel list ranking\", Information and Control, 70 (1): 32–53, doi:10.1016/S0019-9958(86)80023-7.\nKeidar, Idit (2008), \"Distributed computing column 32 – The year in review\", ACM SIGACT News, 39 (4): 53–54, CiteSeerX 10.1.1.116.1285, doi:10.1145/1466390.1466402, S2CID 7607391, archived from the original on 2014-01-16, retrieved 2009-08-20.\nLinial, Nathan (1992), \"Locality in distributed graph algorithms\", SIAM Journal on Computing, 21 (1): 193–201, CiteSeerX 10.1.1.471.6378, doi:10.1137/0221015.\nNaor, Moni; Stockmeyer, Larry (1995), \"What can be computed locally?\" (PDF), SIAM Journal on Computing, 24 (6): 1259–1277, CiteSeerX 10.1.1.29.669, doi:10.1137/S0097539793254571, archived (PDF) from the original on 2013-01-08.\nWeb sites\nGodfrey, Bill (2002). \"A primer on distributed computing\". Archived from the original on 2021-05-13. Retrieved 2021-05-13.\nPeter, Ian (2004). \"Ian Peter's History of the Internet\". Archived from the original on 2010-01-20. Retrieved 2009-08-04.\nFurther reading\nBooks\nAttiya, Hagit and Jennifer Welch (2004), Distributed Computing: Fundamentals, Simulations, and Advanced Topics, Wiley-Interscience ISBN 0-471-45324-2.\nChristian Cachin; Rachid Guerraoui; Luís Rodrigues (2011), Introduction to Reliable and Secure Distributed Programming (2. ed.), Springer, Bibcode:2011itra.book.....C, ISBN 978-3-642-15259-7\nCoulouris, George; et al. (2011), Distributed Systems: Concepts and Design (5th Edition), Addison-Wesley ISBN 0-132-14301-1.\nFaber, Jim (1998), Java Distributed Computing, O'Reilly, archived from the original on 2010-08-24, retrieved 2010-09-29: Java Distributed Computing by Jim Faber, 1998 Archived 2010-08-24 at the Wayback Machine\nGarg, Vijay K. (2002), Elements of Distributed Computing, Wiley-IEEE Press ISBN 0-471-03600-5.\nTel, Gerard (1994), Introduction to Distributed Algorithms, Cambridge University Press\nChandy, Mani; et al. (1988), Parallel Program Design, Addison-Wesley ISBN 0201058669\nDusseau, Remzi H.; Dusseau, Andrea (2016). Operating Systems: Three Easy Pieces, Chapter 48 Distributed Systems (PDF). Archived from the original (PDF) on 31 August 2021. Retrieved 8 October 2021.\nArticles\nKeidar, Idit; Rajsbaum, Sergio, eds. (2000–2009), \"Distributed computing column\", ACM SIGACT News, archived from the original on 2014-01-16, retrieved 2009-08-16.\nBirrell, A. D.; Levin, R.; Schroeder, M. D.; Needham, R. M. (April 1982). \"Grapevine: An exercise in distributed computing\" (PDF). Communications of the ACM. 25 (4): 260–274. doi:10.1145/358468.358487. S2CID 16066616. Archived (PDF) from the original on 2016-07-30.\nConference Papers\nRodriguez, Carlos; Villagra, Marcos; Baran, Benjamin (2007). \"Asynchronous team algorithms for Boolean Satisfiability\". 2007 2nd Bio-Inspired Models of Network, Information and Computing Systems. pp. 66–69. doi:10.1109/BIMNICS.2007.4610083. S2CID 15185219.\nExternal links\n\nWikiquote has quotations related to Distributed computing.\n Media related to Distributed computing at Wikimedia Commons\nvte\nParallel computing\nAuthority control databases Edit this at Wikidata\nCategory: Distributed computing\nThis page was last edited on 4 October 2024, at 08:15 (UTC).\nText is available under the Creative Commons Attribution-ShareAlike 4.0 License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\nPrivacy policyAbout WikipediaDisclaimersContact WikipediaCode of ConductDevelopersStatisticsCookie statementMobile view\nWikimedia FoundationPowered by MediaWiki\n\n\nMain menu\n\nWikipediaThe Free Encyclopedia\nSearch Wikipedia\nSearch\nDonate\nCreate account\nLog in\n\nPersonal tools\nContents hide\n(Top)\nIntroduction\nPatterns\nParallel and distributed computing\nHistory\nArchitectures\nApplications\nExamples\nReactive distributed systems\nTheoretical foundations\nToggle Theoretical foundations subsection\nModels\nAn example\nComplexity measures\nOther problems\nElection\nProperties of distributed systems\nSee also\nNotes\nReferences\nFurther reading\nExternal links\nDistributed computing\n\nArticle\nTalk\nRead\nEdit\nView history\n\nTools\nAppearance hide\nText\n\nSmall\n\nStandard\n\nLarge\nWidth\n\nStandard\n\nWide\nColor (beta)\n\nAutomatic\n\nLight\n\nDark\nFrom Wikipedia, the free encyclopedia\nNot to be confused with Decentralized computing.\nDistributed computing is a field of computer science that studies distributed systems, defined as computer systems whose inter-communicating components are located on different networked computers.[1][2]\n\nThe components of a distributed system communicate and coordinate their actions by passing messages to one another in order to achieve a common goal. Three significant challenges of distributed systems are: maintaining concurrency of components, overcoming the lack of a global clock, and managing the independent failure of components.[1] When a component of one system fails, the entire system does not fail.[3] Examples of distributed systems vary from SOA-based systems to microservices to massively multiplayer online games to peer-to-peer applications. Distributed systems cost significantly more than monolithic architectures, primarily due to increased needs for additional hardware, servers, gateways, firewalls, new subnets, proxies, and so on.[4] Also, distributed systems are prone to fallacies of distributed computing. On the other hand, a well designed distributed system is more scalable, more durable, more changeable and more fine-tuned than a monolithic application deployed on a single machine.[5] According to Marc Brooker: \"a system is scalable in the range where marginal cost of additional workload is nearly constant.\" Serverless technologies fit this definition but you need to consider total cost of ownership not just the infra cost. [6]\n\nA computer program that runs within a distributed system is called a distributed program,[7] and distributed programming is the process of writing such programs.[8] There are many different types of implementations for the message passing mechanism, including pure HTTP, RPC-like connectors and message queues.[9]\n\nDistributed computing also refers to the use of distributed systems to solve computational problems. In distributed computing, a problem is divided into many tasks, each of which is solved by one or more computers,[10] which communicate with each other via message passing.[11]\n\nIntroduction\nThe word distributed in terms such as \"distributed system\", \"distributed programming\", and \"distributed algorithm\" originally referred to computer networks where individual computers were physically distributed within some geographical area.[12] The terms are nowadays used in a much wider sense, even referring to autonomous processes that run on the same physical computer and interact with each other by message passing.[11]\n\nWhile there is no single definition of a distributed system,[13] the following defining properties are commonly used as:\n\nThere are several autonomous computational entities (computers or nodes), each of which has its own local memory.[14]\nThe entities communicate with each other by message passing.[15]\nA distributed system may have a common goal, such as solving a large computational problem;[16] the user then perceives the collection of autonomous processors as a unit. Alternatively, each computer may have its own user with individual needs, and the purpose of the distributed system is to coordinate the use of shared resources or provide communication services to the users.[17]\n\nOther typical properties of distributed systems include the following:\n\nThe system has to tolerate failures in individual computers.[18]\nThe structure of the system (network topology, network latency, number of computers) is not known in advance, the system may consist of different kinds of computers and network links, and the system may change during the execution of a distributed program.[19]\nEach computer has only a limited, incomplete view of the system. Each computer may know only one part of the input.[20]\nPatterns\nHere are common architectural patterns used for distributed computing:[21]\n\nSaga interaction pattern\nMicroservices\nEvent driven architecture\nParallel and distributed computing\n\n(a), (b): a distributed system.\n(c): a parallel system.\nDistributed systems are groups of networked computers which share a common goal for their work. The terms \"concurrent computing\", \"parallel computing\", and \"distributed computing\" have much overlap, and no clear distinction exists between them.[22] The same system may be characterized both as \"parallel\" and \"distributed\"; the processors in a typical distributed system run concurrently in parallel.[23] Parallel computing may be seen as a particularly tightly coupled form of distributed computing,[24] and distributed computing may be seen as a loosely coupled form of parallel computing.[13] Nevertheless, it is possible to roughly classify concurrent systems as \"parallel\" or \"distributed\" using the following criteria:\n\nIn parallel computing, all processors may have access to a shared memory to exchange information between processors.[25]\nIn distributed computing, each processor has its own private memory (distributed memory). Information is exchanged by passing messages between the processors.[26]\nThe figure on the right illustrates the difference between distributed and parallel systems. Figure (a) is a schematic view of a typical distributed system; the system is represented as a network topology in which each node is a computer and each line connecting the nodes is a communication link. Figure (b) shows the same distributed system in more detail: each computer has its own local memory, and information can be exchanged only by passing messages from one node to another by using the available communication links. Figure (c) shows a parallel system in which each processor has a direct access to a shared memory.\n\nThe situation is further complicated by the traditional uses of the terms parallel and distributed algorithm that do not quite match the above definitions of parallel and distributed systems (see below for more detailed discussion). Nevertheless, as a rule of thumb, high-performance parallel computation in a shared-memory multiprocessor uses parallel algorithms while the coordination of a large-scale distributed system uses distributed algorithms.[27]\n\nHistory\nThe use of concurrent processes which communicate through message-passing has its roots in operating system architectures studied in the 1960s.[28] The first widespread distributed systems were local-area networks such as Ethernet, which was invented in the 1970s.[29]\n\nARPANET, one of the predecessors of the Internet, was introduced in the late 1960s, and ARPANET e-mail was invented in the early 1970s. E-mail became the most successful application of ARPANET,[30] and it is probably the earliest example of a large-scale distributed application. In addition to ARPANET (and its successor, the global Internet), other early worldwide computer networks included Usenet and FidoNet from the 1980s, both of which were used to support distributed discussion systems.[31]\n\nThe study of distributed computing became its own branch of computer science in the late 1970s and early 1980s. The first conference in the field, Symposium on Principles of Distributed Computing (PODC), dates back to 1982, and its counterpart International Symposium on Distributed Computing (DISC) was first held in Ottawa in 1985 as the International Workshop on Distributed Algorithms on Graphs.[32]\n\nArchitectures\nVarious hardware and software architectures are used for distributed computing. At a lower level, it is necessary to interconnect multiple CPUs with some sort of network, regardless of whether that network is printed onto a circuit board or made up of loosely coupled devices and cables. At a higher level, it is necessary to interconnect processes running on those CPUs with some sort of communication system.[33]\n\nWhether these CPUs share resources or not determines a first distinction between three types of architecture:\n\nShared memory\nShared disk\nShared nothing.\nDistributed programming typically falls into one of several basic architectures: client–server, three-tier, n-tier, or peer-to-peer; or categories: loose coupling, or tight coupling.[34]\n\nClient–server: architectures where smart clients contact the server for data then format and display it to the users. Input at the client is committed back to the server when it represents a permanent change.\nThree-tier: architectures that move the client intelligence to a middle tier so that stateless clients can be used. This simplifies application deployment. Most web applications are three-tier.\nn-tier: architectures that refer typically to web applications which further forward their requests to other enterprise services. This type of application is the one most responsible for the success of application servers.\nPeer-to-peer: architectures where there are no special machines that provide a service or manage the network resources.[35]: 227  Instead all responsibilities are uniformly divided among all machines, known as peers. Peers can serve both as clients and as servers.[36] Examples of this architecture include BitTorrent and the bitcoin network.\nAnother basic aspect of distributed computing architecture is the method of communicating and coordinating work among concurrent processes. Through various message passing protocols, processes may communicate directly with one another, typically in a main/sub relationship. Alternatively, a \"database-centric\" architecture can enable distributed computing to be done without any form of direct inter-process communication, by utilizing a shared database.[37] Database-centric architecture in particular provides relational processing analytics in a schematic architecture allowing for live environment relay. This enables distributed computing functions both within and beyond the parameters of a networked database.[38]\n\nApplications\nReasons for using distributed systems and distributed computing may include:\n\nThe very nature of an application may require the use of a communication network that connects several computers: for example, data produced in one physical location and required in another location.\nThere are many cases in which the use of a single computer would be possible in principle, but the use of a distributed system is beneficial for practical reasons. For example:\nIt can allow for much larger storage and memory, faster compute, and higher bandwidth than a single machine.\nIt can provide more reliability than a non-distributed system, as there is no single point of failure. Moreover, a distributed system may be easier to expand and manage than a monolithic uniprocessor system.[39]\nIt may be more cost-efficient to obtain the desired level of performance by using a cluster of several low-end computers, in comparison with a single high-end computer.\nExamples\nExamples of distributed systems and applications of distributed computing include the following:[40]\n\ntelecommunications networks:\ntelephone networks and cellular networks,\ncomputer networks such as the Internet,\nwireless sensor networks,\nrouting algorithms;\nnetwork applications:\nWorld Wide Web and peer-to-peer networks,\nmassively multiplayer online games and virtual reality communities,\ndistributed databases and distributed database management systems,\nnetwork file systems,\ndistributed cache such as burst buffers,\ndistributed information processing systems such as banking systems and airline reservation systems;\nreal-time process control:\naircraft control systems,\nindustrial control systems;\nparallel computation:\nscientific computing, including cluster computing, grid computing, cloud computing,[41] and various volunteer computing projects,\ndistributed rendering in computer graphics.\npeer-to-peer\nReactive distributed systems\nAccording to Reactive Manifesto, reactive distributed systems are responsive, resilient, elastic and message-driven. Subsequently, Reactive systems are more flexible, loosely-coupled and scalable. To make your systems reactive, you are advised to implement Reactive Principles. Reactive Principles are a set of principles and patterns which help to make your cloud native application as well as edge native applications more reactive. [42]\n\nTheoretical foundations\nMain article: Distributed algorithm\nModels\nMany tasks that we would like to automate by using a computer are of question–answer type: we would like to ask a question and the computer should produce an answer. In theoretical computer science, such tasks are called computational problems. Formally, a computational problem consists of instances together with a solution for each instance. Instances are questions that we can ask, and solutions are desired answers to these questions.\n\nTheoretical computer science seeks to understand which computational problems can be solved by using a computer (computability theory) and how efficiently (computational complexity theory). Traditionally, it is said that a problem can be solved by using a computer if we can design an algorithm that produces a correct solution for any given instance. Such an algorithm can be implemented as a computer program that runs on a general-purpose computer: the program reads a problem instance from input, performs some computation, and produces the solution as output. Formalisms such as random-access machines or universal Turing machines can be used as abstract models of a sequential general-purpose computer executing such an algorithm.[43][44]\n\nThe field of concurrent and distributed computing studies similar questions in the case of either multiple computers, or a computer that executes a network of interacting processes: which computational problems can be solved in such a network and how efficiently? However, it is not at all obvious what is meant by \"solving a problem\" in the case of a concurrent or distributed system: for example, what is the task of the algorithm designer, and what is the concurrent or distributed equivalent of a sequential general-purpose computer?[citation needed]\n\nThe discussion below focuses on the case of multiple computers, although many of the issues are the same for concurrent processes running on a single computer.\n\nThree viewpoints are commonly used:\n\nParallel algorithms in shared-memory model\nAll processors have access to a shared memory. The algorithm designer chooses the program executed by each processor.\nOne theoretical model is the parallel random-access machines (PRAM) that are used.[45] However, the classical PRAM model assumes synchronous access to the shared memory.\nShared-memory programs can be extended to distributed systems if the underlying operating system encapsulates the communication between nodes and virtually unifies the memory across all individual systems.\nA model that is closer to the behavior of real-world multiprocessor machines and takes into account the use of machine instructions, such as Compare-and-swap (CAS), is that of asynchronous shared memory. There is a wide body of work on this model, a summary of which can be found in the literature.[46][47]\nParallel algorithms in message-passing model\nThe algorithm designer chooses the structure of the network, as well as the program executed by each computer.\nModels such as Boolean circuits and sorting networks are used.[48] A Boolean circuit can be seen as a computer network: each gate is a computer that runs an extremely simple computer program. Similarly, a sorting network can be seen as a computer network: each comparator is a computer.\nDistributed algorithms in message-passing model\nThe algorithm designer only chooses the computer program. All computers run the same program. The system must work correctly regardless of the structure of the network.\nA commonly used model is a graph with one finite-state machine per node.\nIn the case of distributed algorithms, computational problems are typically related to graphs. Often the graph that describes the structure of the computer network is the problem instance. This is illustrated in the following example.[49]\n\nAn example\nConsider the computational problem of finding a coloring of a given graph G. Different fields might take the following approaches:\n\nCentralized algorithms[49]\nThe graph G is encoded as a string, and the string is given as input to a computer. The computer program finds a coloring of the graph, encodes the coloring as a string, and outputs the result.\nParallel algorithms\nAgain, the graph G is encoded as a string. However, multiple computers can access the same string in parallel. Each computer might focus on one part of the graph and produce a coloring for that part.\nThe main focus is on high-performance computation that exploits the processing power of multiple computers in parallel.\nDistributed algorithms\nThe graph G is the structure of the computer network. There is one computer for each node of G and one communication link for each edge of G. Initially, each computer only knows about its immediate neighbors in the graph G; the computers must exchange messages with each other to discover more about the structure of G. Each computer must produce its own color as output.\nThe main focus is on coordinating the operation of an arbitrary distributed system.[49]\nWhile the field of parallel algorithms has a different focus than the field of distributed algorithms, there is much interaction between the two fields. For example, the Cole–Vishkin algorithm for graph coloring[50] was originally presented as a parallel algorithm, but the same technique can also be used directly as a distributed algorithm.\n\nMoreover, a parallel algorithm can be implemented either in a parallel system (using shared memory) or in a distributed system (using message passing).[51] The traditional boundary between parallel and distributed algorithms (choose a suitable network vs. run in any given network) does not lie in the same place as the boundary between parallel and distributed systems (shared memory vs. message passing).\n\nComplexity measures\nIn parallel algorithms, yet another resource in addition to time and space is the number of computers. Indeed, often there is a trade-off between the running time and the number of computers: the problem can be solved faster if there are more computers running in parallel (see speedup). If a decision problem can be solved in polylogarithmic time by using a polynomial number of processors, then the problem is said to be in the class NC.[52] The class NC can be defined equally well by using the PRAM formalism or Boolean circuits—PRAM machines can simulate Boolean circuits efficiently and vice versa.[53]\n\nIn the analysis of distributed algorithms, more attention is usually paid on communication operations than computational steps. Perhaps the simplest model of distributed computing is a synchronous system where all nodes operate in a lockstep fashion. This model is commonly known as the LOCAL model. During each communication round, all nodes in parallel (1) receive the latest messages from their neighbours, (2) perform arbitrary local computation, and (3) send new messages to their neighbors. In such systems, a central complexity measure is the number of synchronous communication rounds required to complete the task.[54]\n\nThis complexity measure is closely related to the diameter of the network. Let D be the diameter of the network. On the one hand, any computable problem can be solved trivially in a synchronous distributed system in approximately 2D communication rounds: simply gather all information in one location (D rounds), solve the problem, and inform each node about the solution (D rounds).\n\nOn the other hand, if the running time of the algorithm is much smaller than D communication rounds, then the nodes in the network must produce their output without having the possibility to obtain information about distant parts of the network. In other words, the nodes must make globally consistent decisions based on information that is available in their local D-neighbourhood. Many distributed algorithms are known with the running time much smaller than D rounds, and understanding which problems can be solved by such algorithms is one of the central research questions of the field.[55] Typically an algorithm which solves a problem in polylogarithmic time in the network size is considered efficient in this model.\n\nAnother commonly used measure is the total number of bits transmitted in the network (cf. communication complexity).[56] The features of this concept are typically captured with the CONGEST(B) model, which is similarly defined as the LOCAL model, but where single messages can only contain B bits.\n\nOther problems\nTraditional computational problems take the perspective that the user asks a question, a computer (or a distributed system) processes the question, then produces an answer and stops. However, there are also problems where the system is required not to stop, including the dining philosophers problem and other similar mutual exclusion problems. In these problems, the distributed system is supposed to continuously coordinate the use of shared resources so that no conflicts or deadlocks occur.\n\nThere are also fundamental challenges that are unique to distributed computing, for example those related to fault-tolerance. Examples of related problems include consensus problems,[57] Byzantine fault tolerance,[58] and self-stabilisation.[59]\n\nMuch research is also focused on understanding the asynchronous nature of distributed systems:\n\nSynchronizers can be used to run synchronous algorithms in asynchronous systems.[60]\nLogical clocks provide a causal happened-before ordering of events.[61]\nClock synchronization algorithms provide globally consistent physical time stamps.[62]\nNote that in distributed systems, latency should be measured through \"99th percentile\" because \"median\" and \"average\" can be misleading.[63]\n\nElection\nCoordinator election (or leader election) is the process of designating a single process as the organizer of some task distributed among several computers (nodes). Before the task is begun, all network nodes are either unaware which node will serve as the \"coordinator\" (or leader) of the task, or unable to communicate with the current coordinator. After a coordinator election algorithm has been run, however, each node throughout the network recognizes a particular, unique node as the task coordinator.[64]\n\nThe network nodes communicate among themselves in order to decide which of them will get into the \"coordinator\" state. For that, they need some method in order to break the symmetry among them. For example, if each node has unique and comparable identities, then the nodes can compare their identities, and decide that the node with the highest identity is the coordinator.[64]\n\nThe definition of this problem is often attributed to LeLann, who formalized it as a method to create a new token in a token ring network in which the token has been lost.[65]\n\nCoordinator election algorithms are designed to be economical in terms of total bytes transmitted, and time. The algorithm suggested by Gallager, Humblet, and Spira[66] for general undirected graphs has had a strong impact on the design of distributed algorithms in general, and won the Dijkstra Prize for an influential paper in distributed computing.\n\nMany other algorithms were suggested for different kinds of network graphs, such as undirected rings, unidirectional rings, complete graphs, grids, directed Euler graphs, and others. A general method that decouples the issue of the graph family from the design of the coordinator election algorithm was suggested by Korach, Kutten, and Moran.[67]\n\nIn order to perform coordination, distributed systems employ the concept of coordinators. The coordinator election problem is to choose a process from among a group of processes on different processors in a distributed system to act as the central coordinator. Several central coordinator election algorithms exist.[68]\n\nProperties of distributed systems\nSo far the focus has been on designing a distributed system that solves a given problem. A complementary research problem is studying the properties of a given distributed system.[69][70]\n\nThe halting problem is an analogous example from the field of centralised computation: we are given a computer program and the task is to decide whether it halts or runs forever. The halting problem is undecidable in the general case, and naturally understanding the behaviour of a computer network is at least as hard as understanding the behaviour of one computer.[71]\n\nHowever, there are many interesting special cases that are decidable. In particular, it is possible to reason about the behaviour of a network of finite-state machines. One example is telling whether a given network of interacting (asynchronous and non-deterministic) finite-state machines can reach a deadlock. This problem is PSPACE-complete,[72] i.e., it is decidable, but not likely that there is an efficient (centralised, parallel or distributed) algorithm that solves the problem in the case of large networks.\n\nSee also\nActor model\nAppScale\nBOINC\nCode mobility\nDataflow programming\nDecentralized computing\nDistributed algorithm\nDistributed algorithmic mechanism design\nDistributed cache\nDistributed GIS\nDistributed networking\nDistributed operating system\nEventual consistency\nEdsger W. Dijkstra Prize in Distributed Computing\nFederation (information technology)\nFlat neighborhood network\nFog computing\nFolding@home\nGrid computing\nInferno\nInternet GIS\nJungle computing\nLayered queueing network\nLibrary Oriented Architecture (LOA)\nList of distributed computing conferences\nList of volunteer computing projects\nModel checking\nOpenHarmony\nHarmonyOS\nParallel distributed processing\nParallel programming model\nPlan 9 from Bell Labs\nShared nothing architecture\nWeb GIS\nNotes\n Tanenbaum, Andrew S.; Steen, Maarten van (2002). Distributed systems: principles and paradigms. Upper Saddle River, NJ: Pearson Prentice Hall. ISBN 0-13-088893-1. Archived from the original on 2020-08-12. Retrieved 2020-08-28.\n \"Distributed Programs\". Texts in Computer Science. London: Springer London. 2010. pp. 373–406. doi:10.1007/978-1-84882-745-5_11. ISBN 978-1-84882-744-8. ISSN 1868-0941. Systems consist of a number of physically distributed components that work independently using their private storage, but also communicate from time to time by explicit message passing. Such systems are called distributed systems.\n Dusseau & Dusseau 2016, p. 1–2.\n Ford, Neal (March 3, 2020). Fundamentals of Software Architecture: An Engineering Approach (1st ed.). O'Reilly Media. pp. 146–147. ISBN 978-1492043454.\n Monolith to Microservices Evolutionary Patterns to Transform Your Monolith. O'Reilly Media. ISBN 9781492047810.\n Building Serverless Applications on Knative. O'Reilly Media. ISBN 9781098142049.\n \"Distributed Programs\". Texts in Computer Science. London: Springer London. 2010. pp. 373–406. doi:10.1007/978-1-84882-745-5_11. ISBN 978-1-84882-744-8. ISSN 1868-0941. Distributed programs are abstract descriptions of distributed systems. A distributed program consists of a collection of processes that work concurrently and communicate by explicit message passing. Each process can access a set of variables which are disjoint from the variables that can be changed by any other process.\n Andrews (2000). Dolev (2000). Ghosh (2007), p. 10.\n Magnoni, L. (2015). \"Modern Messaging for Distributed Sytems (sic)\". Journal of Physics: Conference Series. 608 (1): 012038. doi:10.1088/1742-6596/608/1/012038. ISSN 1742-6596.\n Godfrey (2002).\n Andrews (2000), p. 291–292. Dolev (2000), p. 5.\n Lynch (1996), p. 1.\n Ghosh (2007), p. 10.\n Andrews (2000), pp. 8–9, 291. Dolev (2000), p. 5. Ghosh (2007), p. 3. Lynch (1996), p. xix, 1. Peleg (2000), p. xv.\n Andrews (2000), p. 291. Ghosh (2007), p. 3. Peleg (2000), p. 4.\n Ghosh (2007), p. 3–4. Peleg (2000), p. 1.\n Ghosh (2007), p. 4. Peleg (2000), p. 2.\n Ghosh (2007), p. 4, 8. Lynch (1996), p. 2–3. Peleg (2000), p. 4.\n Lynch (1996), p. 2. Peleg (2000), p. 1.\n Ghosh (2007), p. 7. Lynch (1996), p. xix, 2. Peleg (2000), p. 4.\n Fundamentals of Software Architecture: An Engineering Approach. O'Reilly Media. 2020. ISBN 978-1492043454.\n Ghosh (2007), p. 10. Keidar (2008).\n Lynch (1996), p. xix, 1–2. Peleg (2000), p. 1.\n Peleg (2000), p. 1.\n Papadimitriou (1994), Chapter 15. Keidar (2008).\n See references in Introduction.\n Bentaleb, A.; Yifan, L.; Xin, J.; et al. (2016). \"Parallel and Distributed Algorithms\" (PDF). National University of Singapore. Archived (PDF) from the original on 2017-03-26. Retrieved 20 July 2018.\n Andrews (2000), p. 348.\n Andrews (2000), p. 32.\n Peter (2004), The history of email Archived 2009-04-15 at the Wayback Machine.\n Banks, M. (2012). On the Way to the Web: The Secret History of the Internet and its Founders. Apress. pp. 44–5. ISBN 9781430250746. Archived from the original on 2023-01-20. Retrieved 2018-07-20.\n Tel, G. (2000). Introduction to Distributed Algorithms. Cambridge University Press. pp. 35–36. ISBN 9780521794831. Archived from the original on 2023-01-20. Retrieved 2018-07-20.\n Ohlídal, M.; Jaroš, J.; Schwarz, J.; et al. (2006). \"Evolutionary Design of OAB and AAB Communication Schedules for Interconnection Networks\". In Rothlauf, F.; Branke, J.; Cagnoni, S. (eds.). Applications of Evolutionary Computing. Springer Science & Business Media. pp. 267–78. ISBN 9783540332374.\n \"Real Time And Distributed Computing Systems\" (PDF). ISSN 2278-0661. Archived from the original (PDF) on 2017-01-10. Retrieved 2017-01-09. {{cite journal}}: Cite journal requires |journal= (help)\n Vigna P, Casey MJ. The Age of Cryptocurrency: How Bitcoin and the Blockchain Are Challenging the Global Economic Order St. Martin's Press January 27, 2015 ISBN 9781250065636\n Quang Hieu Vu; Mihai Lupu; Beng Chin Ooi (2010). Peer-to-peer computing : principles and applications. Heidelberg: Springer. p. 16. ISBN 9783642035135. OCLC 663093862.\n Lind P, Alm M (2006), \"A database-centric virtual chemistry system\", J Chem Inf Model, 46 (3): 1034–9, doi:10.1021/ci050360b, PMID 16711722.\n Chiu, G (1990). \"A model for optimal database allocation in distributed computing systems\". Proceedings. IEEE INFOCOM'90: Ninth Annual Joint Conference of the IEEE Computer and Communications Societies.\n Elmasri & Navathe (2000), Section 24.1.2.\n Andrews (2000), p. 10–11. Ghosh (2007), p. 4–6. Lynch (1996), p. xix, 1. Peleg (2000), p. xv. Elmasri & Navathe (2000), Section 24.\n Haussmann, J. (2019). \"Cost-efficient parallel processing of irregularly structured problems in cloud computing environments\". Journal of Cluster Computing. 22 (3): 887–909. doi:10.1007/s10586-018-2879-3. S2CID 54447518.\n Reactive Application Development. Manning. 2018. ISBN 9781638355816.\n Toomarian, N.B.; Barhen, J.; Gulati, S. (1992). \"Neural Networks for Real-Time Robotic Applications\". In Fijany, A.; Bejczy, A. (eds.). Parallel Computation Systems For Robotics: Algorithms And Architectures. World Scientific. p. 214. ISBN 9789814506175. Archived from the original on 2020-08-01. Retrieved 2018-07-20.\n Savage, J.E. (1998). Models of Computation: Exploring the Power of Computing. Addison Wesley. p. 209. ISBN 9780201895391.\n Cormen, Leiserson & Rivest (1990), Section 30.\n Herlihy & Shavit (2008), Chapters 2–6.\n Lynch (1996)\n Cormen, Leiserson & Rivest (1990), Sections 28 and 29.\n TULSIRAMJI GAIKWAD-PATIL College of Engineering & Technology, Nagpur Department of Information Technology Introduction to Distributed Systems[1]\n Cole & Vishkin (1986). Cormen, Leiserson & Rivest (1990), Section 30.5.\n Andrews (2000), p. ix.\n Arora & Barak (2009), Section 6.7. Papadimitriou (1994), Section 15.3.\n Papadimitriou (1994), Section 15.2.\n Lynch (1996), p. 17–23.\n Peleg (2000), Sections 2.3 and 7. Linial (1992). Naor & Stockmeyer (1995).\n Schneider, J.; Wattenhofer, R. (2011). \"Trading Bit, Message, and Time Complexity of Distributed Algorithms\". In Peleg, D. (ed.). Distributed Computing. Springer Science & Business Media. pp. 51–65. ISBN 9783642240997. Archived from the original on 2020-08-01. Retrieved 2018-07-20.\n Lynch (1996), Sections 5–7. Ghosh (2007), Chapter 13.\n Lynch (1996), p. 99–102. Ghosh (2007), p. 192–193.\n Dolev (2000). Ghosh (2007), Chapter 17.\n Lynch (1996), Section 16. Peleg (2000), Section 6.\n Lynch (1996), Section 18. Ghosh (2007), Sections 6.2–6.3.\n Ghosh (2007), Section 6.4.\n Foundations of Data Intensive Applications Large Scale Data Analytics Under the Hood. 2021. ISBN 9781119713012.\n Haloi, S. (2015). Apache ZooKeeper Essentials. Packt Publishing Ltd. pp. 100–101. ISBN 9781784398323. Archived from the original on 2023-01-20. Retrieved 2018-07-20.\n LeLann, G. (1977). \"Distributed systems - toward a formal approach\". Information Processing. 77: 155·160 – via Elsevier.\n R. G. Gallager, P. A. Humblet, and P. M. Spira (January 1983). \"A Distributed Algorithm for Minimum-Weight Spanning Trees\" (PDF). ACM Transactions on Programming Languages and Systems. 5 (1): 66–77. doi:10.1145/357195.357200. S2CID 2758285. Archived (PDF) from the original on 2017-09-26.\n Korach, Ephraim; Kutten, Shay; Moran, Shlomo (1990). \"A Modular Technique for the Design of Efficient Distributed Leader Finding Algorithms\" (PDF). ACM Transactions on Programming Languages and Systems. 12 (1): 84–101. CiteSeerX 10.1.1.139.7342. doi:10.1145/77606.77610. S2CID 9175968. Archived (PDF) from the original on 2007-04-18.\n Hamilton, Howard. \"Distributed Algorithms\". Archived from the original on 2012-11-24. Retrieved 2013-03-03.\n \"Major unsolved problems in distributed systems?\". cstheory.stackexchange.com. Archived from the original on 20 January 2023. Retrieved 16 March 2018.\n \"How big data and distributed systems solve traditional scalability problems\". theserverside.com. Archived from the original on 17 March 2018. Retrieved 16 March 2018.\n Svozil, K. (2011). \"Indeterminism and Randomness Through Physics\". In Hector, Z. (ed.). Randomness Through Computation: Some Answers, More Questions. World Scientific. pp. 112–3. ISBN 9789814462631. Archived from the original on 2020-08-01. Retrieved 2018-07-20.\n Papadimitriou (1994), Section 19.3.\nReferences\nBooks\nAndrews, Gregory R. (2000), Foundations of Multithreaded, Parallel, and Distributed Programming, Addison–Wesley, ISBN 978-0-201-35752-3.\nArora, Sanjeev; Barak, Boaz (2009), Computational Complexity – A Modern Approach, Cambridge, ISBN 978-0-521-42426-4.\nCormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L. (1990), Introduction to Algorithms (1st ed.), MIT Press, ISBN 978-0-262-03141-7.\nDolev, Shlomi (2000), Self-Stabilization, MIT Press, ISBN 978-0-262-04178-2.\nElmasri, Ramez; Navathe, Shamkant B. (2000), Fundamentals of Database Systems (3rd ed.), Addison–Wesley, ISBN 978-0-201-54263-9.\nGhosh, Sukumar (2007), Distributed Systems – An Algorithmic Approach, Chapman & Hall/CRC, ISBN 978-1-58488-564-1.\nLynch, Nancy A. (1996), Distributed Algorithms, Morgan Kaufmann, ISBN 978-1-55860-348-6.\nHerlihy, Maurice P.; Shavit, Nir N. (2008), The Art of Multiprocessor Programming, Morgan Kaufmann, ISBN 978-0-12-370591-4.\nPapadimitriou, Christos H. (1994), Computational Complexity, Addison–Wesley, ISBN 978-0-201-53082-7.\nPeleg, David (2000), Distributed Computing: A Locality-Sensitive Approach, SIAM, ISBN 978-0-89871-464-7, archived from the original on 2009-08-06, retrieved 2009-07-16.\nArticles\nCole, Richard; Vishkin, Uzi (1986), \"Deterministic coin tossing with applications to optimal parallel list ranking\", Information and Control, 70 (1): 32–53, doi:10.1016/S0019-9958(86)80023-7.\nKeidar, Idit (2008), \"Distributed computing column 32 – The year in review\", ACM SIGACT News, 39 (4): 53–54, CiteSeerX 10.1.1.116.1285, doi:10.1145/1466390.1466402, S2CID 7607391, archived from the original on 2014-01-16, retrieved 2009-08-20.\nLinial, Nathan (1992), \"Locality in distributed graph algorithms\", SIAM Journal on Computing, 21 (1): 193–201, CiteSeerX 10.1.1.471.6378, doi:10.1137/0221015.\nNaor, Moni; Stockmeyer, Larry (1995), \"What can be computed locally?\" (PDF), SIAM Journal on Computing, 24 (6): 1259–1277, CiteSeerX 10.1.1.29.669, doi:10.1137/S0097539793254571, archived (PDF) from the original on 2013-01-08.\nWeb sites\nGodfrey, Bill (2002). \"A primer on distributed computing\". Archived from the original on 2021-05-13. Retrieved 2021-05-13.\nPeter, Ian (2004). \"Ian Peter's History of the Internet\". Archived from the original on 2010-01-20. Retrieved 2009-08-04.\nFurther reading\nBooks\nAttiya, Hagit and Jennifer Welch (2004), Distributed Computing: Fundamentals, Simulations, and Advanced Topics, Wiley-Interscience ISBN 0-471-45324-2.\nChristian Cachin; Rachid Guerraoui; Luís Rodrigues (2011), Introduction to Reliable and Secure Distributed Programming (2. ed.), Springer, Bibcode:2011itra.book.....C, ISBN 978-3-642-15259-7\nCoulouris, George; et al. (2011), Distributed Systems: Concepts and Design (5th Edition), Addison-Wesley ISBN 0-132-14301-1.\nFaber, Jim (1998), Java Distributed Computing, O'Reilly, archived from the original on 2010-08-24, retrieved 2010-09-29: Java Distributed Computing by Jim Faber, 1998 Archived 2010-08-24 at the Wayback Machine\nGarg, Vijay K. (2002), Elements of Distributed Computing, Wiley-IEEE Press ISBN 0-471-03600-5.\nTel, Gerard (1994), Introduction to Distributed Algorithms, Cambridge University Press\nChandy, Mani; et al. (1988), Parallel Program Design, Addison-Wesley ISBN 0201058669\nDusseau, Remzi H.; Dusseau, Andrea (2016). Operating Systems: Three Easy Pieces, Chapter 48 Distributed Systems (PDF). Archived from the original (PDF) on 31 August 2021. Retrieved 8 October 2021.\nArticles\nKeidar, Idit; Rajsbaum, Sergio, eds. (2000–2009), \"Distributed computing column\", ACM SIGACT News, archived from the original on 2014-01-16, retrieved 2009-08-16.\nBirrell, A. D.; Levin, R.; Schroeder, M. D.; Needham, R. M. (April 1982). \"Grapevine: An exercise in distributed computing\" (PDF). Communications of the ACM. 25 (4): 260–274. doi:10.1145/358468.358487. S2CID 16066616. Archived (PDF) from the original on 2016-07-30.\nConference Papers\nRodriguez, Carlos; Villagra, Marcos; Baran, Benjamin (2007). \"Asynchronous team algorithms for Boolean Satisfiability\". 2007 2nd Bio-Inspired Models of Network, Information and Computing Systems. pp. 66–69. doi:10.1109/BIMNICS.2007.4610083. S2CID 15185219.\nExternal links\n\nWikiquote has quotations related to Distributed computing.\n Media related to Distributed computing at Wikimedia Commons\nvte\nParallel computing\nAuthority control databases Edit this at Wikidata\nCategory: Distributed computing\nThis page was last edited on 4 October 2024, at 08:15 (UTC).\nText is available under the Creative Commons Attribution-ShareAlike 4.0 License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\nPrivacy policyAbout WikipediaDisclaimersContact WikipediaCode of ConductDevelopersStatisticsCookie statementMobile view\nWikimedia FoundationPowered by MediaWiki\n\n\nMain menu\n\nWikipediaThe Free Encyclopedia\nSearch Wikipedia\nSearch\nDonate\nCreate account\nLog in\n\nPersonal tools\nContents hide\n(Top)\nIntroduction\nPatterns\nParallel and distributed computing\nHistory\nArchitectures\nApplications\nExamples\nReactive distributed systems\nTheoretical foundations\nToggle Theoretical foundations subsection\nModels\nAn example\nComplexity measures\nOther problems\nElection\nProperties of distributed systems\nSee also\nNotes\nReferences\nFurther reading\nExternal links\nDistributed computing\n\nArticle\nTalk\nRead\nEdit\nView history\n\nTools\nAppearance hide\nText\n\nSmall\n\nStandard\n\nLarge\nWidth\n\nStandard\n\nWide\nColor (beta)\n\nAutomatic\n\nLight\n\nDark\nFrom Wikipedia, the free encyclopedia\nNot to be confused with Decentralized computing.\nDistributed computing is a field of computer science that studies distributed systems, defined as computer systems whose inter-communicating components are located on different networked computers.[1][2]\n\nThe components of a distributed system communicate and coordinate their actions by passing messages to one another in order to achieve a common goal. Three significant challenges of distributed systems are: maintaining concurrency of components, overcoming the lack of a global clock, and managing the independent failure of components.[1] When a component of one system fails, the entire system does not fail.[3] Examples of distributed systems vary from SOA-based systems to microservices to massively multiplayer online games to peer-to-peer applications. Distributed systems cost significantly more than monolithic architectures, primarily due to increased needs for additional hardware, servers, gateways, firewalls, new subnets, proxies, and so on.[4] Also, distributed systems are prone to fallacies of distributed computing. On the other hand, a well designed distributed system is more scalable, more durable, more changeable and more fine-tuned than a monolithic application deployed on a single machine.[5] According to Marc Brooker: \"a system is scalable in the range where marginal cost of additional workload is nearly constant.\" Serverless technologies fit this definition but you need to consider total cost of ownership not just the infra cost. [6]\n\nA computer program that runs within a distributed system is called a distributed program,[7] and distributed programming is the process of writing such programs.[8] There are many different types of implementations for the message passing mechanism, including pure HTTP, RPC-like connectors and message queues.[9]\n\nDistributed computing also refers to the use of distributed systems to solve computational problems. In distributed computing, a problem is divided into many tasks, each of which is solved by one or more computers,[10] which communicate with each other via message passing.[11]\n\nIntroduction\nThe word distributed in terms such as \"distributed system\", \"distributed programming\", and \"distributed algorithm\" originally referred to computer networks where individual computers were physically distributed within some geographical area.[12] The terms are nowadays used in a much wider sense, even referring to autonomous processes that run on the same physical computer and interact with each other by message passing.[11]\n\nWhile there is no single definition of a distributed system,[13] the following defining properties are commonly used as:\n\nThere are several autonomous computational entities (computers or nodes), each of which has its own local memory.[14]\nThe entities communicate with each other by message passing.[15]\nA distributed system may have a common goal, such as solving a large computational problem;[16] the user then perceives the collection of autonomous processors as a unit. Alternatively, each computer may have its own user with individual needs, and the purpose of the distributed system is to coordinate the use of shared resources or provide communication services to the users.[17]\n\nOther typical properties of distributed systems include the following:\n\nThe system has to tolerate failures in individual computers.[18]\nThe structure of the system (network topology, network latency, number of computers) is not known in advance, the system may consist of different kinds of computers and network links, and the system may change during the execution of a distributed program.[19]\nEach computer has only a limited, incomplete view of the system. Each computer may know only one part of the input.[20]\nPatterns\nHere are common architectural patterns used for distributed computing:[21]\n\nSaga interaction pattern\nMicroservices\nEvent driven architecture\nParallel and distributed computing\n\n(a), (b): a distributed system.\n(c): a parallel system.\nDistributed systems are groups of networked computers which share a common goal for their work. The terms \"concurrent computing\", \"parallel computing\", and \"distributed computing\" have much overlap, and no clear distinction exists between them.[22] The same system may be characterized both as \"parallel\" and \"distributed\"; the processors in a typical distributed system run concurrently in parallel.[23] Parallel computing may be seen as a particularly tightly coupled form of distributed computing,[24] and distributed computing may be seen as a loosely coupled form of parallel computing.[13] Nevertheless, it is possible to roughly classify concurrent systems as \"parallel\" or \"distributed\" using the following criteria:\n\nIn parallel computing, all processors may have access to a shared memory to exchange information between processors.[25]\nIn distributed computing, each processor has its own private memory (distributed memory). Information is exchanged by passing messages between the processors.[26]\nThe figure on the right illustrates the difference between distributed and parallel systems. Figure (a) is a schematic view of a typical distributed system; the system is represented as a network topology in which each node is a computer and each line connecting the nodes is a communication link. Figure (b) shows the same distributed system in more detail: each computer has its own local memory, and information can be exchanged only by passing messages from one node to another by using the available communication links. Figure (c) shows a parallel system in which each processor has a direct access to a shared memory.\n\nThe situation is further complicated by the traditional uses of the terms parallel and distributed algorithm that do not quite match the above definitions of parallel and distributed systems (see below for more detailed discussion). Nevertheless, as a rule of thumb, high-performance parallel computation in a shared-memory multiprocessor uses parallel algorithms while the coordination of a large-scale distributed system uses distributed algorithms.[27]\n\nHistory\nThe use of concurrent processes which communicate through message-passing has its roots in operating system architectures studied in the 1960s.[28] The first widespread distributed systems were local-area networks such as Ethernet, which was invented in the 1970s.[29]\n\nARPANET, one of the predecessors of the Internet, was introduced in the late 1960s, and ARPANET e-mail was invented in the early 1970s. E-mail became the most successful application of ARPANET,[30] and it is probably the earliest example of a large-scale distributed application. In addition to ARPANET (and its successor, the global Internet), other early worldwide computer networks included Usenet and FidoNet from the 1980s, both of which were used to support distributed discussion systems.[31]\n\nThe study of distributed computing became its own branch of computer science in the late 1970s and early 1980s. The first conference in the field, Symposium on Principles of Distributed Computing (PODC), dates back to 1982, and its counterpart International Symposium on Distributed Computing (DISC) was first held in Ottawa in 1985 as the International Workshop on Distributed Algorithms on Graphs.[32]\n\nArchitectures\nVarious hardware and software architectures are used for distributed computing. At a lower level, it is necessary to interconnect multiple CPUs with some sort of network, regardless of whether that network is printed onto a circuit board or made up of loosely coupled devices and cables. At a higher level, it is necessary to interconnect processes running on those CPUs with some sort of communication system.[33]\n\nWhether these CPUs share resources or not determines a first distinction between three types of architecture:\n\nShared memory\nShared disk\nShared nothing.\nDistributed programming typically falls into one of several basic architectures: client–server, three-tier, n-tier, or peer-to-peer; or categories: loose coupling, or tight coupling.[34]\n\nClient–server: architectures where smart clients contact the server for data then format and display it to the users. Input at the client is committed back to the server when it represents a permanent change.\nThree-tier: architectures that move the client intelligence to a middle tier so that stateless clients can be used. This simplifies application deployment. Most web applications are three-tier.\nn-tier: architectures that refer typically to web applications which further forward their requests to other enterprise services. This type of application is the one most responsible for the success of application servers.\nPeer-to-peer: architectures where there are no special machines that provide a service or manage the network resources.[35]: 227  Instead all responsibilities are uniformly divided among all machines, known as peers. Peers can serve both as clients and as servers.[36] Examples of this architecture include BitTorrent and the bitcoin network.\nAnother basic aspect of distributed computing architecture is the method of communicating and coordinating work among concurrent processes. Through various message passing protocols, processes may communicate directly with one another, typically in a main/sub relationship. Alternatively, a \"database-centric\" architecture can enable distributed computing to be done without any form of direct inter-process communication, by utilizing a shared database.[37] Database-centric architecture in particular provides relational processing analytics in a schematic architecture allowing for live environment relay. This enables distributed computing functions both within and beyond the parameters of a networked database.[38]\n\nApplications\nReasons for using distributed systems and distributed computing may include:\n\nThe very nature of an application may require the use of a communication network that connects several computers: for example, data produced in one physical location and required in another location.\nThere are many cases in which the use of a single computer would be possible in principle, but the use of a distributed system is beneficial for practical reasons. For example:\nIt can allow for much larger storage and memory, faster compute, and higher bandwidth than a single machine.\nIt can provide more reliability than a non-distributed system, as there is no single point of failure. Moreover, a distributed system may be easier to expand and manage than a monolithic uniprocessor system.[39]\nIt may be more cost-efficient to obtain the desired level of performance by using a cluster of several low-end computers, in comparison with a single high-end computer.\nExamples\nExamples of distributed systems and applications of distributed computing include the following:[40]\n\ntelecommunications networks:\ntelephone networks and cellular networks,\ncomputer networks such as the Internet,\nwireless sensor networks,\nrouting algorithms;\nnetwork applications:\nWorld Wide Web and peer-to-peer networks,\nmassively multiplayer online games and virtual reality communities,\ndistributed databases and distributed database management systems,\nnetwork file systems,\ndistributed cache such as burst buffers,\ndistributed information processing systems such as banking systems and airline reservation systems;\nreal-time process control:\naircraft control systems,\nindustrial control systems;\nparallel computation:\nscientific computing, including cluster computing, grid computing, cloud computing,[41] and various volunteer computing projects,\ndistributed rendering in computer graphics.\npeer-to-peer\nReactive distributed systems\nAccording to Reactive Manifesto, reactive distributed systems are responsive, resilient, elastic and message-driven. Subsequently, Reactive systems are more flexible, loosely-coupled and scalable. To make your systems reactive, you are advised to implement Reactive Principles. Reactive Principles are a set of principles and patterns which help to make your cloud native application as well as edge native applications more reactive. [42]\n\nTheoretical foundations\nMain article: Distributed algorithm\nModels\nMany tasks that we would like to automate by using a computer are of question–answer type: we would like to ask a question and the computer should produce an answer. In theoretical computer science, such tasks are called computational problems. Formally, a computational problem consists of instances together with a solution for each instance. Instances are questions that we can ask, and solutions are desired answers to these questions.\n\nTheoretical computer science seeks to understand which computational problems can be solved by using a computer (computability theory) and how efficiently (computational complexity theory). Traditionally, it is said that a problem can be solved by using a computer if we can design an algorithm that produces a correct solution for any given instance. Such an algorithm can be implemented as a computer program that runs on a general-purpose computer: the program reads a problem instance from input, performs some computation, and produces the solution as output. Formalisms such as random-access machines or universal Turing machines can be used as abstract models of a sequential general-purpose computer executing such an algorithm.[43][44]\n\nThe field of concurrent and distributed computing studies similar questions in the case of either multiple computers, or a computer that executes a network of interacting processes: which computational problems can be solved in such a network and how efficiently? However, it is not at all obvious what is meant by \"solving a problem\" in the case of a concurrent or distributed system: for example, what is the task of the algorithm designer, and what is the concurrent or distributed equivalent of a sequential general-purpose computer?[citation needed]\n\nThe discussion below focuses on the case of multiple computers, although many of the issues are the same for concurrent processes running on a single computer.\n\nThree viewpoints are commonly used:\n\nParallel algorithms in shared-memory model\nAll processors have access to a shared memory. The algorithm designer chooses the program executed by each processor.\nOne theoretical model is the parallel random-access machines (PRAM) that are used.[45] However, the classical PRAM model assumes synchronous access to the shared memory.\nShared-memory programs can be extended to distributed systems if the underlying operating system encapsulates the communication between nodes and virtually unifies the memory across all individual systems.\nA model that is closer to the behavior of real-world multiprocessor machines and takes into account the use of machine instructions, such as Compare-and-swap (CAS), is that of asynchronous shared memory. There is a wide body of work on this model, a summary of which can be found in the literature.[46][47]\nParallel algorithms in message-passing model\nThe algorithm designer chooses the structure of the network, as well as the program executed by each computer.\nModels such as Boolean circuits and sorting networks are used.[48] A Boolean circuit can be seen as a computer network: each gate is a computer that runs an extremely simple computer program. Similarly, a sorting network can be seen as a computer network: each comparator is a computer.\nDistributed algorithms in message-passing model\nThe algorithm designer only chooses the computer program. All computers run the same program. The system must work correctly regardless of the structure of the network.\nA commonly used model is a graph with one finite-state machine per node.\nIn the case of distributed algorithms, computational problems are typically related to graphs. Often the graph that describes the structure of the computer network is the problem instance. This is illustrated in the following example.[49]\n\nAn example\nConsider the computational problem of finding a coloring of a given graph G. Different fields might take the following approaches:\n\nCentralized algorithms[49]\nThe graph G is encoded as a string, and the string is given as input to a computer. The computer program finds a coloring of the graph, encodes the coloring as a string, and outputs the result.\nParallel algorithms\nAgain, the graph G is encoded as a string. However, multiple computers can access the same string in parallel. Each computer might focus on one part of the graph and produce a coloring for that part.\nThe main focus is on high-performance computation that exploits the processing power of multiple computers in parallel.\nDistributed algorithms\nThe graph G is the structure of the computer network. There is one computer for each node of G and one communication link for each edge of G. Initially, each computer only knows about its immediate neighbors in the graph G; the computers must exchange messages with each other to discover more about the structure of G. Each computer must produce its own color as output.\nThe main focus is on coordinating the operation of an arbitrary distributed system.[49]\nWhile the field of parallel algorithms has a different focus than the field of distributed algorithms, there is much interaction between the two fields. For example, the Cole–Vishkin algorithm for graph coloring[50] was originally presented as a parallel algorithm, but the same technique can also be used directly as a distributed algorithm.\n\nMoreover, a parallel algorithm can be implemented either in a parallel system (using shared memory) or in a distributed system (using message passing).[51] The traditional boundary between parallel and distributed algorithms (choose a suitable network vs. run in any given network) does not lie in the same place as the boundary between parallel and distributed systems (shared memory vs. message passing).\n\nComplexity measures\nIn parallel algorithms, yet another resource in addition to time and space is the number of computers. Indeed, often there is a trade-off between the running time and the number of computers: the problem can be solved faster if there are more computers running in parallel (see speedup). If a decision problem can be solved in polylogarithmic time by using a polynomial number of processors, then the problem is said to be in the class NC.[52] The class NC can be defined equally well by using the PRAM formalism or Boolean circuits—PRAM machines can simulate Boolean circuits efficiently and vice versa.[53]\n\nIn the analysis of distributed algorithms, more attention is usually paid on communication operations than computational steps. Perhaps the simplest model of distributed computing is a synchronous system where all nodes operate in a lockstep fashion. This model is commonly known as the LOCAL model. During each communication round, all nodes in parallel (1) receive the latest messages from their neighbours, (2) perform arbitrary local computation, and (3) send new messages to their neighbors. In such systems, a central complexity measure is the number of synchronous communication rounds required to complete the task.[54]\n\nThis complexity measure is closely related to the diameter of the network. Let D be the diameter of the network. On the one hand, any computable problem can be solved trivially in a synchronous distributed system in approximately 2D communication rounds: simply gather all information in one location (D rounds), solve the problem, and inform each node about the solution (D rounds).\n\nOn the other hand, if the running time of the algorithm is much smaller than D communication rounds, then the nodes in the network must produce their output without having the possibility to obtain information about distant parts of the network. In other words, the nodes must make globally consistent decisions based on information that is available in their local D-neighbourhood. Many distributed algorithms are known with the running time much smaller than D rounds, and understanding which problems can be solved by such algorithms is one of the central research questions of the field.[55] Typically an algorithm which solves a problem in polylogarithmic time in the network size is considered efficient in this model.\n\nAnother commonly used measure is the total number of bits transmitted in the network (cf. communication complexity).[56] The features of this concept are typically captured with the CONGEST(B) model, which is similarly defined as the LOCAL model, but where single messages can only contain B bits.\n\nOther problems\nTraditional computational problems take the perspective that the user asks a question, a computer (or a distributed system) processes the question, then produces an answer and stops. However, there are also problems where the system is required not to stop, including the dining philosophers problem and other similar mutual exclusion problems. In these problems, the distributed system is supposed to continuously coordinate the use of shared resources so that no conflicts or deadlocks occur.\n\nThere are also fundamental challenges that are unique to distributed computing, for example those related to fault-tolerance. Examples of related problems include consensus problems,[57] Byzantine fault tolerance,[58] and self-stabilisation.[59]\n\nMuch research is also focused on understanding the asynchronous nature of distributed systems:\n\nSynchronizers can be used to run synchronous algorithms in asynchronous systems.[60]\nLogical clocks provide a causal happened-before ordering of events.[61]\nClock synchronization algorithms provide globally consistent physical time stamps.[62]\nNote that in distributed systems, latency should be measured through \"99th percentile\" because \"median\" and \"average\" can be misleading.[63]\n\nElection\nCoordinator election (or leader election) is the process of designating a single process as the organizer of some task distributed among several computers (nodes). Before the task is begun, all network nodes are either unaware which node will serve as the \"coordinator\" (or leader) of the task, or unable to communicate with the current coordinator. After a coordinator election algorithm has been run, however, each node throughout the network recognizes a particular, unique node as the task coordinator.[64]\n\nThe network nodes communicate among themselves in order to decide which of them will get into the \"coordinator\" state. For that, they need some method in order to break the symmetry among them. For example, if each node has unique and comparable identities, then the nodes can compare their identities, and decide that the node with the highest identity is the coordinator.[64]\n\nThe definition of this problem is often attributed to LeLann, who formalized it as a method to create a new token in a token ring network in which the token has been lost.[65]\n\nCoordinator election algorithms are designed to be economical in terms of total bytes transmitted, and time. The algorithm suggested by Gallager, Humblet, and Spira[66] for general undirected graphs has had a strong impact on the design of distributed algorithms in general, and won the Dijkstra Prize for an influential paper in distributed computing.\n\nMany other algorithms were suggested for different kinds of network graphs, such as undirected rings, unidirectional rings, complete graphs, grids, directed Euler graphs, and others. A general method that decouples the issue of the graph family from the design of the coordinator election algorithm was suggested by Korach, Kutten, and Moran.[67]\n\nIn order to perform coordination, distributed systems employ the concept of coordinators. The coordinator election problem is to choose a process from among a group of processes on different processors in a distributed system to act as the central coordinator. Several central coordinator election algorithms exist.[68]\n\nProperties of distributed systems\nSo far the focus has been on designing a distributed system that solves a given problem. A complementary research problem is studying the properties of a given distributed system.[69][70]\n\nThe halting problem is an analogous example from the field of centralised computation: we are given a computer program and the task is to decide whether it halts or runs forever. The halting problem is undecidable in the general case, and naturally understanding the behaviour of a computer network is at least as hard as understanding the behaviour of one computer.[71]\n\nHowever, there are many interesting special cases that are decidable. In particular, it is possible to reason about the behaviour of a network of finite-state machines. One example is telling whether a given network of interacting (asynchronous and non-deterministic) finite-state machines can reach a deadlock. This problem is PSPACE-complete,[72] i.e., it is decidable, but not likely that there is an efficient (centralised, parallel or distributed) algorithm that solves the problem in the case of large networks.\n\nSee also\nActor model\nAppScale\nBOINC\nCode mobility\nDataflow programming\nDecentralized computing\nDistributed algorithm\nDistributed algorithmic mechanism design\nDistributed cache\nDistributed GIS\nDistributed networking\nDistributed operating system\nEventual consistency\nEdsger W. Dijkstra Prize in Distributed Computing\nFederation (information technology)\nFlat neighborhood network\nFog computing\nFolding@home\nGrid computing\nInferno\nInternet GIS\nJungle computing\nLayered queueing network\nLibrary Oriented Architecture (LOA)\nList of distributed computing conferences\nList of volunteer computing projects\nModel checking\nOpenHarmony\nHarmonyOS\nParallel distributed processing\nParallel programming model\nPlan 9 from Bell Labs\nShared nothing architecture\nWeb GIS\nNotes\n Tanenbaum, Andrew S.; Steen, Maarten van (2002). Distributed systems: principles and paradigms. Upper Saddle River, NJ: Pearson Prentice Hall. ISBN 0-13-088893-1. Archived from the original on 2020-08-12. Retrieved 2020-08-28.\n \"Distributed Programs\". Texts in Computer Science. London: Springer London. 2010. pp. 373–406. doi:10.1007/978-1-84882-745-5_11. ISBN 978-1-84882-744-8. ISSN 1868-0941. Systems consist of a number of physically distributed components that work independently using their private storage, but also communicate from time to time by explicit message passing. Such systems are called distributed systems.\n Dusseau & Dusseau 2016, p. 1–2.\n Ford, Neal (March 3, 2020). Fundamentals of Software Architecture: An Engineering Approach (1st ed.). O'Reilly Media. pp. 146–147. ISBN 978-1492043454.\n Monolith to Microservices Evolutionary Patterns to Transform Your Monolith. O'Reilly Media. ISBN 9781492047810.\n Building Serverless Applications on Knative. O'Reilly Media. ISBN 9781098142049.\n \"Distributed Programs\". Texts in Computer Science. London: Springer London. 2010. pp. 373–406. doi:10.1007/978-1-84882-745-5_11. ISBN 978-1-84882-744-8. ISSN 1868-0941. Distributed programs are abstract descriptions of distributed systems. A distributed program consists of a collection of processes that work concurrently and communicate by explicit message passing. Each process can access a set of variables which are disjoint from the variables that can be changed by any other process.\n Andrews (2000). Dolev (2000). Ghosh (2007), p. 10.\n Magnoni, L. (2015). \"Modern Messaging for Distributed Sytems (sic)\". Journal of Physics: Conference Series. 608 (1): 012038. doi:10.1088/1742-6596/608/1/012038. ISSN 1742-6596.\n Godfrey (2002).\n Andrews (2000), p. 291–292. Dolev (2000), p. 5.\n Lynch (1996), p. 1.\n Ghosh (2007), p. 10.\n Andrews (2000), pp. 8–9, 291. Dolev (2000), p. 5. Ghosh (2007), p. 3. Lynch (1996), p. xix, 1. Peleg (2000), p. xv.\n Andrews (2000), p. 291. Ghosh (2007), p. 3. Peleg (2000), p. 4.\n Ghosh (2007), p. 3–4. Peleg (2000), p. 1.\n Ghosh (2007), p. 4. Peleg (2000), p. 2.\n Ghosh (2007), p. 4, 8. Lynch (1996), p. 2–3. Peleg (2000), p. 4.\n Lynch (1996), p. 2. Peleg (2000), p. 1.\n Ghosh (2007), p. 7. Lynch (1996), p. xix, 2. Peleg (2000), p. 4.\n Fundamentals of Software Architecture: An Engineering Approach. O'Reilly Media. 2020. ISBN 978-1492043454.\n Ghosh (2007), p. 10. Keidar (2008).\n Lynch (1996), p. xix, 1–2. Peleg (2000), p. 1.\n Peleg (2000), p. 1.\n Papadimitriou (1994), Chapter 15. Keidar (2008).\n See references in Introduction.\n Bentaleb, A.; Yifan, L.; Xin, J.; et al. (2016). \"Parallel and Distributed Algorithms\" (PDF). National University of Singapore. Archived (PDF) from the original on 2017-03-26. Retrieved 20 July 2018.\n Andrews (2000), p. 348.\n Andrews (2000), p. 32.\n Peter (2004), The history of email Archived 2009-04-15 at the Wayback Machine.\n Banks, M. (2012). On the Way to the Web: The Secret History of the Internet and its Founders. Apress. pp. 44–5. ISBN 9781430250746. Archived from the original on 2023-01-20. Retrieved 2018-07-20.\n Tel, G. (2000). Introduction to Distributed Algorithms. Cambridge University Press. pp. 35–36. ISBN 9780521794831. Archived from the original on 2023-01-20. Retrieved 2018-07-20.\n Ohlídal, M.; Jaroš, J.; Schwarz, J.; et al. (2006). \"Evolutionary Design of OAB and AAB Communication Schedules for Interconnection Networks\". In Rothlauf, F.; Branke, J.; Cagnoni, S. (eds.). Applications of Evolutionary Computing. Springer Science & Business Media. pp. 267–78. ISBN 9783540332374.\n \"Real Time And Distributed Computing Systems\" (PDF). ISSN 2278-0661. Archived from the original (PDF) on 2017-01-10. Retrieved 2017-01-09. {{cite journal}}: Cite journal requires |journal= (help)\n Vigna P, Casey MJ. The Age of Cryptocurrency: How Bitcoin and the Blockchain Are Challenging the Global Economic Order St. Martin's Press January 27, 2015 ISBN 9781250065636\n Quang Hieu Vu; Mihai Lupu; Beng Chin Ooi (2010). Peer-to-peer computing : principles and applications. Heidelberg: Springer. p. 16. ISBN 9783642035135. OCLC 663093862.\n Lind P, Alm M (2006), \"A database-centric virtual chemistry system\", J Chem Inf Model, 46 (3): 1034–9, doi:10.1021/ci050360b, PMID 16711722.\n Chiu, G (1990). \"A model for optimal database allocation in distributed computing systems\". Proceedings. IEEE INFOCOM'90: Ninth Annual Joint Conference of the IEEE Computer and Communications Societies.\n Elmasri & Navathe (2000), Section 24.1.2.\n Andrews (2000), p. 10–11. Ghosh (2007), p. 4–6. Lynch (1996), p. xix, 1. Peleg (2000), p. xv. Elmasri & Navathe (2000), Section 24.\n Haussmann, J. (2019). \"Cost-efficient parallel processing of irregularly structured problems in cloud computing environments\". Journal of Cluster Computing. 22 (3): 887–909. doi:10.1007/s10586-018-2879-3. S2CID 54447518.\n Reactive Application Development. Manning. 2018. ISBN 9781638355816.\n Toomarian, N.B.; Barhen, J.; Gulati, S. (1992). \"Neural Networks for Real-Time Robotic Applications\". In Fijany, A.; Bejczy, A. (eds.). Parallel Computation Systems For Robotics: Algorithms And Architectures. World Scientific. p. 214. ISBN 9789814506175. Archived from the original on 2020-08-01. Retrieved 2018-07-20.\n Savage, J.E. (1998). Models of Computation: Exploring the Power of Computing. Addison Wesley. p. 209. ISBN 9780201895391.\n Cormen, Leiserson & Rivest (1990), Section 30.\n Herlihy & Shavit (2008), Chapters 2–6.\n Lynch (1996)\n Cormen, Leiserson & Rivest (1990), Sections 28 and 29.\n TULSIRAMJI GAIKWAD-PATIL College of Engineering & Technology, Nagpur Department of Information Technology Introduction to Distributed Systems[1]\n Cole & Vishkin (1986). Cormen, Leiserson & Rivest (1990), Section 30.5.\n Andrews (2000), p. ix.\n Arora & Barak (2009), Section 6.7. Papadimitriou (1994), Section 15.3.\n Papadimitriou (1994), Section 15.2.\n Lynch (1996), p. 17–23.\n Peleg (2000), Sections 2.3 and 7. Linial (1992). Naor & Stockmeyer (1995).\n Schneider, J.; Wattenhofer, R. (2011). \"Trading Bit, Message, and Time Complexity of Distributed Algorithms\". In Peleg, D. (ed.). Distributed Computing. Springer Science & Business Media. pp. 51–65. ISBN 9783642240997. Archived from the original on 2020-08-01. Retrieved 2018-07-20.\n Lynch (1996), Sections 5–7. Ghosh (2007), Chapter 13.\n Lynch (1996), p. 99–102. Ghosh (2007), p. 192–193.\n Dolev (2000). Ghosh (2007), Chapter 17.\n Lynch (1996), Section 16. Peleg (2000), Section 6.\n Lynch (1996), Section 18. Ghosh (2007), Sections 6.2–6.3.\n Ghosh (2007), Section 6.4.\n Foundations of Data Intensive Applications Large Scale Data Analytics Under the Hood. 2021. ISBN 9781119713012.\n Haloi, S. (2015). Apache ZooKeeper Essentials. Packt Publishing Ltd. pp. 100–101. ISBN 9781784398323. Archived from the original on 2023-01-20. Retrieved 2018-07-20.\n LeLann, G. (1977). \"Distributed systems - toward a formal approach\". Information Processing. 77: 155·160 – via Elsevier.\n R. G. Gallager, P. A. Humblet, and P. M. Spira (January 1983). \"A Distributed Algorithm for Minimum-Weight Spanning Trees\" (PDF). ACM Transactions on Programming Languages and Systems. 5 (1): 66–77. doi:10.1145/357195.357200. S2CID 2758285. Archived (PDF) from the original on 2017-09-26.\n Korach, Ephraim; Kutten, Shay; Moran, Shlomo (1990). \"A Modular Technique for the Design of Efficient Distributed Leader Finding Algorithms\" (PDF). ACM Transactions on Programming Languages and Systems. 12 (1): 84–101. CiteSeerX 10.1.1.139.7342. doi:10.1145/77606.77610. S2CID 9175968. Archived (PDF) from the original on 2007-04-18.\n Hamilton, Howard. \"Distributed Algorithms\". Archived from the original on 2012-11-24. Retrieved 2013-03-03.\n \"Major unsolved problems in distributed systems?\". cstheory.stackexchange.com. Archived from the original on 20 January 2023. Retrieved 16 March 2018.\n \"How big data and distributed systems solve traditional scalability problems\". theserverside.com. Archived from the original on 17 March 2018. Retrieved 16 March 2018.\n Svozil, K. (2011). \"Indeterminism and Randomness Through Physics\". In Hector, Z. (ed.). Randomness Through Computation: Some Answers, More Questions. World Scientific. pp. 112–3. ISBN 9789814462631. Archived from the original on 2020-08-01. Retrieved 2018-07-20.\n Papadimitriou (1994), Section 19.3.\nReferences\nBooks\nAndrews, Gregory R. (2000), Foundations of Multithreaded, Parallel, and Distributed Programming, Addison–Wesley, ISBN 978-0-201-35752-3.\nArora, Sanjeev; Barak, Boaz (2009), Computational Complexity – A Modern Approach, Cambridge, ISBN 978-0-521-42426-4.\nCormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L. (1990), Introduction to Algorithms (1st ed.), MIT Press, ISBN 978-0-262-03141-7.\nDolev, Shlomi (2000), Self-Stabilization, MIT Press, ISBN 978-0-262-04178-2.\nElmasri, Ramez; Navathe, Shamkant B. (2000), Fundamentals of Database Systems (3rd ed.), Addison–Wesley, ISBN 978-0-201-54263-9.\nGhosh, Sukumar (2007), Distributed Systems – An Algorithmic Approach, Chapman & Hall/CRC, ISBN 978-1-58488-564-1.\nLynch, Nancy A. (1996), Distributed Algorithms, Morgan Kaufmann, ISBN 978-1-55860-348-6.\nHerlihy, Maurice P.; Shavit, Nir N. (2008), The Art of Multiprocessor Programming, Morgan Kaufmann, ISBN 978-0-12-370591-4.\nPapadimitriou, Christos H. (1994), Computational Complexity, Addison–Wesley, ISBN 978-0-201-53082-7.\nPeleg, David (2000), Distributed Computing: A Locality-Sensitive Approach, SIAM, ISBN 978-0-89871-464-7, archived from the original on 2009-08-06, retrieved 2009-07-16.\nArticles\nCole, Richard; Vishkin, Uzi (1986), \"Deterministic coin tossing with applications to optimal parallel list ranking\", Information and Control, 70 (1): 32–53, doi:10.1016/S0019-9958(86)80023-7.\nKeidar, Idit (2008), \"Distributed computing column 32 – The year in review\", ACM SIGACT News, 39 (4): 53–54, CiteSeerX 10.1.1.116.1285, doi:10.1145/1466390.1466402, S2CID 7607391, archived from the original on 2014-01-16, retrieved 2009-08-20.\nLinial, Nathan (1992), \"Locality in distributed graph algorithms\", SIAM Journal on Computing, 21 (1): 193–201, CiteSeerX 10.1.1.471.6378, doi:10.1137/0221015.\nNaor, Moni; Stockmeyer, Larry (1995), \"What can be computed locally?\" (PDF), SIAM Journal on Computing, 24 (6): 1259–1277, CiteSeerX 10.1.1.29.669, doi:10.1137/S0097539793254571, archived (PDF) from the original on 2013-01-08.\nWeb sites\nGodfrey, Bill (2002). \"A primer on distributed computing\". Archived from the original on 2021-05-13. Retrieved 2021-05-13.\nPeter, Ian (2004). \"Ian Peter's History of the Internet\". Archived from the original on 2010-01-20. Retrieved 2009-08-04.\nFurther reading\nBooks\nAttiya, Hagit and Jennifer Welch (2004), Distributed Computing: Fundamentals, Simulations, and Advanced Topics, Wiley-Interscience ISBN 0-471-45324-2.\nChristian Cachin; Rachid Guerraoui; Luís Rodrigues (2011), Introduction to Reliable and Secure Distributed Programming (2. ed.), Springer, Bibcode:2011itra.book.....C, ISBN 978-3-642-15259-7\nCoulouris, George; et al. (2011), Distributed Systems: Concepts and Design (5th Edition), Addison-Wesley ISBN 0-132-14301-1.\nFaber, Jim (1998), Java Distributed Computing, O'Reilly, archived from the original on 2010-08-24, retrieved 2010-09-29: Java Distributed Computing by Jim Faber, 1998 Archived 2010-08-24 at the Wayback Machine\nGarg, Vijay K. (2002), Elements of Distributed Computing, Wiley-IEEE Press ISBN 0-471-03600-5.\nTel, Gerard (1994), Introduction to Distributed Algorithms, Cambridge University Press\nChandy, Mani; et al. (1988), Parallel Program Design, Addison-Wesley ISBN 0201058669\nDusseau, Remzi H.; Dusseau, Andrea (2016). Operating Systems: Three Easy Pieces, Chapter 48 Distributed Systems (PDF). Archived from the original (PDF) on 31 August 2021. Retrieved 8 October 2021.\nArticles\nKeidar, Idit; Rajsbaum, Sergio, eds. (2000–2009), \"Distributed computing column\", ACM SIGACT News, archived from the original on 2014-01-16, retrieved 2009-08-16.\nBirrell, A. D.; Levin, R.; Schroeder, M. D.; Needham, R. M. (April 1982). \"Grapevine: An exercise in distributed computing\" (PDF). Communications of the ACM. 25 (4): 260–274. doi:10.1145/358468.358487. S2CID 16066616. Archived (PDF) from the original on 2016-07-30.\nConference Papers\nRodriguez, Carlos; Villagra, Marcos; Baran, Benjamin (2007). \"Asynchronous team algorithms for Boolean Satisfiability\". 2007 2nd Bio-Inspired Models of Network, Information and Computing Systems. pp. 66–69. doi:10.1109/BIMNICS.2007.4610083. S2CID 15185219.\nExternal links\n\nWikiquote has quotations related to Distributed computing.\n Media related to Distributed computing at Wikimedia Commons\nvte\nParallel computing\nAuthority control databases Edit this at Wikidata\nCategory: Distributed computing\nThis page was last edited on 4 October 2024, at 08:15 (UTC).\nText is available under the Creative Commons Attribution-ShareAlike 4.0 License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\nPrivacy policyAbout WikipediaDisclaimersContact WikipediaCode of ConductDevelopersStatisticsCookie statementMobile view\nWikimedia FoundationPowered by MediaWiki\n\n\nMain menu\n\nWikipediaThe Free Encyclopedia\nSearch Wikipedia\nSearch\nDonate\nCreate account\nLog in\n\nPersonal tools\nContents hide\n(Top)\nIntroduction\nPatterns\nParallel and distributed computing\nHistory\nArchitectures\nApplications\nExamples\nReactive distributed systems\nTheoretical foundations\nToggle Theoretical foundations subsection\nModels\nAn example\nComplexity measures\nOther problems\nElection\nProperties of distributed systems\nSee also\nNotes\nReferences\nFurther reading\nExternal links\nDistributed computing\n\nArticle\nTalk\nRead\nEdit\nView history\n\nTools\nAppearance hide\nText\n\nSmall\n\nStandard\n\nLarge\nWidth\n\nStandard\n\nWide\nColor (beta)\n\nAutomatic\n\nLight\n\nDark\nFrom Wikipedia, the free encyclopedia\nNot to be confused with Decentralized computing.\nDistributed computing is a field of computer science that studies distributed systems, defined as computer systems whose inter-communicating components are located on different networked computers.[1][2]\n\nThe components of a distributed system communicate and coordinate their actions by passing messages to one another in order to achieve a common goal. Three significant challenges of distributed systems are: maintaining concurrency of components, overcoming the lack of a global clock, and managing the independent failure of components.[1] When a component of one system fails, the entire system does not fail.[3] Examples of distributed systems vary from SOA-based systems to microservices to massively multiplayer online games to peer-to-peer applications. Distributed systems cost significantly more than monolithic architectures, primarily due to increased needs for additional hardware, servers, gateways, firewalls, new subnets, proxies, and so on.[4] Also, distributed systems are prone to fallacies of distributed computing. On the other hand, a well designed distributed system is more scalable, more durable, more changeable and more fine-tuned than a monolithic application deployed on a single machine.[5] According to Marc Brooker: \"a system is scalable in the range where marginal cost of additional workload is nearly constant.\" Serverless technologies fit this definition but you need to consider total cost of ownership not just the infra cost. [6]\n\nA computer program that runs within a distributed system is called a distributed program,[7] and distributed programming is the process of writing such programs.[8] There are many different types of implementations for the message passing mechanism, including pure HTTP, RPC-like connectors and message queues.[9]\n\nDistributed computing also refers to the use of distributed systems to solve computational problems. In distributed computing, a problem is divided into many tasks, each of which is solved by one or more computers,[10] which communicate with each other via message passing.[11]\n\nIntroduction\nThe word distributed in terms such as \"distributed system\", \"distributed programming\", and \"distributed algorithm\" originally referred to computer networks where individual computers were physically distributed within some geographical area.[12] The terms are nowadays used in a much wider sense, even referring to autonomous processes that run on the same physical computer and interact with each other by message passing.[11]\n\nWhile there is no single definition of a distributed system,[13] the following defining properties are commonly used as:\n\nThere are several autonomous computational entities (computers or nodes), each of which has its own local memory.[14]\nThe entities communicate with each other by message passing.[15]\nA distributed system may have a common goal, such as solving a large computational problem;[16] the user then perceives the collection of autonomous processors as a unit. Alternatively, each computer may have its own user with individual needs, and the purpose of the distributed system is to coordinate the use of shared resources or provide communication services to the users.[17]\n\nOther typical properties of distributed systems include the following:\n\nThe system has to tolerate failures in individual computers.[18]\nThe structure of the system (network topology, network latency, number of computers) is not known in advance, the system may consist of different kinds of computers and network links, and the system may change during the execution of a distributed program.[19]\nEach computer has only a limited, incomplete view of the system. Each computer may know only one part of the input.[20]\nPatterns\nHere are common architectural patterns used for distributed computing:[21]\n\nSaga interaction pattern\nMicroservices\nEvent driven architecture\nParallel and distributed computing\n\n(a), (b): a distributed system.\n(c): a parallel system.\nDistributed systems are groups of networked computers which share a common goal for their work. The terms \"concurrent computing\", \"parallel computing\", and \"distributed computing\" have much overlap, and no clear distinction exists between them.[22] The same system may be characterized both as \"parallel\" and \"distributed\"; the processors in a typical distributed system run concurrently in parallel.[23] Parallel computing may be seen as a particularly tightly coupled form of distributed computing,[24] and distributed computing may be seen as a loosely coupled form of parallel computing.[13] Nevertheless, it is possible to roughly classify concurrent systems as \"parallel\" or \"distributed\" using the following criteria:\n\nIn parallel computing, all processors may have access to a shared memory to exchange information between processors.[25]\nIn distributed computing, each processor has its own private memory (distributed memory). Information is exchanged by passing messages between the processors.[26]\nThe figure on the right illustrates the difference between distributed and parallel systems. Figure (a) is a schematic view of a typical distributed system; the system is represented as a network topology in which each node is a computer and each line connecting the nodes is a communication link. Figure (b) shows the same distributed system in more detail: each computer has its own local memory, and information can be exchanged only by passing messages from one node to another by using the available communication links. Figure (c) shows a parallel system in which each processor has a direct access to a shared memory.\n\nThe situation is further complicated by the traditional uses of the terms parallel and distributed algorithm that do not quite match the above definitions of parallel and distributed systems (see below for more detailed discussion). Nevertheless, as a rule of thumb, high-performance parallel computation in a shared-memory multiprocessor uses parallel algorithms while the coordination of a large-scale distributed system uses distributed algorithms.[27]\n\nHistory\nThe use of concurrent processes which communicate through message-passing has its roots in operating system architectures studied in the 1960s.[28] The first widespread distributed systems were local-area networks such as Ethernet, which was invented in the 1970s.[29]\n\nARPANET, one of the predecessors of the Internet, was introduced in the late 1960s, and ARPANET e-mail was invented in the early 1970s. E-mail became the most successful application of ARPANET,[30] and it is probably the earliest example of a large-scale distributed application. In addition to ARPANET (and its successor, the global Internet), other early worldwide computer networks included Usenet and FidoNet from the 1980s, both of which were used to support distributed discussion systems.[31]\n\nThe study of distributed computing became its own branch of computer science in the late 1970s and early 1980s. The first conference in the field, Symposium on Principles of Distributed Computing (PODC), dates back to 1982, and its counterpart International Symposium on Distributed Computing (DISC) was first held in Ottawa in 1985 as the International Workshop on Distributed Algorithms on Graphs.[32]\n\nArchitectures\nVarious hardware and software architectures are used for distributed computing. At a lower level, it is necessary to interconnect multiple CPUs with some sort of network, regardless of whether that network is printed onto a circuit board or made up of loosely coupled devices and cables. At a higher level, it is necessary to interconnect processes running on those CPUs with some sort of communication system.[33]\n\nWhether these CPUs share resources or not determines a first distinction between three types of architecture:\n\nShared memory\nShared disk\nShared nothing.\nDistributed programming typically falls into one of several basic architectures: client–server, three-tier, n-tier, or peer-to-peer; or categories: loose coupling, or tight coupling.[34]\n\nClient–server: architectures where smart clients contact the server for data then format and display it to the users. Input at the client is committed back to the server when it represents a permanent change.\nThree-tier: architectures that move the client intelligence to a middle tier so that stateless clients can be used. This simplifies application deployment. Most web applications are three-tier.\nn-tier: architectures that refer typically to web applications which further forward their requests to other enterprise services. This type of application is the one most responsible for the success of application servers.\nPeer-to-peer: architectures where there are no special machines that provide a service or manage the network resources.[35]: 227  Instead all responsibilities are uniformly divided among all machines, known as peers. Peers can serve both as clients and as servers.[36] Examples of this architecture include BitTorrent and the bitcoin network.\nAnother basic aspect of distributed computing architecture is the method of communicating and coordinating work among concurrent processes. Through various message passing protocols, processes may communicate directly with one another, typically in a main/sub relationship. Alternatively, a \"database-centric\" architecture can enable distributed computing to be done without any form of direct inter-process communication, by utilizing a shared database.[37] Database-centric architecture in particular provides relational processing analytics in a schematic architecture allowing for live environment relay. This enables distributed computing functions both within and beyond the parameters of a networked database.[38]\n\nApplications\nReasons for using distributed systems and distributed computing may include:\n\nThe very nature of an application may require the use of a communication network that connects several computers: for example, data produced in one physical location and required in another location.\nThere are many cases in which the use of a single computer would be possible in principle, but the use of a distributed system is beneficial for practical reasons. For example:\nIt can allow for much larger storage and memory, faster compute, and higher bandwidth than a single machine.\nIt can provide more reliability than a non-distributed system, as there is no single point of failure. Moreover, a distributed system may be easier to expand and manage than a monolithic uniprocessor system.[39]\nIt may be more cost-efficient to obtain the desired level of performance by using a cluster of several low-end computers, in comparison with a single high-end computer.\nExamples\nExamples of distributed systems and applications of distributed computing include the following:[40]\n\ntelecommunications networks:\ntelephone networks and cellular networks,\ncomputer networks such as the Internet,\nwireless sensor networks,\nrouting algorithms;\nnetwork applications:\nWorld Wide Web and peer-to-peer networks,\nmassively multiplayer online games and virtual reality communities,\ndistributed databases and distributed database management systems,\nnetwork file systems,\ndistributed cache such as burst buffers,\ndistributed information processing systems such as banking systems and airline reservation systems;\nreal-time process control:\naircraft control systems,\nindustrial control systems;\nparallel computation:\nscientific computing, including cluster computing, grid computing, cloud computing,[41] and various volunteer computing projects,\ndistributed rendering in computer graphics.\npeer-to-peer\nReactive distributed systems\nAccording to Reactive Manifesto, reactive distributed systems are responsive, resilient, elastic and message-driven. Subsequently, Reactive systems are more flexible, loosely-coupled and scalable. To make your systems reactive, you are advised to implement Reactive Principles. Reactive Principles are a set of principles and patterns which help to make your cloud native application as well as edge native applications more reactive. [42]\n\nTheoretical foundations\nMain article: Distributed algorithm\nModels\nMany tasks that we would like to automate by using a computer are of question–answer type: we would like to ask a question and the computer should produce an answer. In theoretical computer science, such tasks are called computational problems. Formally, a computational problem consists of instances together with a solution for each instance. Instances are questions that we can ask, and solutions are desired answers to these questions.\n\nTheoretical computer science seeks to understand which computational problems can be solved by using a computer (computability theory) and how efficiently (computational complexity theory). Traditionally, it is said that a problem can be solved by using a computer if we can design an algorithm that produces a correct solution for any given instance. Such an algorithm can be implemented as a computer program that runs on a general-purpose computer: the program reads a problem instance from input, performs some computation, and produces the solution as output. Formalisms such as random-access machines or universal Turing machines can be used as abstract models of a sequential general-purpose computer executing such an algorithm.[43][44]\n\nThe field of concurrent and distributed computing studies similar questions in the case of either multiple computers, or a computer that executes a network of interacting processes: which computational problems can be solved in such a network and how efficiently? However, it is not at all obvious what is meant by \"solving a problem\" in the case of a concurrent or distributed system: for example, what is the task of the algorithm designer, and what is the concurrent or distributed equivalent of a sequential general-purpose computer?[citation needed]\n\nThe discussion below focuses on the case of multiple computers, although many of the issues are the same for concurrent processes running on a single computer.\n\nThree viewpoints are commonly used:\n\nParallel algorithms in shared-memory model\nAll processors have access to a shared memory. The algorithm designer chooses the program executed by each processor.\nOne theoretical model is the parallel random-access machines (PRAM) that are used.[45] However, the classical PRAM model assumes synchronous access to the shared memory.\nShared-memory programs can be extended to distributed systems if the underlying operating system encapsulates the communication between nodes and virtually unifies the memory across all individual systems.\nA model that is closer to the behavior of real-world multiprocessor machines and takes into account the use of machine instructions, such as Compare-and-swap (CAS), is that of asynchronous shared memory. There is a wide body of work on this model, a summary of which can be found in the literature.[46][47]\nParallel algorithms in message-passing model\nThe algorithm designer chooses the structure of the network, as well as the program executed by each computer.\nModels such as Boolean circuits and sorting networks are used.[48] A Boolean circuit can be seen as a computer network: each gate is a computer that runs an extremely simple computer program. Similarly, a sorting network can be seen as a computer network: each comparator is a computer.\nDistributed algorithms in message-passing model\nThe algorithm designer only chooses the computer program. All computers run the same program. The system must work correctly regardless of the structure of the network.\nA commonly used model is a graph with one finite-state machine per node.\nIn the case of distributed algorithms, computational problems are typically related to graphs. Often the graph that describes the structure of the computer network is the problem instance. This is illustrated in the following example.[49]\n\nAn example\nConsider the computational problem of finding a coloring of a given graph G. Different fields might take the following approaches:\n\nCentralized algorithms[49]\nThe graph G is encoded as a string, and the string is given as input to a computer. The computer program finds a coloring of the graph, encodes the coloring as a string, and outputs the result.\nParallel algorithms\nAgain, the graph G is encoded as a string. However, multiple computers can access the same string in parallel. Each computer might focus on one part of the graph and produce a coloring for that part.\nThe main focus is on high-performance computation that exploits the processing power of multiple computers in parallel.\nDistributed algorithms\nThe graph G is the structure of the computer network. There is one computer for each node of G and one communication link for each edge of G. Initially, each computer only knows about its immediate neighbors in the graph G; the computers must exchange messages with each other to discover more about the structure of G. Each computer must produce its own color as output.\nThe main focus is on coordinating the operation of an arbitrary distributed system.[49]\nWhile the field of parallel algorithms has a different focus than the field of distributed algorithms, there is much interaction between the two fields. For example, the Cole–Vishkin algorithm for graph coloring[50] was originally presented as a parallel algorithm, but the same technique can also be used directly as a distributed algorithm.\n\nMoreover, a parallel algorithm can be implemented either in a parallel system (using shared memory) or in a distributed system (using message passing).[51] The traditional boundary between parallel and distributed algorithms (choose a suitable network vs. run in any given network) does not lie in the same place as the boundary between parallel and distributed systems (shared memory vs. message passing).\n\nComplexity measures\nIn parallel algorithms, yet another resource in addition to time and space is the number of computers. Indeed, often there is a trade-off between the running time and the number of computers: the problem can be solved faster if there are more computers running in parallel (see speedup). If a decision problem can be solved in polylogarithmic time by using a polynomial number of processors, then the problem is said to be in the class NC.[52] The class NC can be defined equally well by using the PRAM formalism or Boolean circuits—PRAM machines can simulate Boolean circuits efficiently and vice versa.[53]\n\nIn the analysis of distributed algorithms, more attention is usually paid on communication operations than computational steps. Perhaps the simplest model of distributed computing is a synchronous system where all nodes operate in a lockstep fashion. This model is commonly known as the LOCAL model. During each communication round, all nodes in parallel (1) receive the latest messages from their neighbours, (2) perform arbitrary local computation, and (3) send new messages to their neighbors. In such systems, a central complexity measure is the number of synchronous communication rounds required to complete the task.[54]\n\nThis complexity measure is closely related to the diameter of the network. Let D be the diameter of the network. On the one hand, any computable problem can be solved trivially in a synchronous distributed system in approximately 2D communication rounds: simply gather all information in one location (D rounds), solve the problem, and inform each node about the solution (D rounds).\n\nOn the other hand, if the running time of the algorithm is much smaller than D communication rounds, then the nodes in the network must produce their output without having the possibility to obtain information about distant parts of the network. In other words, the nodes must make globally consistent decisions based on information that is available in their local D-neighbourhood. Many distributed algorithms are known with the running time much smaller than D rounds, and understanding which problems can be solved by such algorithms is one of the central research questions of the field.[55] Typically an algorithm which solves a problem in polylogarithmic time in the network size is considered efficient in this model.\n\nAnother commonly used measure is the total number of bits transmitted in the network (cf. communication complexity).[56] The features of this concept are typically captured with the CONGEST(B) model, which is similarly defined as the LOCAL model, but where single messages can only contain B bits.\n\nOther problems\nTraditional computational problems take the perspective that the user asks a question, a computer (or a distributed system) processes the question, then produces an answer and stops. However, there are also problems where the system is required not to stop, including the dining philosophers problem and other similar mutual exclusion problems. In these problems, the distributed system is supposed to continuously coordinate the use of shared resources so that no conflicts or deadlocks occur.\n\nThere are also fundamental challenges that are unique to distributed computing, for example those related to fault-tolerance. Examples of related problems include consensus problems,[57] Byzantine fault tolerance,[58] and self-stabilisation.[59]\n\nMuch research is also focused on understanding the asynchronous nature of distributed systems:\n\nSynchronizers can be used to run synchronous algorithms in asynchronous systems.[60]\nLogical clocks provide a causal happened-before ordering of events.[61]\nClock synchronization algorithms provide globally consistent physical time stamps.[62]\nNote that in distributed systems, latency should be measured through \"99th percentile\" because \"median\" and \"average\" can be misleading.[63]\n\nElection\nCoordinator election (or leader election) is the process of designating a single process as the organizer of some task distributed among several computers (nodes). Before the task is begun, all network nodes are either unaware which node will serve as the \"coordinator\" (or leader) of the task, or unable to communicate with the current coordinator. After a coordinator election algorithm has been run, however, each node throughout the network recognizes a particular, unique node as the task coordinator.[64]\n\nThe network nodes communicate among themselves in order to decide which of them will get into the \"coordinator\" state. For that, they need some method in order to break the symmetry among them. For example, if each node has unique and comparable identities, then the nodes can compare their identities, and decide that the node with the highest identity is the coordinator.[64]\n\nThe definition of this problem is often attributed to LeLann, who formalized it as a method to create a new token in a token ring network in which the token has been lost.[65]\n\nCoordinator election algorithms are designed to be economical in terms of total bytes transmitted, and time. The algorithm suggested by Gallager, Humblet, and Spira[66] for general undirected graphs has had a strong impact on the design of distributed algorithms in general, and won the Dijkstra Prize for an influential paper in distributed computing.\n\nMany other algorithms were suggested for different kinds of network graphs, such as undirected rings, unidirectional rings, complete graphs, grids, directed Euler graphs, and others. A general method that decouples the issue of the graph family from the design of the coordinator election algorithm was suggested by Korach, Kutten, and Moran.[67]\n\nIn order to perform coordination, distributed systems employ the concept of coordinators. The coordinator election problem is to choose a process from among a group of processes on different processors in a distributed system to act as the central coordinator. Several central coordinator election algorithms exist.[68]\n\nProperties of distributed systems\nSo far the focus has been on designing a distributed system that solves a given problem. A complementary research problem is studying the properties of a given distributed system.[69][70]\n\nThe halting problem is an analogous example from the field of centralised computation: we are given a computer program and the task is to decide whether it halts or runs forever. The halting problem is undecidable in the general case, and naturally understanding the behaviour of a computer network is at least as hard as understanding the behaviour of one computer.[71]\n\nHowever, there are many interesting special cases that are decidable. In particular, it is possible to reason about the behaviour of a network of finite-state machines. One example is telling whether a given network of interacting (asynchronous and non-deterministic) finite-state machines can reach a deadlock. This problem is PSPACE-complete,[72] i.e., it is decidable, but not likely that there is an efficient (centralised, parallel or distributed) algorithm that solves the problem in the case of large networks.\n\nSee also\nActor model\nAppScale\nBOINC\nCode mobility\nDataflow programming\nDecentralized computing\nDistributed algorithm\nDistributed algorithmic mechanism design\nDistributed cache\nDistributed GIS\nDistributed networking\nDistributed operating system\nEventual consistency\nEdsger W. Dijkstra Prize in Distributed Computing\nFederation (information technology)\nFlat neighborhood network\nFog computing\nFolding@home\nGrid computing\nInferno\nInternet GIS\nJungle computing\nLayered queueing network\nLibrary Oriented Architecture (LOA)\nList of distributed computing conferences\nList of volunteer computing projects\nModel checking\nOpenHarmony\nHarmonyOS\nParallel distributed processing\nParallel programming model\nPlan 9 from Bell Labs\nShared nothing architecture\nWeb GIS\nNotes\n Tanenbaum, Andrew S.; Steen, Maarten van (2002). Distributed systems: principles and paradigms. Upper Saddle River, NJ: Pearson Prentice Hall. ISBN 0-13-088893-1. Archived from the original on 2020-08-12. Retrieved 2020-08-28.\n \"Distributed Programs\". Texts in Computer Science. London: Springer London. 2010. pp. 373–406. doi:10.1007/978-1-84882-745-5_11. ISBN 978-1-84882-744-8. ISSN 1868-0941. Systems consist of a number of physically distributed components that work independently using their private storage, but also communicate from time to time by explicit message passing. Such systems are called distributed systems.\n Dusseau & Dusseau 2016, p. 1–2.\n Ford, Neal (March 3, 2020). Fundamentals of Software Architecture: An Engineering Approach (1st ed.). O'Reilly Media. pp. 146–147. ISBN 978-1492043454.\n Monolith to Microservices Evolutionary Patterns to Transform Your Monolith. O'Reilly Media. ISBN 9781492047810.\n Building Serverless Applications on Knative. O'Reilly Media. ISBN 9781098142049.\n \"Distributed Programs\". Texts in Computer Science. London: Springer London. 2010. pp. 373–406. doi:10.1007/978-1-84882-745-5_11. ISBN 978-1-84882-744-8. ISSN 1868-0941. Distributed programs are abstract descriptions of distributed systems. A distributed program consists of a collection of processes that work concurrently and communicate by explicit message passing. Each process can access a set of variables which are disjoint from the variables that can be changed by any other process.\n Andrews (2000). Dolev (2000). Ghosh (2007), p. 10.\n Magnoni, L. (2015). \"Modern Messaging for Distributed Sytems (sic)\". Journal of Physics: Conference Series. 608 (1): 012038. doi:10.1088/1742-6596/608/1/012038. ISSN 1742-6596.\n Godfrey (2002).\n Andrews (2000), p. 291–292. Dolev (2000), p. 5.\n Lynch (1996), p. 1.\n Ghosh (2007), p. 10.\n Andrews (2000), pp. 8–9, 291. Dolev (2000), p. 5. Ghosh (2007), p. 3. Lynch (1996), p. xix, 1. Peleg (2000), p. xv.\n Andrews (2000), p. 291. Ghosh (2007), p. 3. Peleg (2000), p. 4.\n Ghosh (2007), p. 3–4. Peleg (2000), p. 1.\n Ghosh (2007), p. 4. Peleg (2000), p. 2.\n Ghosh (2007), p. 4, 8. Lynch (1996), p. 2–3. Peleg (2000), p. 4.\n Lynch (1996), p. 2. Peleg (2000), p. 1.\n Ghosh (2007), p. 7. Lynch (1996), p. xix, 2. Peleg (2000), p. 4.\n Fundamentals of Software Architecture: An Engineering Approach. O'Reilly Media. 2020. ISBN 978-1492043454.\n Ghosh (2007), p. 10. Keidar (2008).\n Lynch (1996), p. xix, 1–2. Peleg (2000), p. 1.\n Peleg (2000), p. 1.\n Papadimitriou (1994), Chapter 15. Keidar (2008).\n See references in Introduction.\n Bentaleb, A.; Yifan, L.; Xin, J.; et al. (2016). \"Parallel and Distributed Algorithms\" (PDF). National University of Singapore. Archived (PDF) from the original on 2017-03-26. Retrieved 20 July 2018.\n Andrews (2000), p. 348.\n Andrews (2000), p. 32.\n Peter (2004), The history of email Archived 2009-04-15 at the Wayback Machine.\n Banks, M. (2012). On the Way to the Web: The Secret History of the Internet and its Founders. Apress. pp. 44–5. ISBN 9781430250746. Archived from the original on 2023-01-20. Retrieved 2018-07-20.\n Tel, G. (2000). Introduction to Distributed Algorithms. Cambridge University Press. pp. 35–36. ISBN 9780521794831. Archived from the original on 2023-01-20. Retrieved 2018-07-20.\n Ohlídal, M.; Jaroš, J.; Schwarz, J.; et al. (2006). \"Evolutionary Design of OAB and AAB Communication Schedules for Interconnection Networks\". In Rothlauf, F.; Branke, J.; Cagnoni, S. (eds.). Applications of Evolutionary Computing. Springer Science & Business Media. pp. 267–78. ISBN 9783540332374.\n \"Real Time And Distributed Computing Systems\" (PDF). ISSN 2278-0661. Archived from the original (PDF) on 2017-01-10. Retrieved 2017-01-09. {{cite journal}}: Cite journal requires |journal= (help)\n Vigna P, Casey MJ. The Age of Cryptocurrency: How Bitcoin and the Blockchain Are Challenging the Global Economic Order St. Martin's Press January 27, 2015 ISBN 9781250065636\n Quang Hieu Vu; Mihai Lupu; Beng Chin Ooi (2010). Peer-to-peer computing : principles and applications. Heidelberg: Springer. p. 16. ISBN 9783642035135. OCLC 663093862.\n Lind P, Alm M (2006), \"A database-centric virtual chemistry system\", J Chem Inf Model, 46 (3): 1034–9, doi:10.1021/ci050360b, PMID 16711722.\n Chiu, G (1990). \"A model for optimal database allocation in distributed computing systems\". Proceedings. IEEE INFOCOM'90: Ninth Annual Joint Conference of the IEEE Computer and Communications Societies.\n Elmasri & Navathe (2000), Section 24.1.2.\n Andrews (2000), p. 10–11. Ghosh (2007), p. 4–6. Lynch (1996), p. xix, 1. Peleg (2000), p. xv. Elmasri & Navathe (2000), Section 24.\n Haussmann, J. (2019). \"Cost-efficient parallel processing of irregularly structured problems in cloud computing environments\". Journal of Cluster Computing. 22 (3): 887–909. doi:10.1007/s10586-018-2879-3. S2CID 54447518.\n Reactive Application Development. Manning. 2018. ISBN 9781638355816.\n Toomarian, N.B.; Barhen, J.; Gulati, S. (1992). \"Neural Networks for Real-Time Robotic Applications\". In Fijany, A.; Bejczy, A. (eds.). Parallel Computation Systems For Robotics: Algorithms And Architectures. World Scientific. p. 214. ISBN 9789814506175. Archived from the original on 2020-08-01. Retrieved 2018-07-20.\n Savage, J.E. (1998). Models of Computation: Exploring the Power of Computing. Addison Wesley. p. 209. ISBN 9780201895391.\n Cormen, Leiserson & Rivest (1990), Section 30.\n Herlihy & Shavit (2008), Chapters 2–6.\n Lynch (1996)\n Cormen, Leiserson & Rivest (1990), Sections 28 and 29.\n TULSIRAMJI GAIKWAD-PATIL College of Engineering & Technology, Nagpur Department of Information Technology Introduction to Distributed Systems[1]\n Cole & Vishkin (1986). Cormen, Leiserson & Rivest (1990), Section 30.5.\n Andrews (2000), p. ix.\n Arora & Barak (2009), Section 6.7. Papadimitriou (1994), Section 15.3.\n Papadimitriou (1994), Section 15.2.\n Lynch (1996), p. 17–23.\n Peleg (2000), Sections 2.3 and 7. Linial (1992). Naor & Stockmeyer (1995).\n Schneider, J.; Wattenhofer, R. (2011). \"Trading Bit, Message, and Time Complexity of Distributed Algorithms\". In Peleg, D. (ed.). Distributed Computing. Springer Science & Business Media. pp. 51–65. ISBN 9783642240997. Archived from the original on 2020-08-01. Retrieved 2018-07-20.\n Lynch (1996), Sections 5–7. Ghosh (2007), Chapter 13.\n Lynch (1996), p. 99–102. Ghosh (2007), p. 192–193.\n Dolev (2000). Ghosh (2007), Chapter 17.\n Lynch (1996), Section 16. Peleg (2000), Section 6.\n Lynch (1996), Section 18. Ghosh (2007), Sections 6.2–6.3.\n Ghosh (2007), Section 6.4.\n Foundations of Data Intensive Applications Large Scale Data Analytics Under the Hood. 2021. ISBN 9781119713012.\n Haloi, S. (2015). Apache ZooKeeper Essentials. Packt Publishing Ltd. pp. 100–101. ISBN 9781784398323. Archived from the original on 2023-01-20. Retrieved 2018-07-20.\n LeLann, G. (1977). \"Distributed systems - toward a formal approach\". Information Processing. 77: 155·160 – via Elsevier.\n R. G. Gallager, P. A. Humblet, and P. M. Spira (January 1983). \"A Distributed Algorithm for Minimum-Weight Spanning Trees\" (PDF). ACM Transactions on Programming Languages and Systems. 5 (1): 66–77. doi:10.1145/357195.357200. S2CID 2758285. Archived (PDF) from the original on 2017-09-26.\n Korach, Ephraim; Kutten, Shay; Moran, Shlomo (1990). \"A Modular Technique for the Design of Efficient Distributed Leader Finding Algorithms\" (PDF). ACM Transactions on Programming Languages and Systems. 12 (1): 84–101. CiteSeerX 10.1.1.139.7342. doi:10.1145/77606.77610. S2CID 9175968. Archived (PDF) from the original on 2007-04-18.\n Hamilton, Howard. \"Distributed Algorithms\". Archived from the original on 2012-11-24. Retrieved 2013-03-03.\n \"Major unsolved problems in distributed systems?\". cstheory.stackexchange.com. Archived from the original on 20 January 2023. Retrieved 16 March 2018.\n \"How big data and distributed systems solve traditional scalability problems\". theserverside.com. Archived from the original on 17 March 2018. Retrieved 16 March 2018.\n Svozil, K. (2011). \"Indeterminism and Randomness Through Physics\". In Hector, Z. (ed.). Randomness Through Computation: Some Answers, More Questions. World Scientific. pp. 112–3. ISBN 9789814462631. Archived from the original on 2020-08-01. Retrieved 2018-07-20.\n Papadimitriou (1994), Section 19.3.\nReferences\nBooks\nAndrews, Gregory R. (2000), Foundations of Multithreaded, Parallel, and Distributed Programming, Addison–Wesley, ISBN 978-0-201-35752-3.\nArora, Sanjeev; Barak, Boaz (2009), Computational Complexity – A Modern Approach, Cambridge, ISBN 978-0-521-42426-4.\nCormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L. (1990), Introduction to Algorithms (1st ed.), MIT Press, ISBN 978-0-262-03141-7.\nDolev, Shlomi (2000), Self-Stabilization, MIT Press, ISBN 978-0-262-04178-2.\nElmasri, Ramez; Navathe, Shamkant B. (2000), Fundamentals of Database Systems (3rd ed.), Addison–Wesley, ISBN 978-0-201-54263-9.\nGhosh, Sukumar (2007), Distributed Systems – An Algorithmic Approach, Chapman & Hall/CRC, ISBN 978-1-58488-564-1.\nLynch, Nancy A. (1996), Distributed Algorithms, Morgan Kaufmann, ISBN 978-1-55860-348-6.\nHerlihy, Maurice P.; Shavit, Nir N. (2008), The Art of Multiprocessor Programming, Morgan Kaufmann, ISBN 978-0-12-370591-4.\nPapadimitriou, Christos H. (1994), Computational Complexity, Addison–Wesley, ISBN 978-0-201-53082-7.\nPeleg, David (2000), Distributed Computing: A Locality-Sensitive Approach, SIAM, ISBN 978-0-89871-464-7, archived from the original on 2009-08-06, retrieved 2009-07-16.\nArticles\nCole, Richard; Vishkin, Uzi (1986), \"Deterministic coin tossing with applications to optimal parallel list ranking\", Information and Control, 70 (1): 32–53, doi:10.1016/S0019-9958(86)80023-7.\nKeidar, Idit (2008), \"Distributed computing column 32 – The year in review\", ACM SIGACT News, 39 (4): 53–54, CiteSeerX 10.1.1.116.1285, doi:10.1145/1466390.1466402, S2CID 7607391, archived from the original on 2014-01-16, retrieved 2009-08-20.\nLinial, Nathan (1992), \"Locality in distributed graph algorithms\", SIAM Journal on Computing, 21 (1): 193–201, CiteSeerX 10.1.1.471.6378, doi:10.1137/0221015.\nNaor, Moni; Stockmeyer, Larry (1995), \"What can be computed locally?\" (PDF), SIAM Journal on Computing, 24 (6): 1259–1277, CiteSeerX 10.1.1.29.669, doi:10.1137/S0097539793254571, archived (PDF) from the original on 2013-01-08.\nWeb sites\nGodfrey, Bill (2002). \"A primer on distributed computing\". Archived from the original on 2021-05-13. Retrieved 2021-05-13.\nPeter, Ian (2004). \"Ian Peter's History of the Internet\". Archived from the original on 2010-01-20. Retrieved 2009-08-04.\nFurther reading\nBooks\nAttiya, Hagit and Jennifer Welch (2004), Distributed Computing: Fundamentals, Simulations, and Advanced Topics, Wiley-Interscience ISBN 0-471-45324-2.\nChristian Cachin; Rachid Guerraoui; Luís Rodrigues (2011), Introduction to Reliable and Secure Distributed Programming (2. ed.), Springer, Bibcode:2011itra.book.....C, ISBN 978-3-642-15259-7\nCoulouris, George; et al. (2011), Distributed Systems: Concepts and Design (5th Edition), Addison-Wesley ISBN 0-132-14301-1.\nFaber, Jim (1998), Java Distributed Computing, O'Reilly, archived from the original on 2010-08-24, retrieved 2010-09-29: Java Distributed Computing by Jim Faber, 1998 Archived 2010-08-24 at the Wayback Machine\nGarg, Vijay K. (2002), Elements of Distributed Computing, Wiley-IEEE Press ISBN 0-471-03600-5.\nTel, Gerard (1994), Introduction to Distributed Algorithms, Cambridge University Press\nChandy, Mani; et al. (1988), Parallel Program Design, Addison-Wesley ISBN 0201058669\nDusseau, Remzi H.; Dusseau, Andrea (2016). Operating Systems: Three Easy Pieces, Chapter 48 Distributed Systems (PDF). Archived from the original (PDF) on 31 August 2021. Retrieved 8 October 2021.\nArticles\nKeidar, Idit; Rajsbaum, Sergio, eds. (2000–2009), \"Distributed computing column\", ACM SIGACT News, archived from the original on 2014-01-16, retrieved 2009-08-16.\nBirrell, A. D.; Levin, R.; Schroeder, M. D.; Needham, R. M. (April 1982). \"Grapevine: An exercise in distributed computing\" (PDF). Communications of the ACM. 25 (4): 260–274. doi:10.1145/358468.358487. S2CID 16066616. Archived (PDF) from the original on 2016-07-30.\nConference Papers\nRodriguez, Carlos; Villagra, Marcos; Baran, Benjamin (2007). \"Asynchronous team algorithms for Boolean Satisfiability\". 2007 2nd Bio-Inspired Models of Network, Information and Computing Systems. pp. 66–69. doi:10.1109/BIMNICS.2007.4610083. S2CID 15185219.\nExternal links\n\nWikiquote has quotations related to Distributed computing.\n Media related to Distributed computing at Wikimedia Commons\nvte\nParallel computing\nAuthority control databases Edit this at Wikidata\nCategory: Distributed computing\nThis page was last edited on 4 October 2024, at 08:15 (UTC).\nText is available under the Creative Commons Attribution-ShareAlike 4.0 License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\nPrivacy policyAbout WikipediaDisclaimersContact WikipediaCode of ConductDevelopersStatisticsCookie statementMobile view\nWikimedia FoundationPowered by MediaWiki\n\n\nMain menu\n\nWikipediaThe Free Encyclopedia\nSearch Wikipedia\nSearch\nDonate\nCreate account\nLog in\n\nPersonal tools\nContents hide\n(Top)\nIntroduction\nPatterns\nParallel and distributed computing\nHistory\nArchitectures\nApplications\nExamples\nReactive distributed systems\nTheoretical foundations\nToggle Theoretical foundations subsection\nModels\nAn example\nComplexity measures\nOther problems\nElection\nProperties of distributed systems\nSee also\nNotes\nReferences\nFurther reading\nExternal links\nDistributed computing\n\nArticle\nTalk\nRead\nEdit\nView history\n\nTools\nAppearance hide\nText\n\nSmall\n\nStandard\n\nLarge\nWidth\n\nStandard\n\nWide\nColor (beta)\n\nAutomatic\n\nLight\n\nDark\nFrom Wikipedia, the free encyclopedia\nNot to be confused with Decentralized computing.\nDistributed computing is a field of computer science that studies distributed systems, defined as computer systems whose inter-communicating components are located on different networked computers.[1][2]\n\nThe components of a distributed system communicate and coordinate their actions by passing messages to one another in order to achieve a common goal. Three significant challenges of distributed systems are: maintaining concurrency of components, overcoming the lack of a global clock, and managing the independent failure of components.[1] When a component of one system fails, the entire system does not fail.[3] Examples of distributed systems vary from SOA-based systems to microservices to massively multiplayer online games to peer-to-peer applications. Distributed systems cost significantly more than monolithic architectures, primarily due to increased needs for additional hardware, servers, gateways, firewalls, new subnets, proxies, and so on.[4] Also, distributed systems are prone to fallacies of distributed computing. On the other hand, a well designed distributed system is more scalable, more durable, more changeable and more fine-tuned than a monolithic application deployed on a single machine.[5] According to Marc Brooker: \"a system is scalable in the range where marginal cost of additional workload is nearly constant.\" Serverless technologies fit this definition but you need to consider total cost of ownership not just the infra cost. [6]\n\nA computer program that runs within a distributed system is called a distributed program,[7] and distributed programming is the process of writing such programs.[8] There are many different types of implementations for the message passing mechanism, including pure HTTP, RPC-like connectors and message queues.[9]\n\nDistributed computing also refers to the use of distributed systems to solve computational problems. In distributed computing, a problem is divided into many tasks, each of which is solved by one or more computers,[10] which communicate with each other via message passing.[11]\n\nIntroduction\nThe word distributed in terms such as \"distributed system\", \"distributed programming\", and \"distributed algorithm\" originally referred to computer networks where individual computers were physically distributed within some geographical area.[12] The terms are nowadays used in a much wider sense, even referring to autonomous processes that run on the same physical computer and interact with each other by message passing.[11]\n\nWhile there is no single definition of a distributed system,[13] the following defining properties are commonly used as:\n\nThere are several autonomous computational entities (computers or nodes), each of which has its own local memory.[14]\nThe entities communicate with each other by message passing.[15]\nA distributed system may have a common goal, such as solving a large computational problem;[16] the user then perceives the collection of autonomous processors as a unit. Alternatively, each computer may have its own user with individual needs, and the purpose of the distributed system is to coordinate the use of shared resources or provide communication services to the users.[17]\n\nOther typical properties of distributed systems include the following:\n\nThe system has to tolerate failures in individual computers.[18]\nThe structure of the system (network topology, network latency, number of computers) is not known in advance, the system may consist of different kinds of computers and network links, and the system may change during the execution of a distributed program.[19]\nEach computer has only a limited, incomplete view of the system. Each computer may know only one part of the input.[20]\nPatterns\nHere are common architectural patterns used for distributed computing:[21]\n\nSaga interaction pattern\nMicroservices\nEvent driven architecture\nParallel and distributed computing\n\n(a), (b): a distributed system.\n(c): a parallel system.\nDistributed systems are groups of networked computers which share a common goal for their work. The terms \"concurrent computing\", \"parallel computing\", and \"distributed computing\" have much overlap, and no clear distinction exists between them.[22] The same system may be characterized both as \"parallel\" and \"distributed\"; the processors in a typical distributed system run concurrently in parallel.[23] Parallel computing may be seen as a particularly tightly coupled form of distributed computing,[24] and distributed computing may be seen as a loosely coupled form of parallel computing.[13] Nevertheless, it is possible to roughly classify concurrent systems as \"parallel\" or \"distributed\" using the following criteria:\n\nIn parallel computing, all processors may have access to a shared memory to exchange information between processors.[25]\nIn distributed computing, each processor has its own private memory (distributed memory). Information is exchanged by passing messages between the processors.[26]\nThe figure on the right illustrates the difference between distributed and parallel systems. Figure (a) is a schematic view of a typical distributed system; the system is represented as a network topology in which each node is a computer and each line connecting the nodes is a communication link. Figure (b) shows the same distributed system in more detail: each computer has its own local memory, and information can be exchanged only by passing messages from one node to another by using the available communication links. Figure (c) shows a parallel system in which each processor has a direct access to a shared memory.\n\nThe situation is further complicated by the traditional uses of the terms parallel and distributed algorithm that do not quite match the above definitions of parallel and distributed systems (see below for more detailed discussion). Nevertheless, as a rule of thumb, high-performance parallel computation in a shared-memory multiprocessor uses parallel algorithms while the coordination of a large-scale distributed system uses distributed algorithms.[27]\n\nHistory\nThe use of concurrent processes which communicate through message-passing has its roots in operating system architectures studied in the 1960s.[28] The first widespread distributed systems were local-area networks such as Ethernet, which was invented in the 1970s.[29]\n\nARPANET, one of the predecessors of the Internet, was introduced in the late 1960s, and ARPANET e-mail was invented in the early 1970s. E-mail became the most successful application of ARPANET,[30] and it is probably the earliest example of a large-scale distributed application. In addition to ARPANET (and its successor, the global Internet), other early worldwide computer networks included Usenet and FidoNet from the 1980s, both of which were used to support distributed discussion systems.[31]\n\nThe study of distributed computing became its own branch of computer science in the late 1970s and early 1980s. The first conference in the field, Symposium on Principles of Distributed Computing (PODC), dates back to 1982, and its counterpart International Symposium on Distributed Computing (DISC) was first held in Ottawa in 1985 as the International Workshop on Distributed Algorithms on Graphs.[32]\n\nArchitectures\nVarious hardware and software architectures are used for distributed computing. At a lower level, it is necessary to interconnect multiple CPUs with some sort of network, regardless of whether that network is printed onto a circuit board or made up of loosely coupled devices and cables. At a higher level, it is necessary to interconnect processes running on those CPUs with some sort of communication system.[33]\n\nWhether these CPUs share resources or not determines a first distinction between three types of architecture:\n\nShared memory\nShared disk\nShared nothing.\nDistributed programming typically falls into one of several basic architectures: client–server, three-tier, n-tier, or peer-to-peer; or categories: loose coupling, or tight coupling.[34]\n\nClient–server: architectures where smart clients contact the server for data then format and display it to the users. Input at the client is committed back to the server when it represents a permanent change.\nThree-tier: architectures that move the client intelligence to a middle tier so that stateless clients can be used. This simplifies application deployment. Most web applications are three-tier.\nn-tier: architectures that refer typically to web applications which further forward their requests to other enterprise services. This type of application is the one most responsible for the success of application servers.\nPeer-to-peer: architectures where there are no special machines that provide a service or manage the network resources.[35]: 227  Instead all responsibilities are uniformly divided among all machines, known as peers. Peers can serve both as clients and as servers.[36] Examples of this architecture include BitTorrent and the bitcoin network.\nAnother basic aspect of distributed computing architecture is the method of communicating and coordinating work among concurrent processes. Through various message passing protocols, processes may communicate directly with one another, typically in a main/sub relationship. Alternatively, a \"database-centric\" architecture can enable distributed computing to be done without any form of direct inter-process communication, by utilizing a shared database.[37] Database-centric architecture in particular provides relational processing analytics in a schematic architecture allowing for live environment relay. This enables distributed computing functions both within and beyond the parameters of a networked database.[38]\n\nApplications\nReasons for using distributed systems and distributed computing may include:\n\nThe very nature of an application may require the use of a communication network that connects several computers: for example, data produced in one physical location and required in another location.\nThere are many cases in which the use of a single computer would be possible in principle, but the use of a distributed system is beneficial for practical reasons. For example:\nIt can allow for much larger storage and memory, faster compute, and higher bandwidth than a single machine.\nIt can provide more reliability than a non-distributed system, as there is no single point of failure. Moreover, a distributed system may be easier to expand and manage than a monolithic uniprocessor system.[39]\nIt may be more cost-efficient to obtain the desired level of performance by using a cluster of several low-end computers, in comparison with a single high-end computer.\nExamples\nExamples of distributed systems and applications of distributed computing include the following:[40]\n\ntelecommunications networks:\ntelephone networks and cellular networks,\ncomputer networks such as the Internet,\nwireless sensor networks,\nrouting algorithms;\nnetwork applications:\nWorld Wide Web and peer-to-peer networks,\nmassively multiplayer online games and virtual reality communities,\ndistributed databases and distributed database management systems,\nnetwork file systems,\ndistributed cache such as burst buffers,\ndistributed information processing systems such as banking systems and airline reservation systems;\nreal-time process control:\naircraft control systems,\nindustrial control systems;\nparallel computation:\nscientific computing, including cluster computing, grid computing, cloud computing,[41] and various volunteer computing projects,\ndistributed rendering in computer graphics.\npeer-to-peer\nReactive distributed systems\nAccording to Reactive Manifesto, reactive distributed systems are responsive, resilient, elastic and message-driven. Subsequently, Reactive systems are more flexible, loosely-coupled and scalable. To make your systems reactive, you are advised to implement Reactive Principles. Reactive Principles are a set of principles and patterns which help to make your cloud native application as well as edge native applications more reactive. [42]\n\nTheoretical foundations\nMain article: Distributed algorithm\nModels\nMany tasks that we would like to automate by using a computer are of question–answer type: we would like to ask a question and the computer should produce an answer. In theoretical computer science, such tasks are called computational problems. Formally, a computational problem consists of instances together with a solution for each instance. Instances are questions that we can ask, and solutions are desired answers to these questions.\n\nTheoretical computer science seeks to understand which computational problems can be solved by using a computer (computability theory) and how efficiently (computational complexity theory). Traditionally, it is said that a problem can be solved by using a computer if we can design an algorithm that produces a correct solution for any given instance. Such an algorithm can be implemented as a computer program that runs on a general-purpose computer: the program reads a problem instance from input, performs some computation, and produces the solution as output. Formalisms such as random-access machines or universal Turing machines can be used as abstract models of a sequential general-purpose computer executing such an algorithm.[43][44]\n\nThe field of concurrent and distributed computing studies similar questions in the case of either multiple computers, or a computer that executes a network of interacting processes: which computational problems can be solved in such a network and how efficiently? However, it is not at all obvious what is meant by \"solving a problem\" in the case of a concurrent or distributed system: for example, what is the task of the algorithm designer, and what is the concurrent or distributed equivalent of a sequential general-purpose computer?[citation needed]\n\nThe discussion below focuses on the case of multiple computers, although many of the issues are the same for concurrent processes running on a single computer.\n\nThree viewpoints are commonly used:\n\nParallel algorithms in shared-memory model\nAll processors have access to a shared memory. The algorithm designer chooses the program executed by each processor.\nOne theoretical model is the parallel random-access machines (PRAM) that are used.[45] However, the classical PRAM model assumes synchronous access to the shared memory.\nShared-memory programs can be extended to distributed systems if the underlying operating system encapsulates the communication between nodes and virtually unifies the memory across all individual systems.\nA model that is closer to the behavior of real-world multiprocessor machines and takes into account the use of machine instructions, such as Compare-and-swap (CAS), is that of asynchronous shared memory. There is a wide body of work on this model, a summary of which can be found in the literature.[46][47]\nParallel algorithms in message-passing model\nThe algorithm designer chooses the structure of the network, as well as the program executed by each computer.\nModels such as Boolean circuits and sorting networks are used.[48] A Boolean circuit can be seen as a computer network: each gate is a computer that runs an extremely simple computer program. Similarly, a sorting network can be seen as a computer network: each comparator is a computer.\nDistributed algorithms in message-passing model\nThe algorithm designer only chooses the computer program. All computers run the same program. The system must work correctly regardless of the structure of the network.\nA commonly used model is a graph with one finite-state machine per node.\nIn the case of distributed algorithms, computational problems are typically related to graphs. Often the graph that describes the structure of the computer network is the problem instance. This is illustrated in the following example.[49]\n\nAn example\nConsider the computational problem of finding a coloring of a given graph G. Different fields might take the following approaches:\n\nCentralized algorithms[49]\nThe graph G is encoded as a string, and the string is given as input to a computer. The computer program finds a coloring of the graph, encodes the coloring as a string, and outputs the result.\nParallel algorithms\nAgain, the graph G is encoded as a string. However, multiple computers can access the same string in parallel. Each computer might focus on one part of the graph and produce a coloring for that part.\nThe main focus is on high-performance computation that exploits the processing power of multiple computers in parallel.\nDistributed algorithms\nThe graph G is the structure of the computer network. There is one computer for each node of G and one communication link for each edge of G. Initially, each computer only knows about its immediate neighbors in the graph G; the computers must exchange messages with each other to discover more about the structure of G. Each computer must produce its own color as output.\nThe main focus is on coordinating the operation of an arbitrary distributed system.[49]\nWhile the field of parallel algorithms has a different focus than the field of distributed algorithms, there is much interaction between the two fields. For example, the Cole–Vishkin algorithm for graph coloring[50] was originally presented as a parallel algorithm, but the same technique can also be used directly as a distributed algorithm.\n\nMoreover, a parallel algorithm can be implemented either in a parallel system (using shared memory) or in a distributed system (using message passing).[51] The traditional boundary between parallel and distributed algorithms (choose a suitable network vs. run in any given network) does not lie in the same place as the boundary between parallel and distributed systems (shared memory vs. message passing).\n\nComplexity measures\nIn parallel algorithms, yet another resource in addition to time and space is the number of computers. Indeed, often there is a trade-off between the running time and the number of computers: the problem can be solved faster if there are more computers running in parallel (see speedup). If a decision problem can be solved in polylogarithmic time by using a polynomial number of processors, then the problem is said to be in the class NC.[52] The class NC can be defined equally well by using the PRAM formalism or Boolean circuits—PRAM machines can simulate Boolean circuits efficiently and vice versa.[53]\n\nIn the analysis of distributed algorithms, more attention is usually paid on communication operations than computational steps. Perhaps the simplest model of distributed computing is a synchronous system where all nodes operate in a lockstep fashion. This model is commonly known as the LOCAL model. During each communication round, all nodes in parallel (1) receive the latest messages from their neighbours, (2) perform arbitrary local computation, and (3) send new messages to their neighbors. In such systems, a central complexity measure is the number of synchronous communication rounds required to complete the task.[54]\n\nThis complexity measure is closely related to the diameter of the network. Let D be the diameter of the network. On the one hand, any computable problem can be solved trivially in a synchronous distributed system in approximately 2D communication rounds: simply gather all information in one location (D rounds), solve the problem, and inform each node about the solution (D rounds).\n\nOn the other hand, if the running time of the algorithm is much smaller than D communication rounds, then the nodes in the network must produce their output without having the possibility to obtain information about distant parts of the network. In other words, the nodes must make globally consistent decisions based on information that is available in their local D-neighbourhood. Many distributed algorithms are known with the running time much smaller than D rounds, and understanding which problems can be solved by such algorithms is one of the central research questions of the field.[55] Typically an algorithm which solves a problem in polylogarithmic time in the network size is considered efficient in this model.\n\nAnother commonly used measure is the total number of bits transmitted in the network (cf. communication complexity).[56] The features of this concept are typically captured with the CONGEST(B) model, which is similarly defined as the LOCAL model, but where single messages can only contain B bits.\n\nOther problems\nTraditional computational problems take the perspective that the user asks a question, a computer (or a distributed system) processes the question, then produces an answer and stops. However, there are also problems where the system is required not to stop, including the dining philosophers problem and other similar mutual exclusion problems. In these problems, the distributed system is supposed to continuously coordinate the use of shared resources so that no conflicts or deadlocks occur.\n\nThere are also fundamental challenges that are unique to distributed computing, for example those related to fault-tolerance. Examples of related problems include consensus problems,[57] Byzantine fault tolerance,[58] and self-stabilisation.[59]\n\nMuch research is also focused on understanding the asynchronous nature of distributed systems:\n\nSynchronizers can be used to run synchronous algorithms in asynchronous systems.[60]\nLogical clocks provide a causal happened-before ordering of events.[61]\nClock synchronization algorithms provide globally consistent physical time stamps.[62]\nNote that in distributed systems, latency should be measured through \"99th percentile\" because \"median\" and \"average\" can be misleading.[63]\n\nElection\nCoordinator election (or leader election) is the process of designating a single process as the organizer of some task distributed among several computers (nodes). Before the task is begun, all network nodes are either unaware which node will serve as the \"coordinator\" (or leader) of the task, or unable to communicate with the current coordinator. After a coordinator election algorithm has been run, however, each node throughout the network recognizes a particular, unique node as the task coordinator.[64]\n\nThe network nodes communicate among themselves in order to decide which of them will get into the \"coordinator\" state. For that, they need some method in order to break the symmetry among them. For example, if each node has unique and comparable identities, then the nodes can compare their identities, and decide that the node with the highest identity is the coordinator.[64]\n\nThe definition of this problem is often attributed to LeLann, who formalized it as a method to create a new token in a token ring network in which the token has been lost.[65]\n\nCoordinator election algorithms are designed to be economical in terms of total bytes transmitted, and time. The algorithm suggested by Gallager, Humblet, and Spira[66] for general undirected graphs has had a strong impact on the design of distributed algorithms in general, and won the Dijkstra Prize for an influential paper in distributed computing.\n\nMany other algorithms were suggested for different kinds of network graphs, such as undirected rings, unidirectional rings, complete graphs, grids, directed Euler graphs, and others. A general method that decouples the issue of the graph family from the design of the coordinator election algorithm was suggested by Korach, Kutten, and Moran.[67]\n\nIn order to perform coordination, distributed systems employ the concept of coordinators. The coordinator election problem is to choose a process from among a group of processes on different processors in a distributed system to act as the central coordinator. Several central coordinator election algorithms exist.[68]\n\nProperties of distributed systems\nSo far the focus has been on designing a distributed system that solves a given problem. A complementary research problem is studying the properties of a given distributed system.[69][70]\n\nThe halting problem is an analogous example from the field of centralised computation: we are given a computer program and the task is to decide whether it halts or runs forever. The halting problem is undecidable in the general case, and naturally understanding the behaviour of a computer network is at least as hard as understanding the behaviour of one computer.[71]\n\nHowever, there are many interesting special cases that are decidable. In particular, it is possible to reason about the behaviour of a network of finite-state machines. One example is telling whether a given network of interacting (asynchronous and non-deterministic) finite-state machines can reach a deadlock. This problem is PSPACE-complete,[72] i.e., it is decidable, but not likely that there is an efficient (centralised, parallel or distributed) algorithm that solves the problem in the case of large networks.\n\nSee also\nActor model\nAppScale\nBOINC\nCode mobility\nDataflow programming\nDecentralized computing\nDistributed algorithm\nDistributed algorithmic mechanism design\nDistributed cache\nDistributed GIS\nDistributed networking\nDistributed operating system\nEventual consistency\nEdsger W. Dijkstra Prize in Distributed Computing\nFederation (information technology)\nFlat neighborhood network\nFog computing\nFolding@home\nGrid computing\nInferno\nInternet GIS\nJungle computing\nLayered queueing network\nLibrary Oriented Architecture (LOA)\nList of distributed computing conferences\nList of volunteer computing projects\nModel checking\nOpenHarmony\nHarmonyOS\nParallel distributed processing\nParallel programming model\nPlan 9 from Bell Labs\nShared nothing architecture\nWeb GIS\nNotes\n Tanenbaum, Andrew S.; Steen, Maarten van (2002). Distributed systems: principles and paradigms. Upper Saddle River, NJ: Pearson Prentice Hall. ISBN 0-13-088893-1. Archived from the original on 2020-08-12. Retrieved 2020-08-28.\n \"Distributed Programs\". Texts in Computer Science. London: Springer London. 2010. pp. 373–406. doi:10.1007/978-1-84882-745-5_11. ISBN 978-1-84882-744-8. ISSN 1868-0941. Systems consist of a number of physically distributed components that work independently using their private storage, but also communicate from time to time by explicit message passing. Such systems are called distributed systems.\n Dusseau & Dusseau 2016, p. 1–2.\n Ford, Neal (March 3, 2020). Fundamentals of Software Architecture: An Engineering Approach (1st ed.). O'Reilly Media. pp. 146–147. ISBN 978-1492043454.\n Monolith to Microservices Evolutionary Patterns to Transform Your Monolith. O'Reilly Media. ISBN 9781492047810.\n Building Serverless Applications on Knative. O'Reilly Media. ISBN 9781098142049.\n \"Distributed Programs\". Texts in Computer Science. London: Springer London. 2010. pp. 373–406. doi:10.1007/978-1-84882-745-5_11. ISBN 978-1-84882-744-8. ISSN 1868-0941. Distributed programs are abstract descriptions of distributed systems. A distributed program consists of a collection of processes that work concurrently and communicate by explicit message passing. Each process can access a set of variables which are disjoint from the variables that can be changed by any other process.\n Andrews (2000). Dolev (2000). Ghosh (2007), p. 10.\n Magnoni, L. (2015). \"Modern Messaging for Distributed Sytems (sic)\". Journal of Physics: Conference Series. 608 (1): 012038. doi:10.1088/1742-6596/608/1/012038. ISSN 1742-6596.\n Godfrey (2002).\n Andrews (2000), p. 291–292. Dolev (2000), p. 5.\n Lynch (1996), p. 1.\n Ghosh (2007), p. 10.\n Andrews (2000), pp. 8–9, 291. Dolev (2000), p. 5. Ghosh (2007), p. 3. Lynch (1996), p. xix, 1. Peleg (2000), p. xv.\n Andrews (2000), p. 291. Ghosh (2007), p. 3. Peleg (2000), p. 4.\n Ghosh (2007), p. 3–4. Peleg (2000), p. 1.\n Ghosh (2007), p. 4. Peleg (2000), p. 2.\n Ghosh (2007), p. 4, 8. Lynch (1996), p. 2–3. Peleg (2000), p. 4.\n Lynch (1996), p. 2. Peleg (2000), p. 1.\n Ghosh (2007), p. 7. Lynch (1996), p. xix, 2. Peleg (2000), p. 4.\n Fundamentals of Software Architecture: An Engineering Approach. O'Reilly Media. 2020. ISBN 978-1492043454.\n Ghosh (2007), p. 10. Keidar (2008).\n Lynch (1996), p. xix, 1–2. Peleg (2000), p. 1.\n Peleg (2000), p. 1.\n Papadimitriou (1994), Chapter 15. Keidar (2008).\n See references in Introduction.\n Bentaleb, A.; Yifan, L.; Xin, J.; et al. (2016). \"Parallel and Distributed Algorithms\" (PDF). National University of Singapore. Archived (PDF) from the original on 2017-03-26. Retrieved 20 July 2018.\n Andrews (2000), p. 348.\n Andrews (2000), p. 32.\n Peter (2004), The history of email Archived 2009-04-15 at the Wayback Machine.\n Banks, M. (2012). On the Way to the Web: The Secret History of the Internet and its Founders. Apress. pp. 44–5. ISBN 9781430250746. Archived from the original on 2023-01-20. Retrieved 2018-07-20.\n Tel, G. (2000). Introduction to Distributed Algorithms. Cambridge University Press. pp. 35–36. ISBN 9780521794831. Archived from the original on 2023-01-20. Retrieved 2018-07-20.\n Ohlídal, M.; Jaroš, J.; Schwarz, J.; et al. (2006). \"Evolutionary Design of OAB and AAB Communication Schedules for Interconnection Networks\". In Rothlauf, F.; Branke, J.; Cagnoni, S. (eds.). Applications of Evolutionary Computing. Springer Science & Business Media. pp. 267–78. ISBN 9783540332374.\n \"Real Time And Distributed Computing Systems\" (PDF). ISSN 2278-0661. Archived from the original (PDF) on 2017-01-10. Retrieved 2017-01-09. {{cite journal}}: Cite journal requires |journal= (help)\n Vigna P, Casey MJ. The Age of Cryptocurrency: How Bitcoin and the Blockchain Are Challenging the Global Economic Order St. Martin's Press January 27, 2015 ISBN 9781250065636\n Quang Hieu Vu; Mihai Lupu; Beng Chin Ooi (2010). Peer-to-peer computing : principles and applications. Heidelberg: Springer. p. 16. ISBN 9783642035135. OCLC 663093862.\n Lind P, Alm M (2006), \"A database-centric virtual chemistry system\", J Chem Inf Model, 46 (3): 1034–9, doi:10.1021/ci050360b, PMID 16711722.\n Chiu, G (1990). \"A model for optimal database allocation in distributed computing systems\". Proceedings. IEEE INFOCOM'90: Ninth Annual Joint Conference of the IEEE Computer and Communications Societies.\n Elmasri & Navathe (2000), Section 24.1.2.\n Andrews (2000), p. 10–11. Ghosh (2007), p. 4–6. Lynch (1996), p. xix, 1. Peleg (2000), p. xv. Elmasri & Navathe (2000), Section 24.\n Haussmann, J. (2019). \"Cost-efficient parallel processing of irregularly structured problems in cloud computing environments\". Journal of Cluster Computing. 22 (3): 887–909. doi:10.1007/s10586-018-2879-3. S2CID 54447518.\n Reactive Application Development. Manning. 2018. ISBN 9781638355816.\n Toomarian, N.B.; Barhen, J.; Gulati, S. (1992). \"Neural Networks for Real-Time Robotic Applications\". In Fijany, A.; Bejczy, A. (eds.). Parallel Computation Systems For Robotics: Algorithms And Architectures. World Scientific. p. 214. ISBN 9789814506175. Archived from the original on 2020-08-01. Retrieved 2018-07-20.\n Savage, J.E. (1998). Models of Computation: Exploring the Power of Computing. Addison Wesley. p. 209. ISBN 9780201895391.\n Cormen, Leiserson & Rivest (1990), Section 30.\n Herlihy & Shavit (2008), Chapters 2–6.\n Lynch (1996)\n Cormen, Leiserson & Rivest (1990), Sections 28 and 29.\n TULSIRAMJI GAIKWAD-PATIL College of Engineering & Technology, Nagpur Department of Information Technology Introduction to Distributed Systems[1]\n Cole & Vishkin (1986). Cormen, Leiserson & Rivest (1990), Section 30.5.\n Andrews (2000), p. ix.\n Arora & Barak (2009), Section 6.7. Papadimitriou (1994), Section 15.3.\n Papadimitriou (1994), Section 15.2.\n Lynch (1996), p. 17–23.\n Peleg (2000), Sections 2.3 and 7. Linial (1992). Naor & Stockmeyer (1995).\n Schneider, J.; Wattenhofer, R. (2011). \"Trading Bit, Message, and Time Complexity of Distributed Algorithms\". In Peleg, D. (ed.). Distributed Computing. Springer Science & Business Media. pp. 51–65. ISBN 9783642240997. Archived from the original on 2020-08-01. Retrieved 2018-07-20.\n Lynch (1996), Sections 5–7. Ghosh (2007), Chapter 13.\n Lynch (1996), p. 99–102. Ghosh (2007), p. 192–193.\n Dolev (2000). Ghosh (2007), Chapter 17.\n Lynch (1996), Section 16. Peleg (2000), Section 6.\n Lynch (1996), Section 18. Ghosh (2007), Sections 6.2–6.3.\n Ghosh (2007), Section 6.4.\n Foundations of Data Intensive Applications Large Scale Data Analytics Under the Hood. 2021. ISBN 9781119713012.\n Haloi, S. (2015). Apache ZooKeeper Essentials. Packt Publishing Ltd. pp. 100–101. ISBN 9781784398323. Archived from the original on 2023-01-20. Retrieved 2018-07-20.\n LeLann, G. (1977). \"Distributed systems - toward a formal approach\". Information Processing. 77: 155·160 – via Elsevier.\n R. G. Gallager, P. A. Humblet, and P. M. Spira (January 1983). \"A Distributed Algorithm for Minimum-Weight Spanning Trees\" (PDF). ACM Transactions on Programming Languages and Systems. 5 (1): 66–77. doi:10.1145/357195.357200. S2CID 2758285. Archived (PDF) from the original on 2017-09-26.\n Korach, Ephraim; Kutten, Shay; Moran, Shlomo (1990). \"A Modular Technique for the Design of Efficient Distributed Leader Finding Algorithms\" (PDF). ACM Transactions on Programming Languages and Systems. 12 (1): 84–101. CiteSeerX 10.1.1.139.7342. doi:10.1145/77606.77610. S2CID 9175968. Archived (PDF) from the original on 2007-04-18.\n Hamilton, Howard. \"Distributed Algorithms\". Archived from the original on 2012-11-24. Retrieved 2013-03-03.\n \"Major unsolved problems in distributed systems?\". cstheory.stackexchange.com. Archived from the original on 20 January 2023. Retrieved 16 March 2018.\n \"How big data and distributed systems solve traditional scalability problems\". theserverside.com. Archived from the original on 17 March 2018. Retrieved 16 March 2018.\n Svozil, K. (2011). \"Indeterminism and Randomness Through Physics\". In Hector, Z. (ed.). Randomness Through Computation: Some Answers, More Questions. World Scientific. pp. 112–3. ISBN 9789814462631. Archived from the original on 2020-08-01. Retrieved 2018-07-20.\n Papadimitriou (1994), Section 19.3.\nReferences\nBooks\nAndrews, Gregory R. (2000), Foundations of Multithreaded, Parallel, and Distributed Programming, Addison–Wesley, ISBN 978-0-201-35752-3.\nArora, Sanjeev; Barak, Boaz (2009), Computational Complexity – A Modern Approach, Cambridge, ISBN 978-0-521-42426-4.\nCormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L. (1990), Introduction to Algorithms (1st ed.), MIT Press, ISBN 978-0-262-03141-7.\nDolev, Shlomi (2000), Self-Stabilization, MIT Press, ISBN 978-0-262-04178-2.\nElmasri, Ramez; Navathe, Shamkant B. (2000), Fundamentals of Database Systems (3rd ed.), Addison–Wesley, ISBN 978-0-201-54263-9.\nGhosh, Sukumar (2007), Distributed Systems – An Algorithmic Approach, Chapman & Hall/CRC, ISBN 978-1-58488-564-1.\nLynch, Nancy A. (1996), Distributed Algorithms, Morgan Kaufmann, ISBN 978-1-55860-348-6.\nHerlihy, Maurice P.; Shavit, Nir N. (2008), The Art of Multiprocessor Programming, Morgan Kaufmann, ISBN 978-0-12-370591-4.\nPapadimitriou, Christos H. (1994), Computational Complexity, Addison–Wesley, ISBN 978-0-201-53082-7.\nPeleg, David (2000), Distributed Computing: A Locality-Sensitive Approach, SIAM, ISBN 978-0-89871-464-7, archived from the original on 2009-08-06, retrieved 2009-07-16.\nArticles\nCole, Richard; Vishkin, Uzi (1986), \"Deterministic coin tossing with applications to optimal parallel list ranking\", Information and Control, 70 (1): 32–53, doi:10.1016/S0019-9958(86)80023-7.\nKeidar, Idit (2008), \"Distributed computing column 32 – The year in review\", ACM SIGACT News, 39 (4): 53–54, CiteSeerX 10.1.1.116.1285, doi:10.1145/1466390.1466402, S2CID 7607391, archived from the original on 2014-01-16, retrieved 2009-08-20.\nLinial, Nathan (1992), \"Locality in distributed graph algorithms\", SIAM Journal on Computing, 21 (1): 193–201, CiteSeerX 10.1.1.471.6378, doi:10.1137/0221015.\nNaor, Moni; Stockmeyer, Larry (1995), \"What can be computed locally?\" (PDF), SIAM Journal on Computing, 24 (6): 1259–1277, CiteSeerX 10.1.1.29.669, doi:10.1137/S0097539793254571, archived (PDF) from the original on 2013-01-08.\nWeb sites\nGodfrey, Bill (2002). \"A primer on distributed computing\". Archived from the original on 2021-05-13. Retrieved 2021-05-13.\nPeter, Ian (2004). \"Ian Peter's History of the Internet\". Archived from the original on 2010-01-20. Retrieved 2009-08-04.\nFurther reading\nBooks\nAttiya, Hagit and Jennifer Welch (2004), Distributed Computing: Fundamentals, Simulations, and Advanced Topics, Wiley-Interscience ISBN 0-471-45324-2.\nChristian Cachin; Rachid Guerraoui; Luís Rodrigues (2011), Introduction to Reliable and Secure Distributed Programming (2. ed.), Springer, Bibcode:2011itra.book.....C, ISBN 978-3-642-15259-7\nCoulouris, George; et al. (2011), Distributed Systems: Concepts and Design (5th Edition), Addison-Wesley ISBN 0-132-14301-1.\nFaber, Jim (1998), Java Distributed Computing, O'Reilly, archived from the original on 2010-08-24, retrieved 2010-09-29: Java Distributed Computing by Jim Faber, 1998 Archived 2010-08-24 at the Wayback Machine\nGarg, Vijay K. (2002), Elements of Distributed Computing, Wiley-IEEE Press ISBN 0-471-03600-5.\nTel, Gerard (1994), Introduction to Distributed Algorithms, Cambridge University Press\nChandy, Mani; et al. (1988), Parallel Program Design, Addison-Wesley ISBN 0201058669\nDusseau, Remzi H.; Dusseau, Andrea (2016). Operating Systems: Three Easy Pieces, Chapter 48 Distributed Systems (PDF). Archived from the original (PDF) on 31 August 2021. Retrieved 8 October 2021.\nArticles\nKeidar, Idit; Rajsbaum, Sergio, eds. (2000–2009), \"Distributed computing column\", ACM SIGACT News, archived from the original on 2014-01-16, retrieved 2009-08-16.\nBirrell, A. D.; Levin, R.; Schroeder, M. D.; Needham, R. M. (April 1982). \"Grapevine: An exercise in distributed computing\" (PDF). Communications of the ACM. 25 (4): 260–274. doi:10.1145/358468.358487. S2CID 16066616. Archived (PDF) from the original on 2016-07-30.\nConference Papers\nRodriguez, Carlos; Villagra, Marcos; Baran, Benjamin (2007). \"Asynchronous team algorithms for Boolean Satisfiability\". 2007 2nd Bio-Inspired Models of Network, Information and Computing Systems. pp. 66–69. doi:10.1109/BIMNICS.2007.4610083. S2CID 15185219.\nExternal links\n\nWikiquote has quotations related to Distributed computing.\n Media related to Distributed computing at Wikimedia Commons\nvte\nParallel computing\nAuthority control databases Edit this at Wikidata\nCategory: Distributed computing\nThis page was last edited on 4 October 2024, at 08:15 (UTC).\nText is available under the Creative Commons Attribution-ShareAlike 4.0 License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\nPrivacy policyAbout WikipediaDisclaimersContact WikipediaCode of ConductDevelopersStatisticsCookie statementMobile view\nWikimedia FoundationPowered by MediaWiki\n\n\nMain menu\n\nWikipediaThe Free Encyclopedia\nSearch Wikipedia\nSearch\nDonate\nCreate account\nLog in\n\nPersonal tools\nContents hide\n(Top)\nIntroduction\nPatterns\nParallel and distributed computing\nHistory\nArchitectures\nApplications\nExamples\nReactive distributed systems\nTheoretical foundations\nToggle Theoretical foundations subsection\nModels\nAn example\nComplexity measures\nOther problems\nElection\nProperties of distributed systems\nSee also\nNotes\nReferences\nFurther reading\nExternal links\nDistributed computing\n\nArticle\nTalk\nRead\nEdit\nView history\n\nTools\nAppearance hide\nText\n\nSmall\n\nStandard\n\nLarge\nWidth\n\nStandard\n\nWide\nColor (beta)\n\nAutomatic\n\nLight\n\nDark\nFrom Wikipedia, the free encyclopedia\nNot to be confused with Decentralized computing.\nDistributed computing is a field of computer science that studies distributed systems, defined as computer systems whose inter-communicating components are located on different networked computers.[1][2]\n\nThe components of a distributed system communicate and coordinate their actions by passing messages to one another in order to achieve a common goal. Three significant challenges of distributed systems are: maintaining concurrency of components, overcoming the lack of a global clock, and managing the independent failure of components.[1] When a component of one system fails, the entire system does not fail.[3] Examples of distributed systems vary from SOA-based systems to microservices to massively multiplayer online games to peer-to-peer applications. Distributed systems cost significantly more than monolithic architectures, primarily due to increased needs for additional hardware, servers, gateways, firewalls, new subnets, proxies, and so on.[4] Also, distributed systems are prone to fallacies of distributed computing. On the other hand, a well designed distributed system is more scalable, more durable, more changeable and more fine-tuned than a monolithic application deployed on a single machine.[5] According to Marc Brooker: \"a system is scalable in the range where marginal cost of additional workload is nearly constant.\" Serverless technologies fit this definition but you need to consider total cost of ownership not just the infra cost. [6]\n\nA computer program that runs within a distributed system is called a distributed program,[7] and distributed programming is the process of writing such programs.[8] There are many different types of implementations for the message passing mechanism, including pure HTTP, RPC-like connectors and message queues.[9]\n\nDistributed computing also refers to the use of distributed systems to solve computational problems. In distributed computing, a problem is divided into many tasks, each of which is solved by one or more computers,[10] which communicate with each other via message passing.[11]\n\nIntroduction\nThe word distributed in terms such as \"distributed system\", \"distributed programming\", and \"distributed algorithm\" originally referred to computer networks where individual computers were physically distributed within some geographical area.[12] The terms are nowadays used in a much wider sense, even referring to autonomous processes that run on the same physical computer and interact with each other by message passing.[11]\n\nWhile there is no single definition of a distributed system,[13] the following defining properties are commonly used as:\n\nThere are several autonomous computational entities (computers or nodes), each of which has its own local memory.[14]\nThe entities communicate with each other by message passing.[15]\nA distributed system may have a common goal, such as solving a large computational problem;[16] the user then perceives the collection of autonomous processors as a unit. Alternatively, each computer may have its own user with individual needs, and the purpose of the distributed system is to coordinate the use of shared resources or provide communication services to the users.[17]\n\nOther typical properties of distributed systems include the following:\n\nThe system has to tolerate failures in individual computers.[18]\nThe structure of the system (network topology, network latency, number of computers) is not known in advance, the system may consist of different kinds of computers and network links, and the system may change during the execution of a distributed program.[19]\nEach computer has only a limited, incomplete view of the system. Each computer may know only one part of the input.[20]\nPatterns\nHere are common architectural patterns used for distributed computing:[21]\n\nSaga interaction pattern\nMicroservices\nEvent driven architecture\nParallel and distributed computing\n\n(a), (b): a distributed system.\n(c): a parallel system.\nDistributed systems are groups of networked computers which share a common goal for their work. The terms \"concurrent computing\", \"parallel computing\", and \"distributed computing\" have much overlap, and no clear distinction exists between them.[22] The same system may be characterized both as \"parallel\" and \"distributed\"; the processors in a typical distributed system run concurrently in parallel.[23] Parallel computing may be seen as a particularly tightly coupled form of distributed computing,[24] and distributed computing may be seen as a loosely coupled form of parallel computing.[13] Nevertheless, it is possible to roughly classify concurrent systems as \"parallel\" or \"distributed\" using the following criteria:\n\nIn parallel computing, all processors may have access to a shared memory to exchange information between processors.[25]\nIn distributed computing, each processor has its own private memory (distributed memory). Information is exchanged by passing messages between the processors.[26]\nThe figure on the right illustrates the difference between distributed and parallel systems. Figure (a) is a schematic view of a typical distributed system; the system is represented as a network topology in which each node is a computer and each line connecting the nodes is a communication link. Figure (b) shows the same distributed system in more detail: each computer has its own local memory, and information can be exchanged only by passing messages from one node to another by using the available communication links. Figure (c) shows a parallel system in which each processor has a direct access to a shared memory.\n\nThe situation is further complicated by the traditional uses of the terms parallel and distributed algorithm that do not quite match the above definitions of parallel and distributed systems (see below for more detailed discussion). Nevertheless, as a rule of thumb, high-performance parallel computation in a shared-memory multiprocessor uses parallel algorithms while the coordination of a large-scale distributed system uses distributed algorithms.[27]\n\nHistory\nThe use of concurrent processes which communicate through message-passing has its roots in operating system architectures studied in the 1960s.[28] The first widespread distributed systems were local-area networks such as Ethernet, which was invented in the 1970s.[29]\n\nARPANET, one of the predecessors of the Internet, was introduced in the late 1960s, and ARPANET e-mail was invented in the early 1970s. E-mail became the most successful application of ARPANET,[30] and it is probably the earliest example of a large-scale distributed application. In addition to ARPANET (and its successor, the global Internet), other early worldwide computer networks included Usenet and FidoNet from the 1980s, both of which were used to support distributed discussion systems.[31]\n\nThe study of distributed computing became its own branch of computer science in the late 1970s and early 1980s. The first conference in the field, Symposium on Principles of Distributed Computing (PODC), dates back to 1982, and its counterpart International Symposium on Distributed Computing (DISC) was first held in Ottawa in 1985 as the International Workshop on Distributed Algorithms on Graphs.[32]\n\nArchitectures\nVarious hardware and software architectures are used for distributed computing. At a lower level, it is necessary to interconnect multiple CPUs with some sort of network, regardless of whether that network is printed onto a circuit board or made up of loosely coupled devices and cables. At a higher level, it is necessary to interconnect processes running on those CPUs with some sort of communication system.[33]\n\nWhether these CPUs share resources or not determines a first distinction between three types of architecture:\n\nShared memory\nShared disk\nShared nothing.\nDistributed programming typically falls into one of several basic architectures: client–server, three-tier, n-tier, or peer-to-peer; or categories: loose coupling, or tight coupling.[34]\n\nClient–server: architectures where smart clients contact the server for data then format and display it to the users. Input at the client is committed back to the server when it represents a permanent change.\nThree-tier: architectures that move the client intelligence to a middle tier so that stateless clients can be used. This simplifies application deployment. Most web applications are three-tier.\nn-tier: architectures that refer typically to web applications which further forward their requests to other enterprise services. This type of application is the one most responsible for the success of application servers.\nPeer-to-peer: architectures where there are no special machines that provide a service or manage the network resources.[35]: 227  Instead all responsibilities are uniformly divided among all machines, known as peers. Peers can serve both as clients and as servers.[36] Examples of this architecture include BitTorrent and the bitcoin network.\nAnother basic aspect of distributed computing architecture is the method of communicating and coordinating work among concurrent processes. Through various message passing protocols, processes may communicate directly with one another, typically in a main/sub relationship. Alternatively, a \"database-centric\" architecture can enable distributed computing to be done without any form of direct inter-process communication, by utilizing a shared database.[37] Database-centric architecture in particular provides relational processing analytics in a schematic architecture allowing for live environment relay. This enables distributed computing functions both within and beyond the parameters of a networked database.[38]\n\nApplications\nReasons for using distributed systems and distributed computing may include:\n\nThe very nature of an application may require the use of a communication network that connects several computers: for example, data produced in one physical location and required in another location.\nThere are many cases in which the use of a single computer would be possible in principle, but the use of a distributed system is beneficial for practical reasons. For example:\nIt can allow for much larger storage and memory, faster compute, and higher bandwidth than a single machine.\nIt can provide more reliability than a non-distributed system, as there is no single point of failure. Moreover, a distributed system may be easier to expand and manage than a monolithic uniprocessor system.[39]\nIt may be more cost-efficient to obtain the desired level of performance by using a cluster of several low-end computers, in comparison with a single high-end computer.\nExamples\nExamples of distributed systems and applications of distributed computing include the following:[40]\n\ntelecommunications networks:\ntelephone networks and cellular networks,\ncomputer networks such as the Internet,\nwireless sensor networks,\nrouting algorithms;\nnetwork applications:\nWorld Wide Web and peer-to-peer networks,\nmassively multiplayer online games and virtual reality communities,\ndistributed databases and distributed database management systems,\nnetwork file systems,\ndistributed cache such as burst buffers,\ndistributed information processing systems such as banking systems and airline reservation systems;\nreal-time process control:\naircraft control systems,\nindustrial control systems;\nparallel computation:\nscientific computing, including cluster computing, grid computing, cloud computing,[41] and various volunteer computing projects,\ndistributed rendering in computer graphics.\npeer-to-peer\nReactive distributed systems\nAccording to Reactive Manifesto, reactive distributed systems are responsive, resilient, elastic and message-driven. Subsequently, Reactive systems are more flexible, loosely-coupled and scalable. To make your systems reactive, you are advised to implement Reactive Principles. Reactive Principles are a set of principles and patterns which help to make your cloud native application as well as edge native applications more reactive. [42]\n\nTheoretical foundations\nMain article: Distributed algorithm\nModels\nMany tasks that we would like to automate by using a computer are of question–answer type: we would like to ask a question and the computer should produce an answer. In theoretical computer science, such tasks are called computational problems. Formally, a computational problem consists of instances together with a solution for each instance. Instances are questions that we can ask, and solutions are desired answers to these questions.\n\nTheoretical computer science seeks to understand which computational problems can be solved by using a computer (computability theory) and how efficiently (computational complexity theory). Traditionally, it is said that a problem can be solved by using a computer if we can design an algorithm that produces a correct solution for any given instance. Such an algorithm can be implemented as a computer program that runs on a general-purpose computer: the program reads a problem instance from input, performs some computation, and produces the solution as output. Formalisms such as random-access machines or universal Turing machines can be used as abstract models of a sequential general-purpose computer executing such an algorithm.[43][44]\n\nThe field of concurrent and distributed computing studies similar questions in the case of either multiple computers, or a computer that executes a network of interacting processes: which computational problems can be solved in such a network and how efficiently? However, it is not at all obvious what is meant by \"solving a problem\" in the case of a concurrent or distributed system: for example, what is the task of the algorithm designer, and what is the concurrent or distributed equivalent of a sequential general-purpose computer?[citation needed]\n\nThe discussion below focuses on the case of multiple computers, although many of the issues are the same for concurrent processes running on a single computer.\n\nThree viewpoints are commonly used:\n\nParallel algorithms in shared-memory model\nAll processors have access to a shared memory. The algorithm designer chooses the program executed by each processor.\nOne theoretical model is the parallel random-access machines (PRAM) that are used.[45] However, the classical PRAM model assumes synchronous access to the shared memory.\nShared-memory programs can be extended to distributed systems if the underlying operating system encapsulates the communication between nodes and virtually unifies the memory across all individual systems.\nA model that is closer to the behavior of real-world multiprocessor machines and takes into account the use of machine instructions, such as Compare-and-swap (CAS), is that of asynchronous shared memory. There is a wide body of work on this model, a summary of which can be found in the literature.[46][47]\nParallel algorithms in message-passing model\nThe algorithm designer chooses the structure of the network, as well as the program executed by each computer.\nModels such as Boolean circuits and sorting networks are used.[48] A Boolean circuit can be seen as a computer network: each gate is a computer that runs an extremely simple computer program. Similarly, a sorting network can be seen as a computer network: each comparator is a computer.\nDistributed algorithms in message-passing model\nThe algorithm designer only chooses the computer program. All computers run the same program. The system must work correctly regardless of the structure of the network.\nA commonly used model is a graph with one finite-state machine per node.\nIn the case of distributed algorithms, computational problems are typically related to graphs. Often the graph that describes the structure of the computer network is the problem instance. This is illustrated in the following example.[49]\n\nAn example\nConsider the computational problem of finding a coloring of a given graph G. Different fields might take the following approaches:\n\nCentralized algorithms[49]\nThe graph G is encoded as a string, and the string is given as input to a computer. The computer program finds a coloring of the graph, encodes the coloring as a string, and outputs the result.\nParallel algorithms\nAgain, the graph G is encoded as a string. However, multiple computers can access the same string in parallel. Each computer might focus on one part of the graph and produce a coloring for that part.\nThe main focus is on high-performance computation that exploits the processing power of multiple computers in parallel.\nDistributed algorithms\nThe graph G is the structure of the computer network. There is one computer for each node of G and one communication link for each edge of G. Initially, each computer only knows about its immediate neighbors in the graph G; the computers must exchange messages with each other to discover more about the structure of G. Each computer must produce its own color as output.\nThe main focus is on coordinating the operation of an arbitrary distributed system.[49]\nWhile the field of parallel algorithms has a different focus than the field of distributed algorithms, there is much interaction between the two fields. For example, the Cole–Vishkin algorithm for graph coloring[50] was originally presented as a parallel algorithm, but the same technique can also be used directly as a distributed algorithm.\n\nMoreover, a parallel algorithm can be implemented either in a parallel system (using shared memory) or in a distributed system (using message passing).[51] The traditional boundary between parallel and distributed algorithms (choose a suitable network vs. run in any given network) does not lie in the same place as the boundary between parallel and distributed systems (shared memory vs. message passing).\n\nComplexity measures\nIn parallel algorithms, yet another resource in addition to time and space is the number of computers. Indeed, often there is a trade-off between the running time and the number of computers: the problem can be solved faster if there are more computers running in parallel (see speedup). If a decision problem can be solved in polylogarithmic time by using a polynomial number of processors, then the problem is said to be in the class NC.[52] The class NC can be defined equally well by using the PRAM formalism or Boolean circuits—PRAM machines can simulate Boolean circuits efficiently and vice versa.[53]\n\nIn the analysis of distributed algorithms, more attention is usually paid on communication operations than computational steps. Perhaps the simplest model of distributed computing is a synchronous system where all nodes operate in a lockstep fashion. This model is commonly known as the LOCAL model. During each communication round, all nodes in parallel (1) receive the latest messages from their neighbours, (2) perform arbitrary local computation, and (3) send new messages to their neighbors. In such systems, a central complexity measure is the number of synchronous communication rounds required to complete the task.[54]\n\nThis complexity measure is closely related to the diameter of the network. Let D be the diameter of the network. On the one hand, any computable problem can be solved trivially in a synchronous distributed system in approximately 2D communication rounds: simply gather all information in one location (D rounds), solve the problem, and inform each node about the solution (D rounds).\n\nOn the other hand, if the running time of the algorithm is much smaller than D communication rounds, then the nodes in the network must produce their output without having the possibility to obtain information about distant parts of the network. In other words, the nodes must make globally consistent decisions based on information that is available in their local D-neighbourhood. Many distributed algorithms are known with the running time much smaller than D rounds, and understanding which problems can be solved by such algorithms is one of the central research questions of the field.[55] Typically an algorithm which solves a problem in polylogarithmic time in the network size is considered efficient in this model.\n\nAnother commonly used measure is the total number of bits transmitted in the network (cf. communication complexity).[56] The features of this concept are typically captured with the CONGEST(B) model, which is similarly defined as the LOCAL model, but where single messages can only contain B bits.\n\nOther problems\nTraditional computational problems take the perspective that the user asks a question, a computer (or a distributed system) processes the question, then produces an answer and stops. However, there are also problems where the system is required not to stop, including the dining philosophers problem and other similar mutual exclusion problems. In these problems, the distributed system is supposed to continuously coordinate the use of shared resources so that no conflicts or deadlocks occur.\n\nThere are also fundamental challenges that are unique to distributed computing, for example those related to fault-tolerance. Examples of related problems include consensus problems,[57] Byzantine fault tolerance,[58] and self-stabilisation.[59]\n\nMuch research is also focused on understanding the asynchronous nature of distributed systems:\n\nSynchronizers can be used to run synchronous algorithms in asynchronous systems.[60]\nLogical clocks provide a causal happened-before ordering of events.[61]\nClock synchronization algorithms provide globally consistent physical time stamps.[62]\nNote that in distributed systems, latency should be measured through \"99th percentile\" because \"median\" and \"average\" can be misleading.[63]\n\nElection\nCoordinator election (or leader election) is the process of designating a single process as the organizer of some task distributed among several computers (nodes). Before the task is begun, all network nodes are either unaware which node will serve as the \"coordinator\" (or leader) of the task, or unable to communicate with the current coordinator. After a coordinator election algorithm has been run, however, each node throughout the network recognizes a particular, unique node as the task coordinator.[64]\n\nThe network nodes communicate among themselves in order to decide which of them will get into the \"coordinator\" state. For that, they need some method in order to break the symmetry among them. For example, if each node has unique and comparable identities, then the nodes can compare their identities, and decide that the node with the highest identity is the coordinator.[64]\n\nThe definition of this problem is often attributed to LeLann, who formalized it as a method to create a new token in a token ring network in which the token has been lost.[65]\n\nCoordinator election algorithms are designed to be economical in terms of total bytes transmitted, and time. The algorithm suggested by Gallager, Humblet, and Spira[66] for general undirected graphs has had a strong impact on the design of distributed algorithms in general, and won the Dijkstra Prize for an influential paper in distributed computing.\n\nMany other algorithms were suggested for different kinds of network graphs, such as undirected rings, unidirectional rings, complete graphs, grids, directed Euler graphs, and others. A general method that decouples the issue of the graph family from the design of the coordinator election algorithm was suggested by Korach, Kutten, and Moran.[67]\n\nIn order to perform coordination, distributed systems employ the concept of coordinators. The coordinator election problem is to choose a process from among a group of processes on different processors in a distributed system to act as the central coordinator. Several central coordinator election algorithms exist.[68]\n\nProperties of distributed systems\nSo far the focus has been on designing a distributed system that solves a given problem. A complementary research problem is studying the properties of a given distributed system.[69][70]\n\nThe halting problem is an analogous example from the field of centralised computation: we are given a computer program and the task is to decide whether it halts or runs forever. The halting problem is undecidable in the general case, and naturally understanding the behaviour of a computer network is at least as hard as understanding the behaviour of one computer.[71]\n\nHowever, there are many interesting special cases that are decidable. In particular, it is possible to reason about the behaviour of a network of finite-state machines. One example is telling whether a given network of interacting (asynchronous and non-deterministic) finite-state machines can reach a deadlock. This problem is PSPACE-complete,[72] i.e., it is decidable, but not likely that there is an efficient (centralised, parallel or distributed) algorithm that solves the problem in the case of large networks.\n\nSee also\nActor model\nAppScale\nBOINC\nCode mobility\nDataflow programming\nDecentralized computing\nDistributed algorithm\nDistributed algorithmic mechanism design\nDistributed cache\nDistributed GIS\nDistributed networking\nDistributed operating system\nEventual consistency\nEdsger W. Dijkstra Prize in Distributed Computing\nFederation (information technology)\nFlat neighborhood network\nFog computing\nFolding@home\nGrid computing\nInferno\nInternet GIS\nJungle computing\nLayered queueing network\nLibrary Oriented Architecture (LOA)\nList of distributed computing conferences\nList of volunteer computing projects\nModel checking\nOpenHarmony\nHarmonyOS\nParallel distributed processing\nParallel programming model\nPlan 9 from Bell Labs\nShared nothing architecture\nWeb GIS\nNotes\n Tanenbaum, Andrew S.; Steen, Maarten van (2002). Distributed systems: principles and paradigms. Upper Saddle River, NJ: Pearson Prentice Hall. ISBN 0-13-088893-1. Archived from the original on 2020-08-12. Retrieved 2020-08-28.\n \"Distributed Programs\". Texts in Computer Science. London: Springer London. 2010. pp. 373–406. doi:10.1007/978-1-84882-745-5_11. ISBN 978-1-84882-744-8. ISSN 1868-0941. Systems consist of a number of physically distributed components that work independently using their private storage, but also communicate from time to time by explicit message passing. Such systems are called distributed systems.\n Dusseau & Dusseau 2016, p. 1–2.\n Ford, Neal (March 3, 2020). Fundamentals of Software Architecture: An Engineering Approach (1st ed.). O'Reilly Media. pp. 146–147. ISBN 978-1492043454.\n Monolith to Microservices Evolutionary Patterns to Transform Your Monolith. O'Reilly Media. ISBN 9781492047810.\n Building Serverless Applications on Knative. O'Reilly Media. ISBN 9781098142049.\n \"Distributed Programs\". Texts in Computer Science. London: Springer London. 2010. pp. 373–406. doi:10.1007/978-1-84882-745-5_11. ISBN 978-1-84882-744-8. ISSN 1868-0941. Distributed programs are abstract descriptions of distributed systems. A distributed program consists of a collection of processes that work concurrently and communicate by explicit message passing. Each process can access a set of variables which are disjoint from the variables that can be changed by any other process.\n Andrews (2000). Dolev (2000). Ghosh (2007), p. 10.\n Magnoni, L. (2015). \"Modern Messaging for Distributed Sytems (sic)\". Journal of Physics: Conference Series. 608 (1): 012038. doi:10.1088/1742-6596/608/1/012038. ISSN 1742-6596.\n Godfrey (2002).\n Andrews (2000), p. 291–292. Dolev (2000), p. 5.\n Lynch (1996), p. 1.\n Ghosh (2007), p. 10.\n Andrews (2000), pp. 8–9, 291. Dolev (2000), p. 5. Ghosh (2007), p. 3. Lynch (1996), p. xix, 1. Peleg (2000), p. xv.\n Andrews (2000), p. 291. Ghosh (2007), p. 3. Peleg (2000), p. 4.\n Ghosh (2007), p. 3–4. Peleg (2000), p. 1.\n Ghosh (2007), p. 4. Peleg (2000), p. 2.\n Ghosh (2007), p. 4, 8. Lynch (1996), p. 2–3. Peleg (2000), p. 4.\n Lynch (1996), p. 2. Peleg (2000), p. 1.\n Ghosh (2007), p. 7. Lynch (1996), p. xix, 2. Peleg (2000), p. 4.\n Fundamentals of Software Architecture: An Engineering Approach. O'Reilly Media. 2020. ISBN 978-1492043454.\n Ghosh (2007), p. 10. Keidar (2008).\n Lynch (1996), p. xix, 1–2. Peleg (2000), p. 1.\n Peleg (2000), p. 1.\n Papadimitriou (1994), Chapter 15. Keidar (2008).\n See references in Introduction.\n Bentaleb, A.; Yifan, L.; Xin, J.; et al. (2016). \"Parallel and Distributed Algorithms\" (PDF). National University of Singapore. Archived (PDF) from the original on 2017-03-26. Retrieved 20 July 2018.\n Andrews (2000), p. 348.\n Andrews (2000), p. 32.\n Peter (2004), The history of email Archived 2009-04-15 at the Wayback Machine.\n Banks, M. (2012). On the Way to the Web: The Secret History of the Internet and its Founders. Apress. pp. 44–5. ISBN 9781430250746. Archived from the original on 2023-01-20. Retrieved 2018-07-20.\n Tel, G. (2000). Introduction to Distributed Algorithms. Cambridge University Press. pp. 35–36. ISBN 9780521794831. Archived from the original on 2023-01-20. Retrieved 2018-07-20.\n Ohlídal, M.; Jaroš, J.; Schwarz, J.; et al. (2006). \"Evolutionary Design of OAB and AAB Communication Schedules for Interconnection Networks\". In Rothlauf, F.; Branke, J.; Cagnoni, S. (eds.). Applications of Evolutionary Computing. Springer Science & Business Media. pp. 267–78. ISBN 9783540332374.\n \"Real Time And Distributed Computing Systems\" (PDF). ISSN 2278-0661. Archived from the original (PDF) on 2017-01-10. Retrieved 2017-01-09. {{cite journal}}: Cite journal requires |journal= (help)\n Vigna P, Casey MJ. The Age of Cryptocurrency: How Bitcoin and the Blockchain Are Challenging the Global Economic Order St. Martin's Press January 27, 2015 ISBN 9781250065636\n Quang Hieu Vu; Mihai Lupu; Beng Chin Ooi (2010). Peer-to-peer computing : principles and applications. Heidelberg: Springer. p. 16. ISBN 9783642035135. OCLC 663093862.\n Lind P, Alm M (2006), \"A database-centric virtual chemistry system\", J Chem Inf Model, 46 (3): 1034–9, doi:10.1021/ci050360b, PMID 16711722.\n Chiu, G (1990). \"A model for optimal database allocation in distributed computing systems\". Proceedings. IEEE INFOCOM'90: Ninth Annual Joint Conference of the IEEE Computer and Communications Societies.\n Elmasri & Navathe (2000), Section 24.1.2.\n Andrews (2000), p. 10–11. Ghosh (2007), p. 4–6. Lynch (1996), p. xix, 1. Peleg (2000), p. xv. Elmasri & Navathe (2000), Section 24.\n Haussmann, J. (2019). \"Cost-efficient parallel processing of irregularly structured problems in cloud computing environments\". Journal of Cluster Computing. 22 (3): 887–909. doi:10.1007/s10586-018-2879-3. S2CID 54447518.\n Reactive Application Development. Manning. 2018. ISBN 9781638355816.\n Toomarian, N.B.; Barhen, J.; Gulati, S. (1992). \"Neural Networks for Real-Time Robotic Applications\". In Fijany, A.; Bejczy, A. (eds.). Parallel Computation Systems For Robotics: Algorithms And Architectures. World Scientific. p. 214. ISBN 9789814506175. Archived from the original on 2020-08-01. Retrieved 2018-07-20.\n Savage, J.E. (1998). Models of Computation: Exploring the Power of Computing. Addison Wesley. p. 209. ISBN 9780201895391.\n Cormen, Leiserson & Rivest (1990), Section 30.\n Herlihy & Shavit (2008), Chapters 2–6.\n Lynch (1996)\n Cormen, Leiserson & Rivest (1990), Sections 28 and 29.\n TULSIRAMJI GAIKWAD-PATIL College of Engineering & Technology, Nagpur Department of Information Technology Introduction to Distributed Systems[1]\n Cole & Vishkin (1986). Cormen, Leiserson & Rivest (1990), Section 30.5.\n Andrews (2000), p. ix.\n Arora & Barak (2009), Section 6.7. Papadimitriou (1994), Section 15.3.\n Papadimitriou (1994), Section 15.2.\n Lynch (1996), p. 17–23.\n Peleg (2000), Sections 2.3 and 7. Linial (1992). Naor & Stockmeyer (1995).\n Schneider, J.; Wattenhofer, R. (2011). \"Trading Bit, Message, and Time Complexity of Distributed Algorithms\". In Peleg, D. (ed.). Distributed Computing. Springer Science & Business Media. pp. 51–65. ISBN 9783642240997. Archived from the original on 2020-08-01. Retrieved 2018-07-20.\n Lynch (1996), Sections 5–7. Ghosh (2007), Chapter 13.\n Lynch (1996), p. 99–102. Ghosh (2007), p. 192–193.\n Dolev (2000). Ghosh (2007), Chapter 17.\n Lynch (1996), Section 16. Peleg (2000), Section 6.\n Lynch (1996), Section 18. Ghosh (2007), Sections 6.2–6.3.\n Ghosh (2007), Section 6.4.\n Foundations of Data Intensive Applications Large Scale Data Analytics Under the Hood. 2021. ISBN 9781119713012.\n Haloi, S. (2015). Apache ZooKeeper Essentials. Packt Publishing Ltd. pp. 100–101. ISBN 9781784398323. Archived from the original on 2023-01-20. Retrieved 2018-07-20.\n LeLann, G. (1977). \"Distributed systems - toward a formal approach\". Information Processing. 77: 155·160 – via Elsevier.\n R. G. Gallager, P. A. Humblet, and P. M. Spira (January 1983). \"A Distributed Algorithm for Minimum-Weight Spanning Trees\" (PDF). ACM Transactions on Programming Languages and Systems. 5 (1): 66–77. doi:10.1145/357195.357200. S2CID 2758285. Archived (PDF) from the original on 2017-09-26.\n Korach, Ephraim; Kutten, Shay; Moran, Shlomo (1990). \"A Modular Technique for the Design of Efficient Distributed Leader Finding Algorithms\" (PDF). ACM Transactions on Programming Languages and Systems. 12 (1): 84–101. CiteSeerX 10.1.1.139.7342. doi:10.1145/77606.77610. S2CID 9175968. Archived (PDF) from the original on 2007-04-18.\n Hamilton, Howard. \"Distributed Algorithms\". Archived from the original on 2012-11-24. Retrieved 2013-03-03.\n \"Major unsolved problems in distributed systems?\". cstheory.stackexchange.com. Archived from the original on 20 January 2023. Retrieved 16 March 2018.\n \"How big data and distributed systems solve traditional scalability problems\". theserverside.com. Archived from the original on 17 March 2018. Retrieved 16 March 2018.\n Svozil, K. (2011). \"Indeterminism and Randomness Through Physics\". In Hector, Z. (ed.). Randomness Through Computation: Some Answers, More Questions. World Scientific. pp. 112–3. ISBN 9789814462631. Archived from the original on 2020-08-01. Retrieved 2018-07-20.\n Papadimitriou (1994), Section 19.3.\nReferences\nBooks\nAndrews, Gregory R. (2000), Foundations of Multithreaded, Parallel, and Distributed Programming, Addison–Wesley, ISBN 978-0-201-35752-3.\nArora, Sanjeev; Barak, Boaz (2009), Computational Complexity – A Modern Approach, Cambridge, ISBN 978-0-521-42426-4.\nCormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L. (1990), Introduction to Algorithms (1st ed.), MIT Press, ISBN 978-0-262-03141-7.\nDolev, Shlomi (2000), Self-Stabilization, MIT Press, ISBN 978-0-262-04178-2.\nElmasri, Ramez; Navathe, Shamkant B. (2000), Fundamentals of Database Systems (3rd ed.), Addison–Wesley, ISBN 978-0-201-54263-9.\nGhosh, Sukumar (2007), Distributed Systems – An Algorithmic Approach, Chapman & Hall/CRC, ISBN 978-1-58488-564-1.\nLynch, Nancy A. (1996), Distributed Algorithms, Morgan Kaufmann, ISBN 978-1-55860-348-6.\nHerlihy, Maurice P.; Shavit, Nir N. (2008), The Art of Multiprocessor Programming, Morgan Kaufmann, ISBN 978-0-12-370591-4.\nPapadimitriou, Christos H. (1994), Computational Complexity, Addison–Wesley, ISBN 978-0-201-53082-7.\nPeleg, David (2000), Distributed Computing: A Locality-Sensitive Approach, SIAM, ISBN 978-0-89871-464-7, archived from the original on 2009-08-06, retrieved 2009-07-16.\nArticles\nCole, Richard; Vishkin, Uzi (1986), \"Deterministic coin tossing with applications to optimal parallel list ranking\", Information and Control, 70 (1): 32–53, doi:10.1016/S0019-9958(86)80023-7.\nKeidar, Idit (2008), \"Distributed computing column 32 – The year in review\", ACM SIGACT News, 39 (4): 53–54, CiteSeerX 10.1.1.116.1285, doi:10.1145/1466390.1466402, S2CID 7607391, archived from the original on 2014-01-16, retrieved 2009-08-20.\nLinial, Nathan (1992), \"Locality in distributed graph algorithms\", SIAM Journal on Computing, 21 (1): 193–201, CiteSeerX 10.1.1.471.6378, doi:10.1137/0221015.\nNaor, Moni; Stockmeyer, Larry (1995), \"What can be computed locally?\" (PDF), SIAM Journal on Computing, 24 (6): 1259–1277, CiteSeerX 10.1.1.29.669, doi:10.1137/S0097539793254571, archived (PDF) from the original on 2013-01-08.\nWeb sites\nGodfrey, Bill (2002). \"A primer on distributed computing\". Archived from the original on 2021-05-13. Retrieved 2021-05-13.\nPeter, Ian (2004). \"Ian Peter's History of the Internet\". Archived from the original on 2010-01-20. Retrieved 2009-08-04.\nFurther reading\nBooks\nAttiya, Hagit and Jennifer Welch (2004), Distributed Computing: Fundamentals, Simulations, and Advanced Topics, Wiley-Interscience ISBN 0-471-45324-2.\nChristian Cachin; Rachid Guerraoui; Luís Rodrigues (2011), Introduction to Reliable and Secure Distributed Programming (2. ed.), Springer, Bibcode:2011itra.book.....C, ISBN 978-3-642-15259-7\nCoulouris, George; et al. (2011), Distributed Systems: Concepts and Design (5th Edition), Addison-Wesley ISBN 0-132-14301-1.\nFaber, Jim (1998), Java Distributed Computing, O'Reilly, archived from the original on 2010-08-24, retrieved 2010-09-29: Java Distributed Computing by Jim Faber, 1998 Archived 2010-08-24 at the Wayback Machine\nGarg, Vijay K. (2002), Elements of Distributed Computing, Wiley-IEEE Press ISBN 0-471-03600-5.\nTel, Gerard (1994), Introduction to Distributed Algorithms, Cambridge University Press\nChandy, Mani; et al. (1988), Parallel Program Design, Addison-Wesley ISBN 0201058669\nDusseau, Remzi H.; Dusseau, Andrea (2016). Operating Systems: Three Easy Pieces, Chapter 48 Distributed Systems (PDF). Archived from the original (PDF) on 31 August 2021. Retrieved 8 October 2021.\nArticles\nKeidar, Idit; Rajsbaum, Sergio, eds. (2000–2009), \"Distributed computing column\", ACM SIGACT News, archived from the original on 2014-01-16, retrieved 2009-08-16.\nBirrell, A. D.; Levin, R.; Schroeder, M. D.; Needham, R. M. (April 1982). \"Grapevine: An exercise in distributed computing\" (PDF). Communications of the ACM. 25 (4): 260–274. doi:10.1145/358468.358487. S2CID 16066616. Archived (PDF) from the original on 2016-07-30.\nConference Papers\nRodriguez, Carlos; Villagra, Marcos; Baran, Benjamin (2007). \"Asynchronous team algorithms for Boolean Satisfiability\". 2007 2nd Bio-Inspired Models of Network, Information and Computing Systems. pp. 66–69. doi:10.1109/BIMNICS.2007.4610083. S2CID 15185219.\nExternal links\n\nWikiquote has quotations related to Distributed computing.\n Media related to Distributed computing at Wikimedia Commons\nvte\nParallel computing\nAuthority control databases Edit this at Wikidata\nCategory: Distributed computing\nThis page was last edited on 4 October 2024, at 08:15 (UTC).\nText is available under the Creative Commons Attribution-ShareAlike 4.0 License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\nPrivacy policyAbout WikipediaDisclaimersContact WikipediaCode of ConductDevelopersStatisticsCookie statementMobile view\nWikimedia FoundationPowered by MediaWiki\n\n\nMain menu\n\nWikipediaThe Free Encyclopedia\nSearch Wikipedia\nSearch\nDonate\nCreate account\nLog in\n\nPersonal tools\nContents hide\n(Top)\nIntroduction\nPatterns\nParallel and distributed computing\nHistory\nArchitectures\nApplications\nExamples\nReactive distributed systems\nTheoretical foundations\nToggle Theoretical foundations subsection\nModels\nAn example\nComplexity measures\nOther problems\nElection\nProperties of distributed systems\nSee also\nNotes\nReferences\nFurther reading\nExternal links\nDistributed computing\n\nArticle\nTalk\nRead\nEdit\nView history\n\nTools\nAppearance hide\nText\n\nSmall\n\nStandard\n\nLarge\nWidth\n\nStandard\n\nWide\nColor (beta)\n\nAutomatic\n\nLight\n\nDark\nFrom Wikipedia, the free encyclopedia\nNot to be confused with Decentralized computing.\nDistributed computing is a field of computer science that studies distributed systems, defined as computer systems whose inter-communicating components are located on different networked computers.[1][2]\n\nThe components of a distributed system communicate and coordinate their actions by passing messages to one another in order to achieve a common goal. Three significant challenges of distributed systems are: maintaining concurrency of components, overcoming the lack of a global clock, and managing the independent failure of components.[1] When a component of one system fails, the entire system does not fail.[3] Examples of distributed systems vary from SOA-based systems to microservices to massively multiplayer online games to peer-to-peer applications. Distributed systems cost significantly more than monolithic architectures, primarily due to increased needs for additional hardware, servers, gateways, firewalls, new subnets, proxies, and so on.[4] Also, distributed systems are prone to fallacies of distributed computing. On the other hand, a well designed distributed system is more scalable, more durable, more changeable and more fine-tuned than a monolithic application deployed on a single machine.[5] According to Marc Brooker: \"a system is scalable in the range where marginal cost of additional workload is nearly constant.\" Serverless technologies fit this definition but you need to consider total cost of ownership not just the infra cost. [6]\n\nA computer program that runs within a distributed system is called a distributed program,[7] and distributed programming is the process of writing such programs.[8] There are many different types of implementations for the message passing mechanism, including pure HTTP, RPC-like connectors and message queues.[9]\n\nDistributed computing also refers to the use of distributed systems to solve computational problems. In distributed computing, a problem is divided into many tasks, each of which is solved by one or more computers,[10] which communicate with each other via message passing.[11]\n\nIntroduction\nThe word distributed in terms such as \"distributed system\", \"distributed programming\", and \"distributed algorithm\" originally referred to computer networks where individual computers were physically distributed within some geographical area.[12] The terms are nowadays used in a much wider sense, even referring to autonomous processes that run on the same physical computer and interact with each other by message passing.[11]\n\nWhile there is no single definition of a distributed system,[13] the following defining properties are commonly used as:\n\nThere are several autonomous computational entities (computers or nodes), each of which has its own local memory.[14]\nThe entities communicate with each other by message passing.[15]\nA distributed system may have a common goal, such as solving a large computational problem;[16] the user then perceives the collection of autonomous processors as a unit. Alternatively, each computer may have its own user with individual needs, and the purpose of the distributed system is to coordinate the use of shared resources or provide communication services to the users.[17]\n\nOther typical properties of distributed systems include the following:\n\nThe system has to tolerate failures in individual computers.[18]\nThe structure of the system (network topology, network latency, number of computers) is not known in advance, the system may consist of different kinds of computers and network links, and the system may change during the execution of a distributed program.[19]\nEach computer has only a limited, incomplete view of the system. Each computer may know only one part of the input.[20]\nPatterns\nHere are common architectural patterns used for distributed computing:[21]\n\nSaga interaction pattern\nMicroservices\nEvent driven architecture\nParallel and distributed computing\n\n(a), (b): a distributed system.\n(c): a parallel system.\nDistributed systems are groups of networked computers which share a common goal for their work. The terms \"concurrent computing\", \"parallel computing\", and \"distributed computing\" have much overlap, and no clear distinction exists between them.[22] The same system may be characterized both as \"parallel\" and \"distributed\"; the processors in a typical distributed system run concurrently in parallel.[23] Parallel computing may be seen as a particularly tightly coupled form of distributed computing,[24] and distributed computing may be seen as a loosely coupled form of parallel computing.[13] Nevertheless, it is possible to roughly classify concurrent systems as \"parallel\" or \"distributed\" using the following criteria:\n\nIn parallel computing, all processors may have access to a shared memory to exchange information between processors.[25]\nIn distributed computing, each processor has its own private memory (distributed memory). Information is exchanged by passing messages between the processors.[26]\nThe figure on the right illustrates the difference between distributed and parallel systems. Figure (a) is a schematic view of a typical distributed system; the system is represented as a network topology in which each node is a computer and each line connecting the nodes is a communication link. Figure (b) shows the same distributed system in more detail: each computer has its own local memory, and information can be exchanged only by passing messages from one node to another by using the available communication links. Figure (c) shows a parallel system in which each processor has a direct access to a shared memory.\n\nThe situation is further complicated by the traditional uses of the terms parallel and distributed algorithm that do not quite match the above definitions of parallel and distributed systems (see below for more detailed discussion). Nevertheless, as a rule of thumb, high-performance parallel computation in a shared-memory multiprocessor uses parallel algorithms while the coordination of a large-scale distributed system uses distributed algorithms.[27]\n\nHistory\nThe use of concurrent processes which communicate through message-passing has its roots in operating system architectures studied in the 1960s.[28] The first widespread distributed systems were local-area networks such as Ethernet, which was invented in the 1970s.[29]\n\nARPANET, one of the predecessors of the Internet, was introduced in the late 1960s, and ARPANET e-mail was invented in the early 1970s. E-mail became the most successful application of ARPANET,[30] and it is probably the earliest example of a large-scale distributed application. In addition to ARPANET (and its successor, the global Internet), other early worldwide computer networks included Usenet and FidoNet from the 1980s, both of which were used to support distributed discussion systems.[31]\n\nThe study of distributed computing became its own branch of computer science in the late 1970s and early 1980s. The first conference in the field, Symposium on Principles of Distributed Computing (PODC), dates back to 1982, and its counterpart International Symposium on Distributed Computing (DISC) was first held in Ottawa in 1985 as the International Workshop on Distributed Algorithms on Graphs.[32]\n\nArchitectures\nVarious hardware and software architectures are used for distributed computing. At a lower level, it is necessary to interconnect multiple CPUs with some sort of network, regardless of whether that network is printed onto a circuit board or made up of loosely coupled devices and cables. At a higher level, it is necessary to interconnect processes running on those CPUs with some sort of communication system.[33]\n\nWhether these CPUs share resources or not determines a first distinction between three types of architecture:\n\nShared memory\nShared disk\nShared nothing.\nDistributed programming typically falls into one of several basic architectures: client–server, three-tier, n-tier, or peer-to-peer; or categories: loose coupling, or tight coupling.[34]\n\nClient–server: architectures where smart clients contact the server for data then format and display it to the users. Input at the client is committed back to the server when it represents a permanent change.\nThree-tier: architectures that move the client intelligence to a middle tier so that stateless clients can be used. This simplifies application deployment. Most web applications are three-tier.\nn-tier: architectures that refer typically to web applications which further forward their requests to other enterprise services. This type of application is the one most responsible for the success of application servers.\nPeer-to-peer: architectures where there are no special machines that provide a service or manage the network resources.[35]: 227  Instead all responsibilities are uniformly divided among all machines, known as peers. Peers can serve both as clients and as servers.[36] Examples of this architecture include BitTorrent and the bitcoin network.\nAnother basic aspect of distributed computing architecture is the method of communicating and coordinating work among concurrent processes. Through various message passing protocols, processes may communicate directly with one another, typically in a main/sub relationship. Alternatively, a \"database-centric\" architecture can enable distributed computing to be done without any form of direct inter-process communication, by utilizing a shared database.[37] Database-centric architecture in particular provides relational processing analytics in a schematic architecture allowing for live environment relay. This enables distributed computing functions both within and beyond the parameters of a networked database.[38]\n\nApplications\nReasons for using distributed systems and distributed computing may include:\n\nThe very nature of an application may require the use of a communication network that connects several computers: for example, data produced in one physical location and required in another location.\nThere are many cases in which the use of a single computer would be possible in principle, but the use of a distributed system is beneficial for practical reasons. For example:\nIt can allow for much larger storage and memory, faster compute, and higher bandwidth than a single machine.\nIt can provide more reliability than a non-distributed system, as there is no single point of failure. Moreover, a distributed system may be easier to expand and manage than a monolithic uniprocessor system.[39]\nIt may be more cost-efficient to obtain the desired level of performance by using a cluster of several low-end computers, in comparison with a single high-end computer.\nExamples\nExamples of distributed systems and applications of distributed computing include the following:[40]\n\ntelecommunications networks:\ntelephone networks and cellular networks,\ncomputer networks such as the Internet,\nwireless sensor networks,\nrouting algorithms;\nnetwork applications:\nWorld Wide Web and peer-to-peer networks,\nmassively multiplayer online games and virtual reality communities,\ndistributed databases and distributed database management systems,\nnetwork file systems,\ndistributed cache such as burst buffers,\ndistributed information processing systems such as banking systems and airline reservation systems;\nreal-time process control:\naircraft control systems,\nindustrial control systems;\nparallel computation:\nscientific computing, including cluster computing, grid computing, cloud computing,[41] and various volunteer computing projects,\ndistributed rendering in computer graphics.\npeer-to-peer\nReactive distributed systems\nAccording to Reactive Manifesto, reactive distributed systems are responsive, resilient, elastic and message-driven. Subsequently, Reactive systems are more flexible, loosely-coupled and scalable. To make your systems reactive, you are advised to implement Reactive Principles. Reactive Principles are a set of principles and patterns which help to make your cloud native application as well as edge native applications more reactive. [42]\n\nTheoretical foundations\nMain article: Distributed algorithm\nModels\nMany tasks that we would like to automate by using a computer are of question–answer type: we would like to ask a question and the computer should produce an answer. In theoretical computer science, such tasks are called computational problems. Formally, a computational problem consists of instances together with a solution for each instance. Instances are questions that we can ask, and solutions are desired answers to these questions.\n\nTheoretical computer science seeks to understand which computational problems can be solved by using a computer (computability theory) and how efficiently (computational complexity theory). Traditionally, it is said that a problem can be solved by using a computer if we can design an algorithm that produces a correct solution for any given instance. Such an algorithm can be implemented as a computer program that runs on a general-purpose computer: the program reads a problem instance from input, performs some computation, and produces the solution as output. Formalisms such as random-access machines or universal Turing machines can be used as abstract models of a sequential general-purpose computer executing such an algorithm.[43][44]\n\nThe field of concurrent and distributed computing studies similar questions in the case of either multiple computers, or a computer that executes a network of interacting processes: which computational problems can be solved in such a network and how efficiently? However, it is not at all obvious what is meant by \"solving a problem\" in the case of a concurrent or distributed system: for example, what is the task of the algorithm designer, and what is the concurrent or distributed equivalent of a sequential general-purpose computer?[citation needed]\n\nThe discussion below focuses on the case of multiple computers, although many of the issues are the same for concurrent processes running on a single computer.\n\nThree viewpoints are commonly used:\n\nParallel algorithms in shared-memory model\nAll processors have access to a shared memory. The algorithm designer chooses the program executed by each processor.\nOne theoretical model is the parallel random-access machines (PRAM) that are used.[45] However, the classical PRAM model assumes synchronous access to the shared memory.\nShared-memory programs can be extended to distributed systems if the underlying operating system encapsulates the communication between nodes and virtually unifies the memory across all individual systems.\nA model that is closer to the behavior of real-world multiprocessor machines and takes into account the use of machine instructions, such as Compare-and-swap (CAS), is that of asynchronous shared memory. There is a wide body of work on this model, a summary of which can be found in the literature.[46][47]\nParallel algorithms in message-passing model\nThe algorithm designer chooses the structure of the network, as well as the program executed by each computer.\nModels such as Boolean circuits and sorting networks are used.[48] A Boolean circuit can be seen as a computer network: each gate is a computer that runs an extremely simple computer program. Similarly, a sorting network can be seen as a computer network: each comparator is a computer.\nDistributed algorithms in message-passing model\nThe algorithm designer only chooses the computer program. All computers run the same program. The system must work correctly regardless of the structure of the network.\nA commonly used model is a graph with one finite-state machine per node.\nIn the case of distributed algorithms, computational problems are typically related to graphs. Often the graph that describes the structure of the computer network is the problem instance. This is illustrated in the following example.[49]\n\nAn example\nConsider the computational problem of finding a coloring of a given graph G. Different fields might take the following approaches:\n\nCentralized algorithms[49]\nThe graph G is encoded as a string, and the string is given as input to a computer. The computer program finds a coloring of the graph, encodes the coloring as a string, and outputs the result.\nParallel algorithms\nAgain, the graph G is encoded as a string. However, multiple computers can access the same string in parallel. Each computer might focus on one part of the graph and produce a coloring for that part.\nThe main focus is on high-performance computation that exploits the processing power of multiple computers in parallel.\nDistributed algorithms\nThe graph G is the structure of the computer network. There is one computer for each node of G and one communication link for each edge of G. Initially, each computer only knows about its immediate neighbors in the graph G; the computers must exchange messages with each other to discover more about the structure of G. Each computer must produce its own color as output.\nThe main focus is on coordinating the operation of an arbitrary distributed system.[49]\nWhile the field of parallel algorithms has a different focus than the field of distributed algorithms, there is much interaction between the two fields. For example, the Cole–Vishkin algorithm for graph coloring[50] was originally presented as a parallel algorithm, but the same technique can also be used directly as a distributed algorithm.\n\nMoreover, a parallel algorithm can be implemented either in a parallel system (using shared memory) or in a distributed system (using message passing).[51] The traditional boundary between parallel and distributed algorithms (choose a suitable network vs. run in any given network) does not lie in the same place as the boundary between parallel and distributed systems (shared memory vs. message passing).\n\nComplexity measures\nIn parallel algorithms, yet another resource in addition to time and space is the number of computers. Indeed, often there is a trade-off between the running time and the number of computers: the problem can be solved faster if there are more computers running in parallel (see speedup). If a decision problem can be solved in polylogarithmic time by using a polynomial number of processors, then the problem is said to be in the class NC.[52] The class NC can be defined equally well by using the PRAM formalism or Boolean circuits—PRAM machines can simulate Boolean circuits efficiently and vice versa.[53]\n\nIn the analysis of distributed algorithms, more attention is usually paid on communication operations than computational steps. Perhaps the simplest model of distributed computing is a synchronous system where all nodes operate in a lockstep fashion. This model is commonly known as the LOCAL model. During each communication round, all nodes in parallel (1) receive the latest messages from their neighbours, (2) perform arbitrary local computation, and (3) send new messages to their neighbors. In such systems, a central complexity measure is the number of synchronous communication rounds required to complete the task.[54]\n\nThis complexity measure is closely related to the diameter of the network. Let D be the diameter of the network. On the one hand, any computable problem can be solved trivially in a synchronous distributed system in approximately 2D communication rounds: simply gather all information in one location (D rounds), solve the problem, and inform each node about the solution (D rounds).\n\nOn the other hand, if the running time of the algorithm is much smaller than D communication rounds, then the nodes in the network must produce their output without having the possibility to obtain information about distant parts of the network. In other words, the nodes must make globally consistent decisions based on information that is available in their local D-neighbourhood. Many distributed algorithms are known with the running time much smaller than D rounds, and understanding which problems can be solved by such algorithms is one of the central research questions of the field.[55] Typically an algorithm which solves a problem in polylogarithmic time in the network size is considered efficient in this model.\n\nAnother commonly used measure is the total number of bits transmitted in the network (cf. communication complexity).[56] The features of this concept are typically captured with the CONGEST(B) model, which is similarly defined as the LOCAL model, but where single messages can only contain B bits.\n\nOther problems\nTraditional computational problems take the perspective that the user asks a question, a computer (or a distributed system) processes the question, then produces an answer and stops. However, there are also problems where the system is required not to stop, including the dining philosophers problem and other similar mutual exclusion problems. In these problems, the distributed system is supposed to continuously coordinate the use of shared resources so that no conflicts or deadlocks occur.\n\nThere are also fundamental challenges that are unique to distributed computing, for example those related to fault-tolerance. Examples of related problems include consensus problems,[57] Byzantine fault tolerance,[58] and self-stabilisation.[59]\n\nMuch research is also focused on understanding the asynchronous nature of distributed systems:\n\nSynchronizers can be used to run synchronous algorithms in asynchronous systems.[60]\nLogical clocks provide a causal happened-before ordering of events.[61]\nClock synchronization algorithms provide globally consistent physical time stamps.[62]\nNote that in distributed systems, latency should be measured through \"99th percentile\" because \"median\" and \"average\" can be misleading.[63]\n\nElection\nCoordinator election (or leader election) is the process of designating a single process as the organizer of some task distributed among several computers (nodes). Before the task is begun, all network nodes are either unaware which node will serve as the \"coordinator\" (or leader) of the task, or unable to communicate with the current coordinator. After a coordinator election algorithm has been run, however, each node throughout the network recognizes a particular, unique node as the task coordinator.[64]\n\nThe network nodes communicate among themselves in order to decide which of them will get into the \"coordinator\" state. For that, they need some method in order to break the symmetry among them. For example, if each node has unique and comparable identities, then the nodes can compare their identities, and decide that the node with the highest identity is the coordinator.[64]\n\nThe definition of this problem is often attributed to LeLann, who formalized it as a method to create a new token in a token ring network in which the token has been lost.[65]\n\nCoordinator election algorithms are designed to be economical in terms of total bytes transmitted, and time. The algorithm suggested by Gallager, Humblet, and Spira[66] for general undirected graphs has had a strong impact on the design of distributed algorithms in general, and won the Dijkstra Prize for an influential paper in distributed computing.\n\nMany other algorithms were suggested for different kinds of network graphs, such as undirected rings, unidirectional rings, complete graphs, grids, directed Euler graphs, and others. A general method that decouples the issue of the graph family from the design of the coordinator election algorithm was suggested by Korach, Kutten, and Moran.[67]\n\nIn order to perform coordination, distributed systems employ the concept of coordinators. The coordinator election problem is to choose a process from among a group of processes on different processors in a distributed system to act as the central coordinator. Several central coordinator election algorithms exist.[68]\n\nProperties of distributed systems\nSo far the focus has been on designing a distributed system that solves a given problem. A complementary research problem is studying the properties of a given distributed system.[69][70]\n\nThe halting problem is an analogous example from the field of centralised computation: we are given a computer program and the task is to decide whether it halts or runs forever. The halting problem is undecidable in the general case, and naturally understanding the behaviour of a computer network is at least as hard as understanding the behaviour of one computer.[71]\n\nHowever, there are many interesting special cases that are decidable. In particular, it is possible to reason about the behaviour of a network of finite-state machines. One example is telling whether a given network of interacting (asynchronous and non-deterministic) finite-state machines can reach a deadlock. This problem is PSPACE-complete,[72] i.e., it is decidable, but not likely that there is an efficient (centralised, parallel or distributed) algorithm that solves the problem in the case of large networks.\n\nSee also\nActor model\nAppScale\nBOINC\nCode mobility\nDataflow programming\nDecentralized computing\nDistributed algorithm\nDistributed algorithmic mechanism design\nDistributed cache\nDistributed GIS\nDistributed networking\nDistributed operating system\nEventual consistency\nEdsger W. Dijkstra Prize in Distributed Computing\nFederation (information technology)\nFlat neighborhood network\nFog computing\nFolding@home\nGrid computing\nInferno\nInternet GIS\nJungle computing\nLayered queueing network\nLibrary Oriented Architecture (LOA)\nList of distributed computing conferences\nList of volunteer computing projects\nModel checking\nOpenHarmony\nHarmonyOS\nParallel distributed processing\nParallel programming model\nPlan 9 from Bell Labs\nShared nothing architecture\nWeb GIS\nNotes\n Tanenbaum, Andrew S.; Steen, Maarten van (2002). Distributed systems: principles and paradigms. Upper Saddle River, NJ: Pearson Prentice Hall. ISBN 0-13-088893-1. Archived from the original on 2020-08-12. Retrieved 2020-08-28.\n \"Distributed Programs\". Texts in Computer Science. London: Springer London. 2010. pp. 373–406. doi:10.1007/978-1-84882-745-5_11. ISBN 978-1-84882-744-8. ISSN 1868-0941. Systems consist of a number of physically distributed components that work independently using their private storage, but also communicate from time to time by explicit message passing. Such systems are called distributed systems.\n Dusseau & Dusseau 2016, p. 1–2.\n Ford, Neal (March 3, 2020). Fundamentals of Software Architecture: An Engineering Approach (1st ed.). O'Reilly Media. pp. 146–147. ISBN 978-1492043454.\n Monolith to Microservices Evolutionary Patterns to Transform Your Monolith. O'Reilly Media. ISBN 9781492047810.\n Building Serverless Applications on Knative. O'Reilly Media. ISBN 9781098142049.\n \"Distributed Programs\". Texts in Computer Science. London: Springer London. 2010. pp. 373–406. doi:10.1007/978-1-84882-745-5_11. ISBN 978-1-84882-744-8. ISSN 1868-0941. Distributed programs are abstract descriptions of distributed systems. A distributed program consists of a collection of processes that work concurrently and communicate by explicit message passing. Each process can access a set of variables which are disjoint from the variables that can be changed by any other process.\n Andrews (2000). Dolev (2000). Ghosh (2007), p. 10.\n Magnoni, L. (2015). \"Modern Messaging for Distributed Sytems (sic)\". Journal of Physics: Conference Series. 608 (1): 012038. doi:10.1088/1742-6596/608/1/012038. ISSN 1742-6596.\n Godfrey (2002).\n Andrews (2000), p. 291–292. Dolev (2000), p. 5.\n Lynch (1996), p. 1.\n Ghosh (2007), p. 10.\n Andrews (2000), pp. 8–9, 291. Dolev (2000), p. 5. Ghosh (2007), p. 3. Lynch (1996), p. xix, 1. Peleg (2000), p. xv.\n Andrews (2000), p. 291. Ghosh (2007), p. 3. Peleg (2000), p. 4.\n Ghosh (2007), p. 3–4. Peleg (2000), p. 1.\n Ghosh (2007), p. 4. Peleg (2000), p. 2.\n Ghosh (2007), p. 4, 8. Lynch (1996), p. 2–3. Peleg (2000), p. 4.\n Lynch (1996), p. 2. Peleg (2000), p. 1.\n Ghosh (2007), p. 7. Lynch (1996), p. xix, 2. Peleg (2000), p. 4.\n Fundamentals of Software Architecture: An Engineering Approach. O'Reilly Media. 2020. ISBN 978-1492043454.\n Ghosh (2007), p. 10. Keidar (2008).\n Lynch (1996), p. xix, 1–2. Peleg (2000), p. 1.\n Peleg (2000), p. 1.\n Papadimitriou (1994), Chapter 15. Keidar (2008).\n See references in Introduction.\n Bentaleb, A.; Yifan, L.; Xin, J.; et al. (2016). \"Parallel and Distributed Algorithms\" (PDF). National University of Singapore. Archived (PDF) from the original on 2017-03-26. Retrieved 20 July 2018.\n Andrews (2000), p. 348.\n Andrews (2000), p. 32.\n Peter (2004), The history of email Archived 2009-04-15 at the Wayback Machine.\n Banks, M. (2012). On the Way to the Web: The Secret History of the Internet and its Founders. Apress. pp. 44–5. ISBN 9781430250746. Archived from the original on 2023-01-20. Retrieved 2018-07-20.\n Tel, G. (2000). Introduction to Distributed Algorithms. Cambridge University Press. pp. 35–36. ISBN 9780521794831. Archived from the original on 2023-01-20. Retrieved 2018-07-20.\n Ohlídal, M.; Jaroš, J.; Schwarz, J.; et al. (2006). \"Evolutionary Design of OAB and AAB Communication Schedules for Interconnection Networks\". In Rothlauf, F.; Branke, J.; Cagnoni, S. (eds.). Applications of Evolutionary Computing. Springer Science & Business Media. pp. 267–78. ISBN 9783540332374.\n \"Real Time And Distributed Computing Systems\" (PDF). ISSN 2278-0661. Archived from the original (PDF) on 2017-01-10. Retrieved 2017-01-09. {{cite journal}}: Cite journal requires |journal= (help)\n Vigna P, Casey MJ. The Age of Cryptocurrency: How Bitcoin and the Blockchain Are Challenging the Global Economic Order St. Martin's Press January 27, 2015 ISBN 9781250065636\n Quang Hieu Vu; Mihai Lupu; Beng Chin Ooi (2010). Peer-to-peer computing : principles and applications. Heidelberg: Springer. p. 16. ISBN 9783642035135. OCLC 663093862.\n Lind P, Alm M (2006), \"A database-centric virtual chemistry system\", J Chem Inf Model, 46 (3): 1034–9, doi:10.1021/ci050360b, PMID 16711722.\n Chiu, G (1990). \"A model for optimal database allocation in distributed computing systems\". Proceedings. IEEE INFOCOM'90: Ninth Annual Joint Conference of the IEEE Computer and Communications Societies.\n Elmasri & Navathe (2000), Section 24.1.2.\n Andrews (2000), p. 10–11. Ghosh (2007), p. 4–6. Lynch (1996), p. xix, 1. Peleg (2000), p. xv. Elmasri & Navathe (2000), Section 24.\n Haussmann, J. (2019). \"Cost-efficient parallel processing of irregularly structured problems in cloud computing environments\". Journal of Cluster Computing. 22 (3): 887–909. doi:10.1007/s10586-018-2879-3. S2CID 54447518.\n Reactive Application Development. Manning. 2018. ISBN 9781638355816.\n Toomarian, N.B.; Barhen, J.; Gulati, S. (1992). \"Neural Networks for Real-Time Robotic Applications\". In Fijany, A.; Bejczy, A. (eds.). Parallel Computation Systems For Robotics: Algorithms And Architectures. World Scientific. p. 214. ISBN 9789814506175. Archived from the original on 2020-08-01. Retrieved 2018-07-20.\n Savage, J.E. (1998). Models of Computation: Exploring the Power of Computing. Addison Wesley. p. 209. ISBN 9780201895391.\n Cormen, Leiserson & Rivest (1990), Section 30.\n Herlihy & Shavit (2008), Chapters 2–6.\n Lynch (1996)\n Cormen, Leiserson & Rivest (1990), Sections 28 and 29.\n TULSIRAMJI GAIKWAD-PATIL College of Engineering & Technology, Nagpur Department of Information Technology Introduction to Distributed Systems[1]\n Cole & Vishkin (1986). Cormen, Leiserson & Rivest (1990), Section 30.5.\n Andrews (2000), p. ix.\n Arora & Barak (2009), Section 6.7. Papadimitriou (1994), Section 15.3.\n Papadimitriou (1994), Section 15.2.\n Lynch (1996), p. 17–23.\n Peleg (2000), Sections 2.3 and 7. Linial (1992). Naor & Stockmeyer (1995).\n Schneider, J.; Wattenhofer, R. (2011). \"Trading Bit, Message, and Time Complexity of Distributed Algorithms\". In Peleg, D. (ed.). Distributed Computing. Springer Science & Business Media. pp. 51–65. ISBN 9783642240997. Archived from the original on 2020-08-01. Retrieved 2018-07-20.\n Lynch (1996), Sections 5–7. Ghosh (2007), Chapter 13.\n Lynch (1996), p. 99–102. Ghosh (2007), p. 192–193.\n Dolev (2000). Ghosh (2007), Chapter 17.\n Lynch (1996), Section 16. Peleg (2000), Section 6.\n Lynch (1996), Section 18. Ghosh (2007), Sections 6.2–6.3.\n Ghosh (2007), Section 6.4.\n Foundations of Data Intensive Applications Large Scale Data Analytics Under the Hood. 2021. ISBN 9781119713012.\n Haloi, S. (2015). Apache ZooKeeper Essentials. Packt Publishing Ltd. pp. 100–101. ISBN 9781784398323. Archived from the original on 2023-01-20. Retrieved 2018-07-20.\n LeLann, G. (1977). \"Distributed systems - toward a formal approach\". Information Processing. 77: 155·160 – via Elsevier.\n R. G. Gallager, P. A. Humblet, and P. M. Spira (January 1983). \"A Distributed Algorithm for Minimum-Weight Spanning Trees\" (PDF). ACM Transactions on Programming Languages and Systems. 5 (1): 66–77. doi:10.1145/357195.357200. S2CID 2758285. Archived (PDF) from the original on 2017-09-26.\n Korach, Ephraim; Kutten, Shay; Moran, Shlomo (1990). \"A Modular Technique for the Design of Efficient Distributed Leader Finding Algorithms\" (PDF). ACM Transactions on Programming Languages and Systems. 12 (1): 84–101. CiteSeerX 10.1.1.139.7342. doi:10.1145/77606.77610. S2CID 9175968. Archived (PDF) from the original on 2007-04-18.\n Hamilton, Howard. \"Distributed Algorithms\". Archived from the original on 2012-11-24. Retrieved 2013-03-03.\n \"Major unsolved problems in distributed systems?\". cstheory.stackexchange.com. Archived from the original on 20 January 2023. Retrieved 16 March 2018.\n \"How big data and distributed systems solve traditional scalability problems\". theserverside.com. Archived from the original on 17 March 2018. Retrieved 16 March 2018.\n Svozil, K. (2011). \"Indeterminism and Randomness Through Physics\". In Hector, Z. (ed.). Randomness Through Computation: Some Answers, More Questions. World Scientific. pp. 112–3. ISBN 9789814462631. Archived from the original on 2020-08-01. Retrieved 2018-07-20.\n Papadimitriou (1994), Section 19.3.\nReferences\nBooks\nAndrews, Gregory R. (2000), Foundations of Multithreaded, Parallel, and Distributed Programming, Addison–Wesley, ISBN 978-0-201-35752-3.\nArora, Sanjeev; Barak, Boaz (2009), Computational Complexity – A Modern Approach, Cambridge, ISBN 978-0-521-42426-4.\nCormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L. (1990), Introduction to Algorithms (1st ed.), MIT Press, ISBN 978-0-262-03141-7.\nDolev, Shlomi (2000), Self-Stabilization, MIT Press, ISBN 978-0-262-04178-2.\nElmasri, Ramez; Navathe, Shamkant B. (2000), Fundamentals of Database Systems (3rd ed.), Addison–Wesley, ISBN 978-0-201-54263-9.\nGhosh, Sukumar (2007), Distributed Systems – An Algorithmic Approach, Chapman & Hall/CRC, ISBN 978-1-58488-564-1.\nLynch, Nancy A. (1996), Distributed Algorithms, Morgan Kaufmann, ISBN 978-1-55860-348-6.\nHerlihy, Maurice P.; Shavit, Nir N. (2008), The Art of Multiprocessor Programming, Morgan Kaufmann, ISBN 978-0-12-370591-4.\nPapadimitriou, Christos H. (1994), Computational Complexity, Addison–Wesley, ISBN 978-0-201-53082-7.\nPeleg, David (2000), Distributed Computing: A Locality-Sensitive Approach, SIAM, ISBN 978-0-89871-464-7, archived from the original on 2009-08-06, retrieved 2009-07-16.\nArticles\nCole, Richard; Vishkin, Uzi (1986), \"Deterministic coin tossing with applications to optimal parallel list ranking\", Information and Control, 70 (1): 32–53, doi:10.1016/S0019-9958(86)80023-7.\nKeidar, Idit (2008), \"Distributed computing column 32 – The year in review\", ACM SIGACT News, 39 (4): 53–54, CiteSeerX 10.1.1.116.1285, doi:10.1145/1466390.1466402, S2CID 7607391, archived from the original on 2014-01-16, retrieved 2009-08-20.\nLinial, Nathan (1992), \"Locality in distributed graph algorithms\", SIAM Journal on Computing, 21 (1): 193–201, CiteSeerX 10.1.1.471.6378, doi:10.1137/0221015.\nNaor, Moni; Stockmeyer, Larry (1995), \"What can be computed locally?\" (PDF), SIAM Journal on Computing, 24 (6): 1259–1277, CiteSeerX 10.1.1.29.669, doi:10.1137/S0097539793254571, archived (PDF) from the original on 2013-01-08.\nWeb sites\nGodfrey, Bill (2002). \"A primer on distributed computing\". Archived from the original on 2021-05-13. Retrieved 2021-05-13.\nPeter, Ian (2004). \"Ian Peter's History of the Internet\". Archived from the original on 2010-01-20. Retrieved 2009-08-04.\nFurther reading\nBooks\nAttiya, Hagit and Jennifer Welch (2004), Distributed Computing: Fundamentals, Simulations, and Advanced Topics, Wiley-Interscience ISBN 0-471-45324-2.\nChristian Cachin; Rachid Guerraoui; Luís Rodrigues (2011), Introduction to Reliable and Secure Distributed Programming (2. ed.), Springer, Bibcode:2011itra.book.....C, ISBN 978-3-642-15259-7\nCoulouris, George; et al. (2011), Distributed Systems: Concepts and Design (5th Edition), Addison-Wesley ISBN 0-132-14301-1.\nFaber, Jim (1998), Java Distributed Computing, O'Reilly, archived from the original on 2010-08-24, retrieved 2010-09-29: Java Distributed Computing by Jim Faber, 1998 Archived 2010-08-24 at the Wayback Machine\nGarg, Vijay K. (2002), Elements of Distributed Computing, Wiley-IEEE Press ISBN 0-471-03600-5.\nTel, Gerard (1994), Introduction to Distributed Algorithms, Cambridge University Press\nChandy, Mani; et al. (1988), Parallel Program Design, Addison-Wesley ISBN 0201058669\nDusseau, Remzi H.; Dusseau, Andrea (2016). Operating Systems: Three Easy Pieces, Chapter 48 Distributed Systems (PDF). Archived from the original (PDF) on 31 August 2021. Retrieved 8 October 2021.\nArticles\nKeidar, Idit; Rajsbaum, Sergio, eds. (2000–2009), \"Distributed computing column\", ACM SIGACT News, archived from the original on 2014-01-16, retrieved 2009-08-16.\nBirrell, A. D.; Levin, R.; Schroeder, M. D.; Needham, R. M. (April 1982). \"Grapevine: An exercise in distributed computing\" (PDF). Communications of the ACM. 25 (4): 260–274. doi:10.1145/358468.358487. S2CID 16066616. Archived (PDF) from the original on 2016-07-30.\nConference Papers\nRodriguez, Carlos; Villagra, Marcos; Baran, Benjamin (2007). \"Asynchronous team algorithms for Boolean Satisfiability\". 2007 2nd Bio-Inspired Models of Network, Information and Computing Systems. pp. 66–69. doi:10.1109/BIMNICS.2007.4610083. S2CID 15185219.\nExternal links\n\nWikiquote has quotations related to Distributed computing.\n Media related to Distributed computing at Wikimedia Commons\nvte\nParallel computing\nAuthority control databases Edit this at Wikidata\nCategory: Distributed computing\nThis page was last edited on 4 October 2024, at 08:15 (UTC).\nText is available under the Creative Commons Attribution-ShareAlike 4.0 License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\nPrivacy policyAbout WikipediaDisclaimersContact WikipediaCode of ConductDevelopersStatisticsCookie statementMobile view\nWikimedia FoundationPowered by MediaWiki\n\n"
    },
    {
      "file_path": "main.cpp",
      "content": "#include \"Daemon.cpp\"\n#include <bits/stdc++.h>\nusing namespace std;\n\n\nint main (int argc, char* argv[]) {\n    // parse command line arguments and pass to Daemon - ./daemon <port> <introducer port>, introducer port is optional\n    // ./Daemon 4560 9870 localhost local/remote introducer\n    int introducerNumArgs = 6;\n    if(argc>=5){\n        bool isIntroducer = argc == introducerNumArgs ? true : false;\n        string machineType = isIntroducer ? \"introducer\":\"node\";\n        string env = argv[4];\n        char *daemonPort = argv[1];\n        char *hydfsPort = argv[2];\n        char *introducerName = argv[3];\n        cout<<\"Starting \"<<machineType<<\" on \"<<daemonPort<<\" with introducer on \"<<introducerName<<\" with \"<<env<<\" environment\\n\";\n\n        HyDFS dfs(hydfsPort);\n        dfs.start();\n        Daemon d(daemonPort, hydfsPort);\n        d.addListener(&dfs);\n        d.start(isIntroducer, introducerName);\n\n        if(string(argv[4]) == \"remote\") {\n            dfs.setLocalFilesPath(\"./files/remote\");\n        } else {\n            if(isIntroducer){\n                dfs.setLocalFilesPath(\"./files/introducer\");\n            } else {\n                dfs.setLocalFilesPath(\"\");\n            }\n        }\n\n        while(1){\n            string command;\n            getline(cin, command);\n            if(command == \"leave\"){\n                dfs.stopThreads();\n                d.stopThreads();\n                break;\n            } else {\n                d.runCommand(command);\n            }\n        }\n    } else {\n        cout<<\"Invalid arguments\\n\";\n    }\n    return 0;\n}"
    },
    {
      "file_path": "scripts/scripts/cache_plot.py",
      "content": "import matplotlib.pyplot as plt\n\n# Cache sizes for x-axis labels\ncache_sizes = [10, 20, 50, 100]\n\n## Averages after taking 3 readings per point\n\nlatencies_with_cache_uniform = [39.745, 36.221, 31.407, 22.948]\nstddev_with_cache_uniform = [1.54, 1.26, 1.32, 1.93]\n\nlatencies_with_cache_zipfian = [39.192, 35.565, 28.948, 19.420]\nstddev_with_cache_zipfian = [1.88, 1.55, 1.58, 2.11]\n\nlatencies_without_cache_uniform = [41.410, 41.410, 41.410, 41.410]\nstddev_without_cache_uniform = [1.94, 1.94, 1.94, 1.94]\n\nlatencies_without_cache_zipfian = [42.049, 42.049, 42.049, 42.049]\nstddev_without_cache_zipfian = [1.81, 1.81, 1.81, 1.81]\n\n# Cache sizes as percentages\ncache_sizes = [10, 20, 50, 100]\n\n\n# Plotting\nplt.figure(figsize=(12, 6))\n\n# Plot for \"With Cache\" - Uniform\nplt.errorbar(\n    cache_sizes, latencies_with_cache_uniform, yerr=stddev_with_cache_uniform,\n    label=\"With Cache - Uniform\", fmt='-o', capsize=5\n)\nfor i, (avg, std) in enumerate(zip(latencies_with_cache_uniform, stddev_with_cache_uniform)):\n    plt.annotate(f'{avg:.2f}±{std:.2f}', (cache_sizes[i], latencies_with_cache_uniform[i]))\n\n\n# Plot for \"With Cache\" - Zipfian\nplt.errorbar(\n    cache_sizes, latencies_with_cache_zipfian, yerr=stddev_with_cache_zipfian,\n    label=\"With Cache - Zipfian\", fmt='-o', capsize=5\n)\nfor i, (avg, std) in enumerate(zip(latencies_with_cache_zipfian, stddev_with_cache_zipfian)):\n    plt.annotate(f'{avg:.2f}±{std:.2f}', (cache_sizes[i], latencies_with_cache_zipfian[i]))\n\n\n# Plot for \"Without Cache\" - Uniform\nplt.errorbar(\n    cache_sizes, latencies_without_cache_uniform, yerr=stddev_without_cache_uniform,\n    label=\"Without Cache - Uniform\", fmt='-o', capsize=5\n)\nfor i, (avg, std) in enumerate(zip(latencies_without_cache_uniform, stddev_without_cache_uniform)):\n    plt.annotate(f'{avg:.2f}±{std:.2f}', (cache_sizes[i], latencies_without_cache_uniform[i]))\n\n\n# Plot for \"Without Cache\" - Zipfian\nplt.errorbar(\n    cache_sizes, latencies_without_cache_zipfian, yerr=stddev_without_cache_zipfian,\n    label=\"Without Cache - Zipfian\", fmt='-o', capsize=5\n)\nfor i, (avg, std) in enumerate(zip(latencies_without_cache_zipfian, stddev_without_cache_zipfian)):\n    plt.annotate(f'{avg:.2f}±{std:.2f}', (cache_sizes[i], latencies_without_cache_zipfian[i]))\n\n\n# Labels and title\nplt.xlabel(\"Cache Size (%)\")\nplt.ylabel(\"Latency (in s)\")\nplt.title(\"Read Latency with and without Client-Side Caching\")\nplt.legend()\nplt.grid(True)\nplt.xticks(cache_sizes)\nplt.tight_layout()\n\n# Show plot\nplt.show()\n\n"
    },
    {
      "file_path": "scripts/scripts/cache_plot2.py",
      "content": "import matplotlib.pyplot as plt\nimport numpy as np\n\nresults = {\n    \"with_cache\": {\n        \"uniform_50%\": (33.112, 1.95),\n        \"zipfian_50%\": (31.712, 1.80)\n    },\n    \"without_cache\": {\n        \"uniform_50%\": (44.52, 1.44),\n        \"zipfian_50%\": (44.83, 1.51)\n    }\n}\n\n# Data for plotting\nlabels = ['Uniform Distribution (50% Cache)', 'Zipfian Distribution (50% Cache)']\ncache_means = [results[\"with_cache\"][\"uniform_50%\"][0], results[\"with_cache\"][\"zipfian_50%\"][0]]\ncache_stddevs = [results[\"with_cache\"][\"uniform_50%\"][1], results[\"with_cache\"][\"zipfian_50%\"][1]]\nno_cache_means = [results[\"without_cache\"][\"uniform_50%\"][0], results[\"without_cache\"][\"zipfian_50%\"][0]]\nno_cache_stddevs = [results[\"without_cache\"][\"uniform_50%\"][1], results[\"without_cache\"][\"zipfian_50%\"][1]]\n\n# X-axis positions for the bars\nx = np.arange(len(labels))  # label locations\nwidth = 0.35  # width of the bars\n\n# Plotting the bar chart\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Bars for \"With Cache\"\nbars1 = ax.bar(x - width/2, cache_means, width, label='With Cache', yerr=cache_stddevs, capsize=5, color='skyblue')\n\n# Bars for \"Without Cache\"\nbars2 = ax.bar(x + width/2, no_cache_means, width, label='Without Cache', yerr=no_cache_stddevs, capsize=5, color='salmon')\n\n# Labels and formatting\nax.set_xlabel('Distribution and Cache Configuration')\nax.set_ylabel('Average Latency (ms)')\nax.set_title('Cache Performance with Appends)')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\n\n# Display bar values on top of bars\n# def add_bar_labels(bars):\n#     for rect in bars:\n#         height = rect.get_height()\n#         ax.annotate(f'{height:.1f}',\n#                     xy=(rect.get_x() + rect.get_width() / 2, height),\n#                     xytext=(0, 3),  # Offset text above bar\n#                     textcoords=\"offset points\",\n#                     ha='center', va='bottom')\n\n# add_bar_labels(bars1)\n# add_bar_labels(bars2)\n\ndef add_labels(bars, means, stddevs):\n    for bar, mean, stddev in zip(bars, means, stddevs):\n        height = bar.get_height()\n        ax.annotate(f'{mean} ± {stddev}',\n                    xy=(bar.get_x() + bar.get_width() / 2, height),\n                    xytext=(0, 3),\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\n\nadd_labels(bars1, cache_means, cache_stddevs)\nadd_labels(bars2, no_cache_means, no_cache_stddevs)\n\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "file_path": "scripts/scripts/createTestLocalFiles.sh",
      "content": "#!/bin/bash\n\n# Define the target directory\nTARGET_DIR=\"./files/remote/local\"\n\n# Create the target directory if it doesn't exist\nmkdir -p \"$TARGET_DIR\"\n\n# Create files of specified sizes\n# dd if=/dev/zero of=\"$TARGET_DIR/test5mb\" bs=1M count=5\ndd if=/dev/zero of=\"$TARGET_DIR/test10mb\" bs=1M count=10\ndd if=/dev/zero of=\"$TARGET_DIR/test20mb\" bs=1M count=20\ndd if=/dev/zero of=\"$TARGET_DIR/test50mb\" bs=1M count=50\ndd if=/dev/zero of=\"$TARGET_DIR/test75mb\" bs=1M count=75\ndd if=/dev/zero of=\"$TARGET_DIR/test100mb\" bs=1M count=100\n\necho \"Files created successfully in $TARGET_DIR\"\n"
    },
    {
      "file_path": "scripts/scripts/generate_dataset.py",
      "content": "import os\ndataset_dir = './dataset'\n\n\n\nos.makedirs(dataset_dir, exist_ok=True)\n\nfor i in range(1, 5001):\n    with open(os.path.join(dataset_dir, f\"file_{i}\"), \"wb\") as f:\n        f.write(os.urandom(4000))  # 4KB of random data\n\n"
    },
    {
      "file_path": "scripts/scripts/graph.py",
      "content": "import matplotlib.pyplot as plt\nimport numpy as np\n\n# Cache sizes for x-axis labels\n# file_size = [5, 10, 20, 50, 75, 100]\nfile_size = [1, 2, 5, 10]\n\n## Averages after taking 3 readings per point\n\n# re_replication_time_ms = [[29, 29, 36, 24, 27], [58, 30, 45, 39, 55], [128, 107, 113, 118, 208], [301, 255, 224, 250, 173], [334, 373, 250, 329, 346], [538, 532, 419, 535, 486]]\n# re_replication_std_deviation = [np.std(times) for times in re_replication_time_ms]\n# re_replication_time_ms = [np.mean(times) for times in re_replication_time_ms]\n\n# re_replication_bandwidth = [[172.4, 172, 138, 208, 185], [172, 333, 222, 256, 181], [156, 186, 176, 169, 196], [223, 200, 289, 166, 196], [295, 227, 216, 224, 201], [238, 186, 205, 185, 187]]\n# re_replication_bandwidth_std_deviation = [np.std(times) for times in re_replication_bandwidth]\n# re_replication_bandwidth = [np.mean(times) for times in re_replication_bandwidth]\n\n# re_replication_bandwidth = [[172.4, 172, 138, 208, 185], [172, 333, 222, 256, 181], [156, 186, 176, 169, 196], [223, 200, 289, 166, 196], [295, 227, 216, 224, 201], [238, 186, 205, 185, 187]]\n# re_replication_bandwidth_std_deviation = [np.std(times) for times in re_replication_bandwidth]\n# re_replication_bandwidth = [np.mean(times) for times in re_replication_bandwidth]\n\nmulti_append_4kb = [[887, 861], [924, 905], [1071, 898], [864, 868]]\nmulti_append_4kb_std_deviation = [np.std(times) for times in multi_append_4kb]\nmulti_append_4kb = [np.mean(times) for times in multi_append_4kb]\n\nmulti_append_40kb = [[1110, 1310], [1320, 1420], [1410, 1280], [1219, 1207]]\nmulti_append_40kb_std_deviation = [np.std(times) for times in multi_append_40kb]\nmulti_append_40kb = [np.mean(times) for times in multi_append_40kb]\n\n\n\n\n\n# Plotting\nplt.figure(figsize=(12, 6))\n\n# Plot for \"Multi Append 4KB\"\nplt.errorbar(\n    file_size, multi_append_4kb, yerr=multi_append_4kb_std_deviation,\n    label=\"Multi Append 4KB\", fmt='-o', capsize=5\n)\nfor i, (avg, std) in enumerate(zip(multi_append_4kb, multi_append_4kb_std_deviation)):\n    plt.annotate(f'{avg:.2f}±{std:.2f}', (file_size[i], multi_append_4kb[i]))\n\n# Plot for \"Multi Append 40KB\"\nplt.errorbar(\n    file_size, multi_append_40kb, yerr=multi_append_40kb_std_deviation,\n    label=\"Multi Append 40KB\", fmt='-o', capsize=5\n)\nfor i, (avg, std) in enumerate(zip(multi_append_40kb, multi_append_40kb_std_deviation)):\n    plt.annotate(f'{avg:.2f}±{std:.2f}', (file_size[i], multi_append_40kb[i]))\n\n# Labels and title\nplt.xlabel(\"Number of clients\")\nplt.ylabel(\"Time (ms)\")\nplt.title(\"Multi Append Time for 1000 concurrent appends\")\nplt.legend()\nplt.grid(True)\nplt.xticks(file_size)\nplt.tight_layout()\n\n# Show plot\nplt.show()\n\n\n\n\n\n# # Plotting\n# plt.figure(figsize=(12, 6))\n\n# # Plot for \"With Cache\" - Uniform\n# # plt.errorbar(\n# #     file_size, re_replication_time_ms, yerr=re_replication_std_deviation,\n# #     label=\"Re-replication time\", fmt='-o', capsize=5\n# # )\n# # for i, (avg, std) in enumerate(zip(re_replication_time_ms, re_replication_std_deviation)):\n# #     plt.annotate(f'{avg:.2f}±{std:.2f}', (file_size[i], re_replication_time_ms[i]))\n\n# plt.errorbar(\n#     file_size, re_replication_bandwidth, yerr=re_replication_bandwidth_std_deviation,\n#     label=\"Re-replication bandwidth\", fmt='-o', capsize=5\n# )\n# for i, (avg, std) in enumerate(zip(re_replication_bandwidth, re_replication_bandwidth_std_deviation)):\n#     plt.annotate(f'{avg:.2f}±{std:.2f}', (file_size[i], re_replication_bandwidth[i]))\n\n\n\n# Labels and title\n# plt.xlabel(\"File Size (MB)\")\n# plt.ylabel(\"Re replication bandwidth (in MB/s)\")\n# plt.title(\"Re-replication bandwidth\")\n# plt.legend()\n# plt.grid(True)\n# plt.xticks(file_size)\n# plt.tight_layout()\n\n# # Show plot\n# plt.show()"
    },
    {
      "file_path": "scripts/scripts/init.sh",
      "content": "#!/bin/bash\n\n\nHOSTS=(\n\"fa24-cs425-7301.cs.illinois.edu\"\n\"fa24-cs425-7302.cs.illinois.edu\"\n\"fa24-cs425-7303.cs.illinois.edu\"\n\"fa24-cs425-7304.cs.illinois.edu\"\n\"fa24-cs425-7305.cs.illinois.edu\"\n\"fa24-cs425-7306.cs.illinois.edu\"\n\"fa24-cs425-7307.cs.illinois.edu\"\n\"fa24-cs425-7308.cs.illinois.edu\"\n\"fa24-cs425-7309.cs.illinois.edu\"\n\"fa24-cs425-7310.cs.illinois.edu\"\n)\n\nUSERNAME=\"rahuls14\"\nCURR_BRANCH=$(git branch --show-current 2>/dev/null)\necho $CURR_BRANCH\nREPO_URL=\"https://gitlab.engr.illinois.edu/abandal2/g73_mp2.git\"\n\nREPO_DIR=\"g73_mp2\"\nPASSWORD_FILE=\"/Users/rahul/password.txt\"\nACCESS_TOKEN_FILE=\"/Users/rahul/gitlabAccessToken.txt\"\nGIT_ACCESS_TOKEN=$(cat \"$ACCESS_TOKEN_FILE\")\nREPO_URL=\"https://cs425:${GIT_ACCESS_TOKEN}@gitlab.engr.illinois.edu/abandal2/g73_mp2.git\"\nREMOTE_COMMANDS=\"cd ~/g73_mp2\"\nLOGS_REPO=\"logs\"\nTEST_LOGS_REPO=\"test_logs\"\nPORT=4560\nTCPPORT=7890\necho $REPO_URL\n\n\nif [[ -f \"$PASSWORD_FILE\" ]]; then\n    PASSWORD=$(<\"$PASSWORD_FILE\")\nelse\n    echo \"Password file not found!\"\n    exit 1\nfi\n\ni=1\n\nfor HOST in \"${HOSTS[@]}\"\ndo\n    echo \"Connecting to $HOST\"\n\n    sshpass -p $PASSWORD ssh -o StrictHostKeyChecking=no \"$USERNAME@$HOST\" bash -c \"'\n\n\n        ## pull git repo\n        rm -r $REPO_DIR\n        if [ ! -d \\\"$REPO_DIR\\\" ]; then\n            echo \\\"Cloning repository on $HOST...\\\"\n            git clone $REPO_URL $REPO_DIR\n        fi\n\n        cd $REPO_DIR\n        echo \\\"pulling $HOST...\\\"\n        git checkout $CURR_BRANCH\n        # git pull origin main\n        cd ~\n\n        # build and run server\n\n        cd $REPO_DIR \n        make\n        PID=\\$(lsof -t -i:$PORT)\n        if [ -z \"\\$PID\" ]; then\n            echo \"No process found running on port $PORT\"\n        else\n            # Kill the process\n            kill -9 \\$PID\n            echo \"Process running on port has been killed\"\n        fi\n\n        PID=\\$(lsof -t -i:$TCPPORT)\n        if [ -z \"\\$PID\" ]; then\n            echo \"No process found running on port $TCPPORT\"\n        else\n            # Kill the process\n            kill -9 \\$PID\n            echo \"Process running on port has been killed\"\n        fi\n\n        exit\n    '\"\n\n    # if [ $? -eq 0 ]; then\n    #     echo \"Commands executed successfully on $HOST.\"\n    # else\n    #     echo \"Failed to execute commands on $HOST.\"\n    # fi\n\n    ((i++))\n    echo \"---------------------------------\"\ndone\n"
    },
    {
      "file_path": "scripts/scripts/logvm.sh",
      "content": "# ./logvm.sh $vmid\n\n# #!/bin/bash\n\nvmid=$1\nHOST=\"fa24-cs425-73${vmid}.cs.illinois.edu\"\n\nUSERNAME=\"rahuls14\"\nPASSWORD_FILE=\"/Users/rahul/password.txt\"\nSCRIPT_INPUT=\nif [[ \"$vmid\" == \"01\" ]]; then\n    SCRIPT_INPUT=0\nelse\n    SCRIPT_INPUT=1\nfi\n\nif [[ -f \"$PASSWORD_FILE\" ]]; then\n    PASSWORD=$(<\"$PASSWORD_FILE\")\nelse\n    echo \"Password file not found!\"\n    exit 1\nfi\n\necho $HOST\n\nsshpass -p \"$PASSWORD\" ssh \"$USERNAME@$HOST\" bash -c \"'\n    cd g73_mp2\n    git fetch\n    git checkout mp3-experiments\n    git pull\n    make cfiles\n    make clean\n    make\n    bash scripts/rrun.sh $SCRIPT_INPUT\n'\"\n\n\n"
    },
    {
      "file_path": "scripts/scripts/lrun.sh",
      "content": "\n\n# get first argument\nif [ -z \"$1\" ]; then\n    echo \"Usage: $0 <command>\"\n    exit 1\nfi\n\n# declare a variable port = 4560+$1\nport=$((4560 + $1))\ntcpPort=$((7890 + $1))\n\n# kill the process on the port\nkill_process_on_port() {\n    local port=$1\n    local pid=$(lsof -ti :$port)\n    if [ ! -z \"$pid\" ]; then\n        echo \"Killing process on port $port (PID: $pid)\"\n        kill -9 $pid\n    else\n        echo \"No process found on port $port\"\n    fi\n}\n\n# call the function\nkill_process_on_port $port\nkill_process_on_port $tcpPort\n\n# if port == 4560\nif [ $port -eq 4560 ]; then\n    echo \"Starting introducer on port $port $tcpPort\"\n    ./Daemon $port $tcpPort localhost local introducer \n    # echo \"Introducer started on port $port\"\n\n# echo \"Starting daemon on port $port\"\n# Replace the following line with your actual daemon command\n# For example: your_daemon_command --port $port &\nelse\n    echo \"Starting daemon on port $port\"\n    ./Daemon $port $tcpPort localhost local\nfi\n\n\n# write demo if else\n# if [ $port -eq 4560 ]; then\n#     echo \"Starting introducer on port $port\"\n# else\n#     echo \"Starting daemon on port $port\"\n# fi\n\n"
    },
    {
      "file_path": "scripts/scripts/pb.sh",
      "content": "git pull && make"
    },
    {
      "file_path": "scripts/scripts/rrun.sh",
      "content": "\n\n# get first argument\nif [ -z \"$1\" ]; then\n    echo \"Usage: $0 <command>\"\n    exit 1\nfi\n\n# // ./rrun.sh 0\n# // ./rrun.sh 1\n\n# declare a variable port = 4560+$1\nport=4560\ntcpPort=7890\n\n# kill the process on the port\nkill_process_on_port() {\n    local port=$1\n    local pid=$(lsof -ti :$port)\n    if [ ! -z \"$pid\" ]; then\n        echo \"Killing process on port $port (PID: $pid)\"\n        kill -9 $pid\n    else\n        echo \"No process found on port $port\"\n    fi\n}\n\n# call the function\nkill_process_on_port $port\nkill_process_on_port $tcpPort\n\n# if port == 4560\nif [ $1 -eq 0 ]; then\n    echo \"Starting introducer on port $port $tcpPort\"\n    ./Daemon $port $tcpPort fa24-cs425-7301.cs.illinois.edu remote introducer \n    # echo \"Introducer started on port $port\"\n\n# echo \"Starting daemon on port $port\"\n# Replace the following line with your actual daemon command\n# For example: your_daemon_command --port $port &\nelse\n    echo \"Starting daemon on port $port\"\n    ./Daemon $port $tcpPort fa24-cs425-7301.cs.illinois.edu remote\nfi\n\n\n# write demo if else\n# if [ $port -eq 4560 ]; then\n#     echo \"Starting introducer on port $port\"\n# else\n#     echo \"Starting daemon on port $port\"\n# fi\n\n"
    },
    {
      "file_path": "scripts/scripts/run_introducer_locally.sh",
      "content": "./Daemon 4560 9870 localhost introducer"
    },
    {
      "file_path": "scripts/scripts/run_nodes_locally.sh",
      "content": "\n#!/bin/bash\n\n# Define the range of ports\nSTART_PORT=4560\nSTART_TCP_PORT=7890\n\n# Function to start the daemon on a given port\nstart_daemon() {\n    local port=$1\n    local tcpPort=$2\n    echo \"Starting daemon on port $port\"\n    # Replace the following line with your actual daemon command\n    # For example: your_daemon_command --port $port &\n    \n    nohup ./Daemon $port $tcpPort localhost > /dev/null 2>&1 &\n}\n\n\necho \"Starting introducer on port $START_PORT $START_TCP_PORT\"\nnohup ./Daemon $START_PORT $START_TCP_PORT localhost introducer > /dev/null 2>&1 &\necho \"Introducer started on port $START_PORT\"\n# Loop through the port range and start daemons\nfor i in {1..4}\ndo\n    port=$((START_PORT + i))\n    tcpPort=$((START_TCP_PORT + i))\n    start_daemon $port $tcpPort\ndone\n\necho \"Daemons started on ports $START_PORT to $END_PORT\""
    },
    {
      "file_path": "scripts/scripts/script.py",
      "content": "import random\nimport numpy as np\n\nwith open('output.txt', 'w') as file:\n    for i in range(1, 10001):\n        file.write(f'create file_{i} hydfs_{i}\\n')\n\n\n# uniform distribution           \nwith open('get_uniform.txt', 'w') as file:\n    for i in range(1, 20001):\n        file_id = random.randint(1, 1000)\n        file.write(f'get hydfs_{file_id} local_{file_id}\\n')\n  \n  \n# zipfian distribution      \ndef zipfian_distribution(n, alpha=1.0):\n    ranks = np.arange(1, n + 1)\n    weights = ranks ** (-alpha)\n    weights /= weights.sum()\n    return np.random.choice(ranks, size=n, p=weights)\n\nzipfian_ids = zipfian_distribution(20000)\n\nwith open('get_zipfian.txt', 'w') as file:\n    for file_id in zipfian_ids:\n        file.write(f'get hydfs_{file_id} local_{file_id}\\n')"
    },
    {
      "file_path": "scripts/scripts/stop_local_nodes.sh",
      "content": "#!/bin/bash\n\n# Check if the number of ports is provided\nif [ $# -eq 0 ]; then\n    echo \"Usage: $0 <number_of_ports>\"\n    exit 1\nfi\n\n# Number of ports to check (x)\nNUM_PORTS=$1\n\n# Starting port\nSTART_PORT=4560\nSTART_TCP_PORT=7890\n\n# Function to kill process on a given port\nkill_process_on_port() {\n    local port=$1\n    local pid=$(lsof -ti :$port)\n    if [ ! -z \"$pid\" ]; then\n        echo \"Killing process on port $port (PID: $pid)\"\n        kill -9 $pid\n    else\n        echo \"No process found on port $port\"\n    fi\n}\n\n# Loop through ports and kill processes\nfor i in $(seq 0 $NUM_PORTS)\ndo\n    port=$((START_PORT + i))\n    tcpPort=$((START_TCP_PORT + i))\n    kill_process_on_port $port\n    kill_process_on_port $tcpPort\ndone\n\necho \"Finished killing processes on ports $START_PORT to $((START_PORT + NUM_PORTS))\""
    },
    {
      "file_path": "scripts/scripts/vm.sh",
      "content": "./Daemon 4560 fa24-cs425-7310.cs.illinois.edu"
    },
    {
      "file_path": "scripts/scripts/vmi.sh",
      "content": "./Daemon 4560 localhost introducer"
    },
    {
      "file_path": "utils.cpp",
      "content": "#include <netdb.h> // Add this include for addrinfo\n#include <unistd.h> // Add this include for close\n#include <random>  // Add this include for std::random_device\n#include <chrono>  // Add this include for std::chrono\n#include \"utils.h\"\nusing namespace std;\nint setupTCPSocket(char *port){\n\n    struct addrinfo hints, *res, *tmp;\n    int socketfd, status;\n\n    memset(&hints, 0, sizeof(hints));\n    hints.ai_socktype = SOCK_STREAM;\n    hints.ai_family = AF_INET;\n    hints.ai_flags = AI_PASSIVE;\n\n    if((status = getaddrinfo(NULL, port, &hints, &res)) != 0){\n        fprintf(stderr, \"getaddrinfo: %s\\n\", gai_strerror(status));\n    }\n\n    tmp = res;\n\n    while(tmp){\n\n        if((socketfd = socket(tmp->ai_family, tmp->ai_socktype, tmp->ai_protocol)) == -1){\n            perror(\"server: socket\");\n            tmp = tmp->ai_next;\n            continue;\n        }\n\n        int opt = 1;\n        setsockopt(socketfd, SOL_SOCKET, SO_REUSEADDR, &opt, sizeof(opt));\n\n        if(::bind(socketfd, tmp->ai_addr, tmp->ai_addrlen) == -1){\n            perror(\"server: bind\");\n            close(socketfd);\n            tmp = tmp->ai_next;\n            continue;\n        }\n\n        break;\n    }\n\n    freeaddrinfo(res);\n\n    if(tmp == NULL){\n        fprintf(stderr, \"server: failed to bind tcp\\n\");\n        exit(1);\n    }\n    cout<<\"Socket binded\\n\";\n    return socketfd;\n}\n\n\nint setupSocket(char *port) {\n        struct addrinfo hints, *res, *tmp;\n        int socketfd, status;\n\n        memset(&hints, 0, sizeof(hints));\n        hints.ai_socktype = SOCK_DGRAM;\n        hints.ai_family = AF_INET;\n        hints.ai_flags = AI_PASSIVE;\n\n        if((status = getaddrinfo(NULL, port, &hints, &res)) != 0){\n            fprintf(stderr, \"getaddrinfo: %s\\n\", gai_strerror(status));\n        }\n\n        tmp = res;\n\n        while(tmp){\n\n            if((socketfd = socket(tmp->ai_family, tmp->ai_socktype, tmp->ai_protocol)) == -1){\n                perror(\"server: socket\");\n                tmp = tmp->ai_next;\n                continue;\n            }\n\n            if(::bind(socketfd, tmp->ai_addr, tmp->ai_addrlen) == -1){\n                perror(\"server: bind\");\n                close(socketfd);\n                tmp = tmp->ai_next;\n                continue;\n            }\n\n            break;\n        }\n\n        freeaddrinfo(res);\n\n        if(tmp == NULL){\n            fprintf(stderr, \"server: failed to bind udp\\n\");\n            exit(1);\n        }\n        cout<<\"Socket binded\\n\";\n        return socketfd;\n    }\n\nstruct sockaddr_in getAddrFromNode(Node node){\n\n    // node.printNode();\n    struct addrinfo hints, *res;\n    memset(&hints, 0, sizeof(hints));\n    hints.ai_family = AF_INET;\n    hints.ai_socktype = SOCK_DGRAM;\n\n    // cout<<node.getNodeName()<<endl;\n    int status = getaddrinfo(node.getNodeName(), node.getPort(), &hints, &res);\n    if (status != 0) {\n        std::cerr << \"Error resolving hostname: \"<<node.getNodeName()<<\" :\" << gai_strerror(status) << std::endl;\n        exit(1);\n    }\n\n    struct sockaddr_in addr = *(struct sockaddr_in *)res->ai_addr;\n    freeaddrinfo(res);\n    return addr;\n}\n\nstruct sockaddr_in getTCPAddrFromNode(Node node){\n        // node.printNode();\n        struct addrinfo hints, *res;\n        memset(&hints, 0, sizeof(hints));\n        hints.ai_family = AF_INET;\n        hints.ai_socktype = SOCK_STREAM;\n\n        // cout<<node.getNodeName()<<endl;\n        int status = getaddrinfo(node.getNodeName(), node.getTcpPort(), &hints, &res);\n        if (status != 0) {\n            std::cerr << \"Error resolving hostname: \"<<node.getNodeName()<<\" :\" << gai_strerror(status) << std::endl;\n            exit(1);\n        }\n\n        struct sockaddr_in addr = *(struct sockaddr_in *)res->ai_addr;\n        freeaddrinfo(res);\n        return addr;\n}\n\n\nbool withProbability(double probability) {\n    std::random_device rd;  // Non-deterministic random number generator\n    std::mt19937 gen(rd()); // Seed the generator\n    std::uniform_real_distribution<> dis(0.0, 1.0); // Uniform distribution between 0 and 1\n\n    return dis(gen) < probability; // Perform action if random number is less than the probability\n}\n\n// unsigned long long generateHash(const std::string& str, int n) {\n//     unsigned long long hash = 0;\n//     const int p = 31; // A small prime number\n//     const int m = 1e9 + 9; // A large prime modulus\n//     unsigned long long p_pow = 1; // p^0\n\n//     for (char c : str) {\n//         hash = (hash + (c - 'a' + 1) * p_pow) % m;\n//         p_pow = (p_pow * p) % m; // Increment p^i\n//     }\n\n//     return hash%(int)pow(2, n);\n// }\n\nchar* getCurrentFullTimestamp() {\n    char* timestamp = new char[24];\n    time_t now = std::time(nullptr);\n    tm* localTime = localtime(&now);\n    auto now_ms = std::chrono::system_clock::now();\n    auto duration = now_ms.time_since_epoch();\n    auto millis = std::chrono::duration_cast<std::chrono::milliseconds>(duration).count() % 1000;\n    std::strftime(timestamp, 20, \"%Y-%m-%d %H:%M:%S\", localTime);\n    std::snprintf(timestamp + 19, 5, \".%03d\", static_cast<int>(millis));\n    return timestamp;\n}\n\nstd::string getCurrentTSinEpoch() {\n    auto now = std::chrono::system_clock::now();\n    auto now_ms = std::chrono::time_point_cast<std::chrono::milliseconds>(now);\n    auto epoch = now_ms.time_since_epoch();\n    auto value = std::chrono::duration_cast<std::chrono::milliseconds>(epoch);\n    return std::to_string(value.count());\n}\n\n\n int differenceWithCurrentEpoch(string ts){\n        auto now = std::chrono::system_clock::now();\n        auto now_ms = std::chrono::time_point_cast<std::chrono::milliseconds>(now);\n        auto epoch = now_ms.time_since_epoch();\n        auto value = std::chrono::duration_cast<std::chrono::milliseconds>(epoch);\n        return value.count() - stoll(ts);\n    }\n\nint hashFunction(string key, int ringSize) {\n        // Simple hash function for consistent hashing\n        hash<string> hashFunc;\n\n        size_t hashValue = hashFunc(key);\n        return hashValue % ringSize;\n    }"
    },
    {
      "file_path": "utils.h",
      "content": "#ifndef UTILS_H\n#define UTILS_H\n\n// Includes required libraries for the function\n#include <netdb.h>\n#include <sys/socket.h>\n#include <cstring>\n#include <cmath>\n#include <cstdio>\n#include <string>\n#include \"Node.h\"\n\n#define RING_SIZE pow(2, 10)\n\n// Function prototype for setting up a TCP socket\nint setupTCPSocket(char *port);\nint setupSocket(char *port);\nstruct sockaddr_in getAddrFromNode(Node node);\nstruct sockaddr_in getTCPAddrFromNode(Node node);\nbool withProbability(double probability);\n// unsigned long long generateHash(const std::string& str, int n=10);\nchar* getCurrentFullTimestamp();\nstd::string getCurrentTSinEpoch();\nint differenceWithCurrentEpoch(std::string ts);\nint hashFunction(std::string key, int m = RING_SIZE);\n\n#endif // UTILS_H\n"
    }
  ],
  "submodules": []
}